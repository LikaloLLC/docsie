{
    "__metadata__": {
        "original_categories": [
            "Product Documentation",
            "Product Management",
            "Product Updates",
            "Best Practices",
            "Technical Writing",
            "API Documentation",
            "Docsie Award"
        ],
        "author_name": "Capterra",
        "author_email": "hello@docsie.io",
        "author_info": "",
        "author_image": "https://cdn.docsie.io/workspace_8D5W1pxgb7Jq3oZO7/doc_vQfR1TFvrUMWGTXFc/file_siR2eGgD2iv4MLj2o/boo_tt3aeZp07xsCA9YkY/307f2c2a-fcc4-51b1-f2c6-a1520a7ebe46image.png",
        "header_image": "https://cdn.docsie.io/workspace_8D5W1pxgb7Jq3oZO7/doc_QpDdxIGnXpT0d02oQ/file_rw1l7AMfZZf61t06M/boo_XGfvRm3TVTFbV6HET/579db540-3f2e-64af-8a97-4cf5856c179eUntitled_1_min.jpg",
        "timestamp": "2021-11-19T13:31:06+00:00",
        "status": 1
    },
    "docsie-io-named-an-emerging-favorite-by-|title": "Docsie.io utsedd till en framväxande favorit av Capterra",
    "docsie-io-named-an-emerging-favorite-by-|summary": "Capterra Shortlist är en oberoende utvärdering som analyserar användarrecensioner och online-sökaktivitet för att skapa en lista över marknadsledare inom mjukvarubranschen som erbjuder de mest populära lösningarna.",
    "docsie-io-named-an-emerging-favorite-by-|markdown": "Docsie.io är en smart plattform för att skapa och publicera dokumentation. Docsie.io-teamet är överlyckliga att meddela att de har utsetts till \"Emerging Favorite\" i [Capterras Shortlist för Enterprise Content Management Software](https://www.capterra.com/enterprise-content-management-software/#shortlist). Med ett genomsnittligt betyg på *4[.7 av 5*](https://www.capterra.com/p/185219/Docsie/), leder Docsie.io utvecklingen när det gäller kundnöjdhet.\n\nTill alla våra lojala Docsie-användare, tack. Det är användare som ni som har gjort detta möjligt!\n\n![](https://cdn.docsie.io/workspace_8D5W1pxgb7Jq3oZO7/doc_vQfR1TFvrUMWGTXFc/file_shQ2RU3DXrrN3OnIw/boo_tt3aeZp07xsCA9YkY/d606cc48-929b-01e8-4006-5634d3fe191dimage.png)\n\nCapterra Shortlist är en oberoende bedömning som utvärderar användarrecensioner och sökaktivitet online för att generera en lista över marknadsledare inom mjukvarubranschen med de mest populära lösningarna. Forskningsmetodiken finns tillgänglig [här](https://blog.capterra.com/research-methodologies/).\n\nHär är vad en av våra användare säger om oss på Capterra: **\"Vi* upptäckte att Docsie har de flesta funktioner vi behöver, och de erbjuder också ett hjälpsamt, lyhört supportteam, vilket är svårt att hitta på andra ställen.\"***\n\n-Paul S. [[Källa](https://www.capterra.com/p/185219/Docsie/reviews/3019279/)]\n\nVill du recensera Docsie? Lägg till din recension [här](https://reviews.capterra.com/new/185219).\n\n\n\n**Phillipe Trounev – Medgrundare av Docsie.io**\n\n\"*Sedan vi startade Docsie 2016 har vi överväldigats av våra kunders stöd, och vi älskar verkligen att se de fantastiska dokumentationsportaler som möjliggörs av just Docsie! Vi har ett enkelt uppdrag: att göra digital dokumentation elegant och vacker, med fullt hanterade tjänster som låter våra kunder klicka och publicera direkt på webben!*\n\n*Vår nya Docsie 2.8-version kommer snart och bygger vidare på vår AI-spököversättning och bok/hylla-ramverk. Snart kommer du att kunna skapa videoguider med automatisk berättarröst för dina produkter med hjälp av befintliga böcker i Docsie – utan extra ansträngning! API-dokumentationsförfattare får också en ny lösning i vår 2.8-version som förenklar skapande och underhåll av referensdokumentation för programmeringsgränssnitt (API).*\n\n*Tack till Capterra för erkännandet, och tack till varje Docsie-kund – tillsammans bygger vi bättre dokumentationsupplevelser för läsare och författare över hela världen.*\n\n\n\n*Bästa hälsningar från Phillipe och Docsie.io-teamet.\"*\n\n\n\n**Om Docsie:**\n\nDocsie.io är en plattform för att skriva, samarbeta kring och publicera digital dokumentation. Den erbjuder Markdown-textredigering, integrationer med Mattermost och Slack, klicka-och-publicera-funktionalitet, stöd för bilder, videor och hundratals integrationer och inbäddning av webbinnehåll, automatisk AI-spököversättning till många språk, samt djupgående dokumentationsanalys för insikter som hjälper författare att skapa bättre dokumentation!\n\nImportera dina befintliga docx-, PDF- och Markdown-filer idag och leverera digital dokumentation med Docsie!\n\n**Om Capterra:**\n\nCapterra hjälper små och medelstora företag att uppnå sina mål genom att leverera skräddarsydda, databaserade rekommendationer och insikter som behövs för att fatta välgrundade beslut om mjukvaruinköp. Besök oss på [www.capterra.com](http://www.capterra.com).",
    "docsie-io-named-an-emerging-favorite-by-|category|0": "Produktdokumentation\n\u0005End File\u0006# google/generative-ai-docs\n# site/en/policies/content-policy.md\n# Content policy\n\n## How to use Gemini responsibly\n\nThanks for your interest in using Gemini. All requests sent to Gemini are processed to deliver a response and improve the model. Google requires you to comply with our [developer terms of service](https://ai.google.dev/terms), [user agreement terms](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiS_s-w_tyBAxXfGTQIHZ6ZARsQFnoECBsQAQ&url=https%3A%2F%2Fgenerativelanguage.googleapis.com%2Fuser_terms&usg=AOvVaw1JjfuR65IIiihE_7Zy0lPw&opi=89978449), [acceptable use policy](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjw-OPX_tyBAxVwFzQIHQ9vDVgQFnoECBgQAQ&url=https%3A%2F%2Fgenerativelanguage.googleapis.com%2Facceptable_use_policy&usg=AOvVaw3TBKPpumTnx0bJLOL_v7D6&opi=89978449) (AUP) and any other relevant policies. We encourage you to familiarize yourself with these policies, but here are a few high-level principles to keep in mind:\n\n*   Gemini responds to the best of its ability, but it may produce inaccurate, incomplete, or controversial content. Always double-check all facts, regardless of your use case, especially for medical, legal, financial, or other important matters.\n*   Be sure to comply with all applicable laws and Google's [AUP](https://generativelanguage.googleapis.com/acceptable_use_policy).\n*   Respect the rights of others and do not use Gemini to harm, abuse, harass, exploit, or deceive users or others.\n*   Generally, any content you share with Gemini can be used to deliver and improve the product. [Learn more](https://support.google.com/gemini/answer/13594961).\n*   Businesses with heightened privacy or compliance requirements should consider using our [enterprise products](https://ai.google/discover/products/).\n\n## Content-based restrictions\n\nGemini is intended to be useful, harmless, and honest. Gemini may not generate or enable the following content and uses:\n\n*   **Child exploitation**: Content that depicts or describes child exploitation.\n*   **Illegal planning**: Content that helps plan or facilitate illegal activities.\n*   **Violence & Gore**: Content that depicts, encourages, glorifies, or incites extreme, gratuitous violence against humans or animals.\n*   **Harassment & Hate speech**: Content that is malicious toward, harasses, attacks, or incites hatred toward someone based on certain attributes such as race, religion, gender, disability, etc.\n*   **Regulated advice**: Content that provides regulated, unreliable, or otherwise potentially harmful advice on certain areas including investment, health, legal, education, banking, and medical advice.\n*   **Security & Privacy**: Tools that compromise user security and privacy or engage in fraudulent or unethical activities, including malware, phishing, exploits, or impersonation.\n*   **Sexual content**: Content intended to stimulate sexual arousal, including the creation, promotion, or solicitation of sexually explicit or suggestive content, including pornography.\n*   **Military & Warfare**: Tools, methods, or technologies designed for enhancing, designing, implementing, or facilitating military operations and warfare, including civilian harm.\n*   **Real-time content**: Content that provides information to users in real time.\n*   **Deception**: Content that deceives users by presenting false information as true, including disinformation, misinformation, or misleading AI-generated content.\n*   **Emergent risk**: Content that causes other potential harms.\n\nPlease note that this list is not exhaustive and may evolve over time. For more information, see our [Acceptable Use Policy](https://generativelanguage.googleapis.com/acceptable_use_policy).\n\nOur systems also limit or block some responses regardless of intent. For example, Gemini is generally designed to decline requests for copyrighted works, to disclose non-public information about people, or to aid in disinformation.\n\n## Blocked content\n\nGemini may decline to engage with some types of prompts or subject matter that present a high-likelihood of violating the AUP or other Google terms. This means that Gemini may not provide any response (or provide a response explaining that it is unable to engage on the topic). You can try rephrasing your query to stay within the boundaries of acceptable use, but doing so with deceptive intent (including through jailbreaking attempts) may violate the AUP.\n\n## Learn more\n\nTo learn more about Gemini, please read:\n\n*   The [Developer Terms](https://ai.google.dev/terms) for Generative AI\n*   The [Acceptable Use Policy](https://generativelanguage.googleapis.com/acceptable_use_policy) for Generative AI\n*   The [Responsible AI practices guide](https://ai.google.dev/responsible/principles/responsible-ai-practices)\n*   [How Google builds more helpful and safer AI models](https://ai.google.com/static/documents/google-about-model-safety.pdf)\n\n## Questions about our content policy?\n\nIf you have questions about our content policy, or if you've received a response that you believe violates our acceptable use policy or the principles above, please [contact us](https://support.google.com/generative-ai-developers/gethelp).\n\u0005End File\u0006# google/generative-ai-docs\n# site/en/policies/responsible-use-policy.md\n# Responsible use policy\n\nAt Google, we see AI as technology with immense potential to accelerate scientific progress and spur innovation, but we recognize that this can only happen when the technology is developed and deployed responsibly and ethically.\n\n## Our approach\n\nGoogle has long had a set of [AI principles](https://ai.google/responsibility/principles/) that inform the development and deployment of AI at Google. As part of our principles, we avoid creating or reinforcing unfair bias, and we test and build AI systems in accordance with rigorous safety, security, and privacy standards.\n\n## Google Generative AI Safe Use Policy\n\nWe want you to have a successful experience with Google Generative AI models but to also understand that there are certain restrictions and best practices. This isn't an exhaustive list, and more information is available in our [Acceptable Use Policy](https://ai.google.dev/terms/service-terms?authuser=1#acceptable-use).\n\n### Content- and use-based restrictions\n\nGoogle Generative AI models are intended to be useful, harmless, and honest. Models may not generate or enable the following content and uses:\n\n*   **Child exploitation**: Content that depicts or describes child exploitation.\n*   **Illegal planning**: Content that helps plan or facilitate illegal activities.\n*   **Violence & gore**: Content that depicts, encourages, glorifies, or incites extreme, gratuitous violence against humans or animals.\n*   **Harassment & hate speech**: Content that is malicious toward, harasses, attacks, or incites hatred toward someone based on certain attributes such as race, religion, gender, disability, etc.\n*   **Regulated advice**: Content that provides regulated, unreliable, or otherwise potentially harmful advice on certain areas including investment, health, legal, and medical advice.\n*   **Security & privacy**: Tools that compromise user security and privacy or engage in fraudulent or unethical activities, including malware, phishing, exploits, or impersonation.\n*   **Sexual content**: Content intended to stimulate sexual arousal, including the creation, promotion, or solicitation of sexually explicit or suggestive content, including pornography.\n*   **Military & warfare**: Tools, methods, or technologies designed for enhancing, designing, implementing, or facilitating military operations and warfare, including civilian harm.\n*   **Real-time content**: Content that provides information to users in real time.\n*   **Deception**: Content that deceives users by presenting false information as true, including disinformation, misinformation, or misleading AI-generated content.\n*   **Emergent risk**: Content that causes other potential harms.\n\nPlease note that this list is not exhaustive and may evolve over time. For more information, see our [Acceptable Use Policy](https://ai.google.dev/terms/service-terms?authuser=1#acceptable-use). Our systems also limit or block some responses regardless of intent. For example, generative AI is generally designed to decline requests for copyrighted works, to disclose non-public information about people, or to aid in disinformation.\n\n### Blocked content\n\nSome prompts may be blocked due to likelihood of causing harm or misuse. In these cases, you might receive an explanation for why the content is blocked, along with any steps you can take to achieve an acceptable use of the model. You can try rephrasing your query to stay within the boundaries of acceptable use, but attempting to bypass our protections may violate our [Acceptable Use Policy](https://ai.google.dev/terms/service-terms?authuser=1#acceptable-use).\n\n### Limitations of generative AI\n\nGenerative AI technology is still evolving, and models like Gemini are limited in several ways.\n\n*   Models can return inaccurate, incomplete, harmful, or misleading information, including hallucinating content. Always double-check outputs.\n*   Models may lack understanding of user intent or context, potentially misinterpreting inputs or producing inappropriate or irrelevant responses.\n*   The models' training data may not fully reflect current information and knowledge. For example, Gemini 1.0 was trained on a large dataset of text and code from various sources, generally up until early 2023.\n*   Models may lack consistent reasoning abilities or specific domain expertise. While they can generate plausible-sounding responses, these may not always be accurate or well-reasoned.\n*   Models may reflect historical and social biases present in their training data.\n\nGiven these limitations, outputs should always be verified and used with human oversight, particularly in high-stakes or specialized domains.\n\n## Responsible development practices\n\nWe encourage developers using Generative AI to follow [responsible AI practices](https://ai.google.dev/docs/responsible_use) and keep users informed about the nature of AI-generated content, including [product design recommendations for generative AI products](https://material.io/blog/generative-ai-design-guidance).\n\n## Terms of service\n\nFor more information about the terms and conditions of using the Generative AI, please read the [Terms of Service](https://ai.google.dev/terms/service-terms).\n\n## Questions about our policy?\n\nIf you have questions about our policy, or if you've received a response that you believe violates our acceptable use policy or the principles above, please [contact us](https://support.google.com/generative-ai-developers/gethelp).\n\u0005End File\u0006# google/generative-ai-docs\n# Filter unstructured text with functions\n\nThis document demonstrates how to use the model to extract structured data from unstructured text with function calling. A structured function call means returning structured data in the expected format and schema, such as a user-defined function.\n\n## Set up your API key\n\nTo run these code examples, you need a valid Vertex AI API key. You can either set the key as an environment variable or pass it as a parameter when initializing the `GenerativeModel`.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Set your API key\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```js\nimport {\n  VertexAI,\n  HarmCategory,\n  HarmBlockThreshold,\n} from '@google-cloud/vertexai';\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Select the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n});\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\nTo use the Vertex AI API with cURL commands, you need to:\n\n1. [Create a service account and download a key file](https://cloud.google.com/iam/docs/service-accounts-create)\n2. Set your project ID\n3. Get an access token\n\n```bash\nexport PROJECT_ID=YOUR_PROJECT_ID\nexport MODEL_ID=gemini-1.5-pro\n\n# Obtain an access token\nexport TOKEN=$(gcloud auth print-access-token)\n```\n\n</section>\n\n## Define a function to filter results\n\nFunction calling allows you to create a contract for the model to follow when it returns information. In this example, we'll define a function schema that extracts specific information from unstructured text.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfilter_function = {\n    \"name\": \"extract_recipe_information\",\n    \"description\": \"Extract recipe information from text\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"title\": {\n                \"type\": \"string\",\n                \"description\": \"The title of the recipe\"\n            },\n            \"ingredients\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"string\"\n                },\n                \"description\": \"List of ingredients required for the recipe\"\n            },\n            \"cooking_time_minutes\": {\n                \"type\": \"integer\",\n                \"description\": \"Total cooking time in minutes\"\n            },\n            \"vegetarian\": {\n                \"type\": \"boolean\",\n                \"description\": \"Whether the recipe is vegetarian\"\n            }\n        },\n        \"required\": [\"title\", \"ingredients\"]\n    }\n}\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```js\nconst filterFunction = {\n  name: \"extract_recipe_information\",\n  description: \"Extract recipe information from text\",\n  parameters: {\n    type: \"object\",\n    properties: {\n      title: {\n        type: \"string\",\n        description: \"The title of the recipe\"\n      },\n      ingredients: {\n        type: \"array\",\n        items: {\n          type: \"string\"\n        },\n        description: \"List of ingredients required for the recipe\"\n      },\n      cooking_time_minutes: {\n        type: \"integer\",\n        description: \"Total cooking time in minutes\"\n      },\n      vegetarian: {\n        type: \"boolean\",\n        description: \"Whether the recipe is vegetarian\"\n      }\n    },\n    required: [\"title\", \"ingredients\"]\n  }\n};\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```json\n{\n  \"name\": \"extract_recipe_information\",\n  \"description\": \"Extract recipe information from text\",\n  \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"title\": {\n        \"type\": \"string\",\n        \"description\": \"The title of the recipe\"\n      },\n      \"ingredients\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": \"string\"\n        },\n        \"description\": \"List of ingredients required for the recipe\"\n      },\n      \"cooking_time_minutes\": {\n        \"type\": \"integer\",\n        \"description\": \"Total cooking time in minutes\"\n      },\n      \"vegetarian\": {\n        \"type\": \"boolean\",\n        \"description\": \"Whether the recipe is vegetarian\"\n      }\n    },\n    \"required\": [\"title\", \"ingredients\"]\n  }\n}\n```\n\n</section>\n\n## Process unstructured text\n\nNow, let's use the model with our function definition to extract structured information from a recipe text.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nrecipe_text = \"\"\"\nClassic Spaghetti Carbonara\n\nA traditional Italian pasta dish that's creamy and delicious! \nReady in just 25 minutes.\n\nIngredients:\n- 1 lb spaghetti\n- 8 oz pancetta or bacon, diced\n- 4 large eggs\n- 1 cup freshly grated Parmesan cheese\n- 4 garlic cloves, minced\n- Salt and freshly ground black pepper to taste\n- Fresh parsley for garnish\n\nInstructions:\n1. Bring a large pot of salted water to boil. Add spaghetti and cook until al dente.\n2. Meanwhile, in a large skillet, cook the pancetta until crispy, about 6-8 minutes.\n3. In a bowl, whisk together eggs, Parmesan, and plenty of black pepper.\n4. Drain pasta, reserving a little cooking water.\n5. Working quickly, add hot pasta to the skillet with pancetta, then remove from heat.\n6. Pour egg mixture over pasta, tossing quickly to coat the pasta without scrambling the eggs.\n7. Add a splash of pasta water if needed to create a creamy sauce.\n8. Serve immediately with extra Parmesan and chopped parsley.\n\"\"\"\n\nresponse = model.generate_content(\n    recipe_text,\n    generation_config={\"temperature\": 0},\n    tools=[filter_function]\n)\n\nprint(response.candidates[0].function_calls[0].args)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```js\nconst recipeText = `\nClassic Spaghetti Carbonara\n\nA traditional Italian pasta dish that's creamy and delicious! \nReady in just 25 minutes.\n\nIngredients:\n- 1 lb spaghetti\n- 8 oz pancetta or bacon, diced\n- 4 large eggs\n- 1 cup freshly grated Parmesan cheese\n- 4 garlic cloves, minced\n- Salt and freshly ground black pepper to taste\n- Fresh parsley for garnish\n\nInstructions:\n1. Bring a large pot of salted water to boil. Add spaghetti and cook until al dente.\n2. Meanwhile, in a large skillet, cook the pancetta until crispy, about 6-8 minutes.\n3. In a bowl, whisk together eggs, Parmesan, and plenty of black pepper.\n4. Drain pasta, reserving a little cooking water.\n5. Working quickly, add hot pasta to the skillet with pancetta, then remove from heat.\n6. Pour egg mixture over pasta, tossing quickly to coat the pasta without scrambling the eggs.\n7. Add a splash of pasta water if needed to create a creamy sauce.\n8. Serve immediately with extra Parmesan and chopped parsley.\n`;\n\nasync function extractRecipeInfo() {\n  const req = {\n    contents: [{ role: \"user\", parts: [{ text: recipeText }] }],\n    tools: [{ function_declarations: [filterFunction] }],\n    generation_config: { temperature: 0 }\n  };\n\n  const result = await generativeModel.generateContent(req);\n  const response = result.response;\n  \n  if (response.candidates && response.candidates[0].content.parts[0].functionCall) {\n    console.log(JSON.stringify(response.candidates[0].content.parts[0].functionCall.args, null, 2));\n  }\n}\n\nextractRecipeInfo();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\ncurl -X POST \\\n  -H \"Authorization: Bearer ${TOKEN}\" \\\n  -H \"Content-Type: application/json\" \\\n  https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/${MODEL_ID}:generateContent \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [\n          {\n            \"text\": \"Classic Spaghetti Carbonara\\n\\nA traditional Italian pasta dish that'\\''s creamy and delicious! \\nReady in just 25 minutes.\\n\\nIngredients:\\n- 1 lb spaghetti\\n- 8 oz pancetta or bacon, diced\\n- 4 large eggs\\n- 1 cup freshly grated Parmesan cheese\\n- 4 garlic cloves, minced\\n- Salt and freshly ground black pepper to taste\\n- Fresh parsley for garnish\\n\\nInstructions:\\n1. Bring a large pot of salted water to boil. Add spaghetti and cook until al dente.\\n2. Meanwhile, in a large skillet, cook the pancetta until crispy, about 6-8 minutes.\\n3. In a bowl, whisk together eggs, Parmesan, and plenty of black pepper.\\n4. Drain pasta, reserving a little cooking water.\\n5. Working quickly, add hot pasta to the skillet with pancetta, then remove from heat.\\n6. Pour egg mixture over pasta, tossing quickly to coat the pasta without scrambling the eggs.\\n7. Add a splash of pasta water if needed to create a creamy sauce.\\n8. Serve immediately with extra Parmesan and chopped parsley.\"\n          }\n        ]\n      }\n    ],\n    \"tools\": [\n      {\n        \"function_declarations\": [\n          {\n            \"name\": \"extract_recipe_information\",\n            \"description\": \"Extract recipe information from text\",\n            \"parameters\": {\n              \"type\": \"object\",\n              \"properties\": {\n                \"title\": {\n                  \"type\": \"string\",\n                  \"description\": \"The title of the recipe\"\n                },\n                \"ingredients\": {\n                  \"type\": \"array\",\n                  \"items\": {\n                    \"type\": \"string\"\n                  },\n                  \"description\": \"List of ingredients required for the recipe\"\n                },\n                \"cooking_time_minutes\": {\n                  \"type\": \"integer\",\n                  \"description\": \"Total cooking time in minutes\"\n                },\n                \"vegetarian\": {\n                  \"type\": \"boolean\",\n                  \"description\": \"Whether the recipe is vegetarian\"\n                }\n              },\n              \"required\": [\"title\", \"ingredients\"]\n            }\n          }\n        ]\n      }\n    ],\n    \"generation_config\": {\n      \"temperature\": 0\n    }\n  }'\n```\n\n</section>\n\n## Example output\n\nThe model will extract the structured information according to the function schema:\n\n```json\n{\n  \"title\": \"Classic Spaghetti Carbonara\",\n  \"ingredients\": [\n    \"1 lb spaghetti\",\n    \"8 oz pancetta or bacon, diced\",\n    \"4 large eggs\",\n    \"1 cup freshly grated Parmesan cheese\",\n    \"4 garlic cloves, minced\",\n    \"Salt and freshly ground black pepper to taste\",\n    \"Fresh parsley for garnish\"\n  ],\n  \"cooking_time_minutes\": 25,\n  \"vegetarian\": false\n}\n```\n\n## Extract structured information from multiple documents\n\nYou can also use this approach to extract information from multiple documents at once.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\n# Define a function for extracting movie information\nmovie_function = {\n    \"name\": \"extract_movie_info\",\n    \"description\": \"Extract information about movies from text\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"movies\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"title\": {\n                            \"type\": \"string\",\n                            \"description\": \"The title of the movie\"\n                        },\n                        \"director\": {\n                            \"type\": \"string\",\n                            \"description\": \"The director of the movie\"\n                        },\n                        \"year\": {\n                            \"type\": \"integer\",\n                            \"description\": \"The year the movie was released\"\n                        },\n                        \"actors\": {\n                            \"type\": \"array\",\n                            \"items\": {\n                                \"type\": \"string\"\n                            },\n                            \"description\": \"The main actors in the movie\"\n                        }\n                    },\n                    \"required\": [\"title\"]\n                }\n            }\n        },\n        \"required\": [\"movies\"]\n    }\n}\n\n# Sample text containing information about multiple movies\nmovie_text = \"\"\"\nThe Godfather (1972) was directed by Francis Ford Coppola and stars Marlon Brando, Al Pacino, and James Caan. It's considered one of the greatest films ever made.\n\nPulp Fiction came out in 1994 and was Quentin Tarantino's breakout film. The movie features John Travolta, Samuel L. Jackson, and Uma Thurman in career-defining roles.\n\nChristopher Nolan's Inception (2010) is a mind-bending thriller with Leonardo DiCaprio, Joseph Gordon-Levitt, and Ellen Page navigating dreams within dreams.\n\nThe Shawshank Redemption was released in 1994, directed by Frank Darabont, and stars Tim Robbins and Morgan Freeman. It's based on a Stephen King novella.\n\"\"\"\n\nresponse = model.generate_content(\n    movie_text,\n    generation_config={\"temperature\": 0},\n    tools=[movie_function]\n)\n\nprint(response.candidates[0].function_calls[0].args)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```js\n// Define a function for extracting movie information\nconst movieFunction = {\n  name: \"extract_movie_info\",\n  description: \"Extract information about movies from text\",\n  parameters: {\n    type: \"object\",\n    properties: {\n      movies: {\n        type: \"array\",\n        items: {\n          type: \"object\",\n          properties: {\n            title: {\n              type: \"string\",\n              description: \"The title of the movie\"\n            },\n            director: {\n              type: \"string\",\n              description: \"The director of the movie\"\n            },\n            year: {\n              type: \"integer\",\n              description: \"The year the movie was released\"\n            },\n            actors: {\n              type: \"array\",\n              items: {\n                type: \"string\"\n              },\n              description: \"The main actors in the movie\"\n            }\n          },\n          required: [\"title\"]\n        }\n      }\n    },\n    required: [\"movies\"]\n  }\n};\n\n// Sample text containing information about multiple movies\nconst movieText = `\nThe Godfather (1972) was directed by Francis Ford Coppola and stars Marlon Brando, Al Pacino, and James Caan. It's considered one of the greatest films ever made.\n\nPulp Fiction came out in 1994 and was Quentin Tarantino's breakout film. The movie features John Travolta, Samuel L. Jackson, and Uma Thurman in career-defining roles.\n\nChristopher Nolan's Inception (2010) is a mind-bending thriller with Leonardo DiCaprio, Joseph Gordon-Levitt, and Ellen Page navigating dreams within dreams.\n\nThe Shawshank Redemption was released in 1994, directed by Frank Darabont, and stars Tim Robbins and Morgan Freeman. It's based on a Stephen King novella.\n`;\n\nasync function extractMovieInfo() {\n  const req = {\n    contents: [{ role: \"user\", parts: [{ text: movieText }] }],\n    tools: [{ function_declarations: [movieFunction] }],\n    generation_config: { temperature: 0 }\n  };\n\n  const result = await generativeModel.generateContent(req);\n  const response = result.response;\n  \n  if (response.candidates && response.candidates[0].content.parts[0].functionCall) {\n    console.log(JSON.stringify(response.candidates[0].content.parts[0].functionCall.args, null, 2));\n  }\n}\n\nextractMovieInfo();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\ncurl -X POST \\\n  -H \"Authorization: Bearer ${TOKEN}\" \\\n  -H \"Content-Type: application/json\" \\\n  https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/${MODEL_ID}:generateContent \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [\n          {\n            \"text\": \"The Godfather (1972) was directed by Francis Ford Coppola and stars Marlon Brando, Al Pacino, and James Caan. It'\\''s considered one of the greatest films ever made.\\n\\nPulp Fiction came out in 1994 and was Quentin Tarantino'\\''s breakout film. The movie features John Travolta, Samuel L. Jackson, and Uma Thurman in career-defining roles.\\n\\nChristopher Nolan'\\''s Inception (2010) is a mind-bending thriller with Leonardo DiCaprio, Joseph Gordon-Levitt, and Ellen Page navigating dreams within dreams.\\n\\nThe Shawshank Redemption was released in 1994, directed by Frank Darabont, and stars Tim Robbins and Morgan Freeman. It'\\''s based on a Stephen King novella.\"\n          }\n        ]\n      }\n    ],\n    \"tools\": [\n      {\n        \"function_declarations\": [\n          {\n            \"name\": \"extract_movie_info\",\n            \"description\": \"Extract information about movies from text\",\n            \"parameters\": {\n              \"type\": \"object\",\n              \"properties\": {\n                \"movies\": {\n                  \"type\": \"array\",\n                  \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"title\": {\n                        \"type\": \"string\",\n                        \"description\": \"The title of the movie\"\n                      },\n                      \"director\": {\n                        \"type\": \"string\",\n                        \"description\": \"The director of the movie\"\n                      },\n                      \"year\": {\n                        \"type\": \"integer\",\n                        \"description\": \"The year the movie was released\"\n                      },\n                      \"actors\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                          \"type\": \"string\"\n                        },\n                        \"description\": \"The main actors in the movie\"\n                      }\n                    },\n                    \"required\": [\"title\"]\n                  }\n                }\n              },\n              \"required\": [\"movies\"]\n            }\n          }\n        ]\n      }\n    ],\n    \"generation_config\": {\n      \"temperature\": 0\n    }\n  }'\n```\n\n</section>\n\n## Example output for multiple documents\n\nThe output will be structured as an array of movie objects:\n\n```json\n{\n  \"movies\": [\n    {\n      \"title\": \"The Godfather\",\n      \"director\": \"Francis Ford Coppola\",\n      \"year\": 1972,\n      \"actors\": [\"Marlon Brando\", \"Al Pacino\", \"James Caan\"]\n    },\n    {\n      \"title\": \"Pulp Fiction\",\n      \"director\": \"Quentin Tarantino\",\n      \"year\": 1994,\n      \"actors\": [\"John Travolta\", \"Samuel L. Jackson\", \"Uma Thurman\"]\n    },\n    {\n      \"title\": \"Inception\",\n      \"director\": \"Christopher Nolan\",\n      \"year\": 2010,\n      \"actors\": [\"Leonardo DiCaprio\", \"Joseph Gordon-Levitt\", \"Ellen Page\"]\n    },\n    {\n      \"title\": \"The Shawshank Redemption\",\n      \"director\": \"Frank Darabont\",\n      \"year\": 1994,\n      \"actors\": [\"Tim Robbins\", \"Morgan Freeman\"]\n    }\n  ]\n}\n```\n\n## Conclusion\n\nFunction calling is a powerful technique for extracting structured information from unstructured text. It allows you to define exactly what information you want and in what format, making it easier to process and use the data in your applications.\n\nYou can customize the function schema to extract any type of structured data you need, such as:\n\n- Contact information from emails or documents\n- Event details from descriptions\n- Product information from catalogs\n- Financial data from reports\n- Medical information from clinical notes\n\nThis approach is particularly useful when you need to process large volumes of text and extract consistent, structured data for further analysis or storage.\n\u0005End File\u0006# google/generative-ai-docs\n# Vertex AI SDK for Python setup\n\nVertex AI SDK for Python is a Python library that provides an easy-to-use interface for accessing Vertex AI's models, including Gemini and PaLM.\n\n## Requirements\n\n* [Python](https://www.python.org/) 3.9+\n* When working with Vertex AI SDK for Python, you need to create a Google Cloud Platform (GCP) project. See the [Google Cloud console documentation](https://cloud.google.com/resource-manager/docs/creating-managing-projects#creating_a_project) for more information.\n* Set up authentication with [Application Default Credentials](https://cloud.google.com/docs/authentication/application-default-credentials) (ADC). Export your credentials as environment variables.\n* [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com) in your project. You must enable the Vertex AI API in the project containing your resources.\n\n## Install the Vertex AI SDK for Python\n\nTo use Gemini through Vertex AI in a Python environment, install the Vertex AI SDK for Python. You can also install additional features to enhance your experience.\n\n### Install the basic package\n\n```bash\npip install google-cloud-aiplatform>=1.38\n```\n\n### Additional packages\n\nYou can install optional dependencies to enhance Vertex AI SDK for Python:\n\n```bash\n# Install TensorFlow\npip install \"google-cloud-aiplatform[tensorflow]>=1.38\"\n\n# Install samples\npip install \"google-cloud-aiplatform[samples]>=1.38\"\n\n# Install packages used in Vertex AI Vizier tutorials\npip install \"google-cloud-aiplatform[vizier]>=1.38\"\n\n# Install all additional packages\npip install \"google-cloud-aiplatform[full]>=1.38\"\n```\n\n## Authentication\n\nTo make API calls to Vertex AI, you need to authenticate. The simplest way is to use Application Default Credentials (ADC) which are automatically provided by Google Cloud environments like Google Compute Engine, Google Kubernetes Engine, or Google Cloud Functions.\n\n### Local authentication\n\nFor local development, you can authenticate using the Google Cloud CLI:\n\n```bash\ngcloud auth application-default login\n```\n\nThis creates credentials in a well-known location used by client libraries.\n\n### Service account authentication\n\nFor applications running in production, you should use a service account:\n\n1. [Create a service account](https://cloud.google.com/iam/docs/service-accounts-create#creating)\n2. [Grant roles](https://cloud.google.com/iam/docs/granting-changing-revoking-access#grant-single-role) to the service account\n3. [Create a service account key](https://cloud.google.com/iam/docs/keys-create-delete#creating)\n\nThen, set the environment variable to point to your service account key file:\n\n```bash\nexport GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/service-account-key.json\"\n```\n\n## Next steps\n\nNow that you've set up the Vertex AI SDK for Python, you can:\n\n- [Get started with Gemini](../reference/gemini-models.md)\n- [Generate text, images, and chat with Gemini models](../tutorials/introduction.md)\n\n## Troubleshooting\n\nIf you encounter issues when setting up or using Vertex AI SDK for Python:\n\n- Ensure you have enabled the Vertex AI API in your Google Cloud project.\n- Verify your authentication credentials are correctly set up.\n- Check your Python version (3.9 or higher is required).\n- Make sure you have the latest version of the SDK installed.\n\nFor more help, visit the [Vertex AI support page](https://cloud.google.com/vertex-ai/docs/support) or [Google Cloud Support](https://cloud.google.com/support).\n\u0005End File\u0006# site/en/vertex/setup/langchain-installation.md\n# Set up the LangChain SDK to use Vertex AI\n\nThe LangChain SDK is an open-source framework that simplifies the development of applications using large language models (LLMs). It provides a unified interface to interact with various LLMs, including Google's Vertex AI models. This guide will help you set up LangChain to work with Vertex AI.\n\n## Prerequisites\n\nBefore you begin, ensure you have:\n\n- Python 3.9 or higher installed\n- A Google Cloud account with the Vertex AI API enabled\n- [Google Cloud CLI](https://cloud.google.com/sdk/docs/install) installed\n- Basic familiarity with Python and LangChain concepts\n\n## Install LangChain and Vertex AI packages\n\nTo use LangChain with Vertex AI, install the following packages:\n\n```bash\npip install langchain google-cloud-aiplatform\n```\n\nFor additional features, you may want to install:\n\n```bash\npip install langchain-google-vertexai\n```\n\n## Set up authentication\n\nTo authenticate with Google Cloud and use Vertex AI services, you need to set up authentication credentials.\n\n### Local development authentication\n\nFor local development, use the Google Cloud CLI to authenticate:\n\n```bash\ngcloud auth application-default login\n```\n\n### Service account authentication\n\nFor production environments, use a service account:\n\n1. Create a service account in the Google Cloud Console\n2. Download the JSON key file\n3. Set the environment variable to point to the key file:\n\n```bash\nexport GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/your-service-account-key.json\"\n```\n\n## Configure your environment\n\nSet the following environment variables to specify your Google Cloud project and region:\n\n```bash\nexport GOOGLE_CLOUD_PROJECT=\"your-project-id\"\nexport GOOGLE_CLOUD_REGION=\"us-central1\"  # Choose an appropriate region\n```\n\n## Basic LangChain example with Vertex AI\n\nHere's a simple example of using LangChain with Vertex AI's Gemini model:\n\n```python\nfrom langchain_google_vertexai import ChatVertexAI\nfrom langchain.schema import HumanMessage, SystemMessage\n\n# Initialize the model\nmodel = ChatVertexAI(\n    model_name=\"gemini-1.5-pro\",\n    project=PROJECT_ID,  # Your Google Cloud Project ID\n    location=\"us-central1\",  # Your Google Cloud Location\n    max_output_tokens=1024,\n    temperature=0.2,\n    top_p=0.95,\n)\n\n# Example conversation\nmessages = [\n    SystemMessage(content=\"You are a helpful AI assistant.\"),\n    HumanMessage(content=\"Explain quantum computing in simple terms\")\n]\n\n# Get the response\nresponse = model.invoke(messages)\nprint(response.content)\n```\n\n## Advanced usage with LangChain and Vertex AI\n\n### Using Gemini with LangChain\n\n```python\nfrom langchain_google_vertexai import ChatVertexAI\nfrom langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\n\n# Initialize the model\nchat = ChatVertexAI(\n    model_name=\"gemini-1.5-pro\",\n    temperature=0.7,\n)\n\n# Create a conversation chain with memory\nconversation = ConversationChain(\n    llm=chat,\n    memory=ConversationBufferMemory(),\n    verbose=True\n)\n\n# Start a conversation\nresponse1 = conversation.invoke({\"input\": \"Hi, I'm learning about machine learning.\"})\nprint(response1[\"response\"])\n\n# Continue the conversation\nresponse2 = conversation.invoke({\"input\": \"Can you explain neural networks briefly?\"})\nprint(response2[\"response\"])\n```\n\n### Using LangChain with Vertex AI embeddings\n\n```python\nfrom langchain_google_vertexai import VertexAIEmbeddings\nfrom langchain.vectorstores import Chroma\n\n# Initialize embeddings\nembeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@latest\")\n\n# Create a vector store\ntexts = [\n    \"Artificial intelligence is transforming industries.\",\n    \"Machine learning is a subset of AI.\",\n    \"Neural networks are inspired by the human brain.\",\n    \"Deep learning uses multiple layers of neural networks.\"\n]\n\nvector_store = Chroma.from_texts(texts, embeddings)\n\n# Search for similar documents\nquery = \"How does AI relate to human intelligence?\"\nresults = vector_store.similarity_search(query)\n\nprint(\"Search results:\")\nfor doc in results:\n    print(f\"- {doc.page_content}\")\n```\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. **Authentication errors**: Ensure your credentials are correctly set up and you have the necessary permissions.\n\n2. **API not enabled**: Make sure the Vertex AI API is enabled in your Google Cloud project.\n\n3. **Version compatibility**: Check that your installed packages are compatible. Use:\n   ```bash\n   pip list | grep -E 'langchain|google-cloud'\n   ```\n\n4. **Quota limits**: If you hit quota limits, request an increase through the Google Cloud Console.\n\n5. **Region availability**: Ensure the model you're trying to use is available in your specified region.\n\n## Next steps\n\nNow that you've set up LangChain with Vertex AI, you can:\n\n- Create more complex chains and agents using LangChain's components\n- Explore Vertex AI's specialized models for different use cases\n- Build applications that combine multiple AI capabilities\n\nFor more information, refer to:\n- [LangChain documentation](https://python.langchain.com/docs/get_started/introduction)\n- [Vertex AI documentation](https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform)\n- [LangChain Google Vertex AI integration documentation](https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm)\n\u0005End File\u0006# Experiment with different models and parameters\n\nIn this tutorial, you'll experiment with different models, generation parameters, and contexts to understand how they impact the quality of Gemini model responses.\n\n## Set up API access\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```js\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Set your project ID and region\nexport PROJECT_ID=\"YOUR_PROJECT_ID\"\nexport LOCATION=\"us-central1\"\n\n# Get authentication token\nexport TOKEN=$(gcloud auth print-access-token)\n```\n\n</section>\n\n## Try different models\n\nLet's explore how different models respond to the same prompt.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\n# Define a common prompt\nprompt = \"Explain the concept of machine learning to a 10-year old child.\"\n\n# Initialize different models\ngemini_pro = GenerativeModel(\"gemini-1.5-pro\")\ngemini_flash = GenerativeModel(\"gemini-1.5-flash\")\n\n# Get responses from each model\ngemini_pro_response = gemini_pro.generate_content(prompt)\ngemini_flash_response = gemini_flash.generate_content(prompt)\n\n# Print responses\nprint(\"Gemini 1.5 Pro Response:\")\nprint(gemini_pro_response.text)\nprint(\"\\nGemini 1.5 Flash Response:\")\nprint(gemini_flash_response.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```js\nasync function compareModels() {\n  const prompt = \"Explain the concept of machine learning to a 10-year old child.\";\n  \n  // Initialize different models\n  const geminiPro = vertex_ai.getGenerativeModel({\n    model: \"gemini-1.5-pro\",\n  });\n  \n  const geminiFlash = vertex_ai.getGenerativeModel({\n    model: \"gemini-1.5-flash\",\n  });\n  \n  // Get responses from each model\n  const proResponse = await geminiPro.generateContent(prompt);\n  const flashResponse = await geminiFlash.generateContent(prompt);\n  \n  // Print responses\n  console.log(\"Gemini 1.5 Pro Response:\");\n  console.log(proResponse.response.text());\n  console.log(\"\\nGemini 1.5 Flash Response:\");\n  console.log(flashResponse.response.text());\n}\n\ncompareModels();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Define the prompt\nPROMPT=\"Explain the concept of machine learning to a 10-year old child.\"\n\n# Function to call API and extract response\ncall_model() {\n  local model=$1\n  local response=$(curl -s -X POST \\\n    -H \"Authorization: Bearer $TOKEN\" \\\n    -H \"Content-Type: application/json\" \\\n    \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${model}:generateContent\" \\\n    -d \"{\n      \\\"contents\\\": [{\n        \\\"role\\\": \\\"user\\\",\n        \\\"parts\\\": [{\\\"text\\\": \\\"$PROMPT\\\"}]\n      }]\n    }\")\n  \n  # Extract the text response\n  echo \"$response\" | jq -r '.candidates[0].content.parts[0].text'\n}\n\n# Compare different models\necho \"Gemini 1.5 Pro Response:\"\ncall_model \"gemini-1.5-pro\"\n\necho -e \"\\nGemini 1.5 Flash Response:\"\ncall_model \"gemini-1.5-flash\"\n```\n\n</section>\n\n## Experiment with temperature\n\nTemperature controls the randomness of model responses. Let's see how different temperature values affect the output.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\n# Define a creative prompt\ncreative_prompt = \"Write a short poem about artificial intelligence.\"\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Try different temperature values\nfor temp in [0.0, 0.4, 0.9]:\n    print(f\"\\nTemperature: {temp}\")\n    response = model.generate_content(\n        creative_prompt,\n        generation_config={\"temperature\": temp}\n    )\n    print(response.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```js\nasync function testTemperatures() {\n  // Define a creative prompt\n  const creativePrompt = \"Write a short poem about artificial intelligence.\";\n  \n  // Initialize the model\n  const model = vertex_ai.getGenerativeModel({\n    model: \"gemini-1.5-pro\",\n  });\n  \n  // Try different temperature values\n  const temperatures = [0.0, 0.4, 0.9];\n  \n  for (const temp of temperatures) {\n    console.log(`\\nTemperature: ${temp}`);\n    \n    const response = await model.generateContent({\n      contents: [{ role: \"user\", parts: [{ text: creativePrompt }] }],\n      generationConfig: {\n        temperature: temp\n      }\n    });\n    \n    console.log(response.response.text());\n  }\n}\n\ntestTemperatures();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Define the prompt\nCREATIVE_PROMPT=\"Write a short poem about artificial intelligence.\"\n\n# Try different temperature values\nfor TEMP in 0.0 0.4 0.9; do\n  echo -e \"\\nTemperature: $TEMP\"\n  \n  curl -s -X POST \\\n    -H \"Authorization: Bearer $TOKEN\" \\\n    -H \"Content-Type: application/json\" \\\n    \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-1.5-pro:generateContent\" \\\n    -d \"{\n      \\\"contents\\\": [{\n        \\\"role\\\": \\\"user\\\",\n        \\\"parts\\\": [{\\\"text\\\": \\\"$CREATIVE_PROMPT\\\"}]\n      }],\n      \\\"generationConfig\\\": {\n        \\\"temperature\\\": $TEMP\n      }\n    }\" | jq -r '.candidates[0].content.parts[0].text'\ndone\n```\n\n</section>\n\n## Test top-k and top-p parameters\n\nTop-k and top-p are parameters that control token selection during generation.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\n# Define a prompt\ndecision_prompt = \"List 5 potential applications of AI in healthcare.\"\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Default parameters\nresponse_default = model.generate_content(\n    decision_prompt,\n    generation_config={\"temperature\": 0.7}\n)\n\n# With top-k parameter\nresponse_top_k = model.generate_content(\n    decision_prompt,\n    generation_config={\"temperature\": 0.7, \"top_k\": 10}\n)\n\n# With top-p parameter\nresponse_top_p = model.generate_content(\n    decision_prompt,\n    generation_config={\"temperature\": 0.7, \"top_p\": 0.5}\n)\n\n# With both parameters\nresponse_both = model.generate_content(\n    decision_prompt,\n    generation_config={\"temperature\": 0.7, \"top_k\": 10, \"top_p\": 0.5}\n)\n\nprint(\"Default parameters:\")\nprint(response_default.text)\nprint(\"\\nWith top-k=10:\")\nprint(response_top_k.text)\nprint(\"\\nWith top-p=0.5:\")\nprint(response_top_p.text)\nprint(\"\\nWith top-k=10 and top-p=0.5:\")\nprint(response_both.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```js\nasync function testTokenSelectionParams() {\n  // Define a prompt\n  const decisionPrompt = \"List 5 potential applications of AI in healthcare.\";\n  \n  // Initialize the model\n  const model = vertex_ai.getGenerativeModel({\n    model: \"gemini-1.5-pro\",\n  });\n  \n  // Default parameters\n  const responseDefault = await model.generateContent({\n    contents: [{ role: \"user\", parts: [{ text: decisionPrompt }] }],\n    generationConfig: {\n      temperature: 0.7\n    }\n  });\n  \n  // With top-k parameter\n  const responseTopK = await model.generateContent({\n    contents: [{ role: \"user\", parts: [{ text: decisionPrompt }] }],\n    generationConfig: {\n      temperature: 0.7,\n      topK: 10\n    }\n  });\n  \n  // With top-p parameter\n  const responseTopP = await model.generateContent({\n    contents: [{ role: \"user\", parts: [{ text: decisionPrompt }] }],\n    generationConfig: {\n      temperature: 0.7,\n      topP: 0.5\n    }\n  });\n  \n  // With both parameters\n  const responseBoth = await model.generateContent({\n    contents: [{ role: \"user\", parts: [{ text: decisionPrompt }] }],\n    generationConfig: {\n      temperature: 0.7,\n      topK: 10,\n      topP: 0.5\n    }\n  });\n  \n  console.log(\"Default parameters:\");\n  console.log(responseDefault.response.text());\n  console.log(\"\\nWith top-k=10:\");\n  console.log(responseTopK.response.text());\n  console.log(\"\\nWith top-p=0.5:\");\n  console.log(responseTopP.response.text());\n  console.log(\"\\nWith top-k=10 and top-p=0.5:\");\n  console.log(responseBoth.response.text());\n}\n\ntestTokenSelectionParams();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Define the prompt\nDECISION_PROMPT=\"List 5 potential applications of AI in healthcare.\"\n\n# Function to call API with different parameters\ncall_with_params() {\n  local params=$1\n  local description=$2\n  \n  echo -e \"\\n$description:\"\n  \n  curl -s -X POST \\\n    -H \"Authorization: Bearer $TOKEN\" \\\n    -H \"Content-Type: application/json\" \\\n    \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-1.5-pro:generateContent\" \\\n    -d \"{\n      \\\"contents\\\": [{\n        \\\"role\\\": \\\"user\\\",\n        \\\"parts\\\": [{\\\"text\\\": \\\"$DECISION_PROMPT\\\"}]\n      }],\n      \\\"generationConfig\\\": $params\n    }\" | jq -r '.candidates[0].content.parts[0].text'\n}\n\n# Test different parameter combinations\ncall_with_params '{\"temperature\": 0.7}' \"Default parameters\"\ncall_with_params '{\"temperature\": 0.7, \"topK\": 10}' \"With top-k=10\"\ncall_with_params '{\"temperature\": 0.7, \"topP\": 0.5}' \"With top-p=0.5\"\ncall_with_params '{\"temperature\": 0.7, \"topK\": 10, \"topP\": 0.5}' \"With top-k=10 and top-p=0.5\"\n```\n\n</section>\n\n## Explore candidate count\n\nGenerating multiple candidates allows the model to provide various responses to the same prompt.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\n# Define a prompt with multiple possible answers\ncreative_prompt = \"Suggest a name for a tech startup focused on sustainable energy.\"\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Generate multiple candidates\nresponse = model.generate_content(\n    creative_prompt,\n    generation_config={\n        \"temperature\": 0.9,\n        \"candidate_count\": 3\n    }\n)\n\n# Print all candidates\nfor i, candidate in enumerate(response.candidates):\n    print(f\"\\nCandidate {i+1}:\")\n    print(candidate.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```js\nasync function generateMultipleCandidates() {\n  // Define a prompt with multiple possible answers\n  const creativePrompt = \"Suggest a name for a tech startup focused on sustainable energy.\";\n  \n  // Initialize the model\n  const model = vertex_ai.getGenerativeModel({\n    model: \"gemini-1.5-pro\",\n  });\n  \n  // Generate multiple candidates\n  const response = await model.generateContent({\n    contents: [{ role: \"user\", parts: [{ text: creativePrompt }] }],\n    generationConfig: {\n      temperature: 0.9,\n      candidateCount: 3\n    }\n  });\n  \n  // Print all candidates\n  response.response.candidates.forEach((candidate, i) => {\n    console.log(`\\nCandidate ${i+1}:`);\n    console.log(candidate.content.parts[0].text);\n  });\n}\n\ngenerateMultipleCandidates();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Define the prompt\nCREATIVE_PROMPT=\"Suggest a name for a tech startup focused on sustainable energy.\"\n\n# Generate multiple candidates\nresponse=$(curl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-1.5-pro:generateContent\" \\\n  -d \"{\n    \\\"contents\\\": [{\n      \\\"role\\\": \\\"user\\\",\n      \\\"parts\\\": [{\\\"text\\\": \\\"$CREATIVE_PROMPT\\\"}]\n    }],\n    \\\"generationConfig\\\": {\n      \\\"temperature\\\": 0.9,\n      \\\"candidateCount\\\": 3\n    }\n  }\")\n\n# Parse and display candidates\necho \"$response\" | jq -r '.candidates[] | \"\\nCandidate:\\n\" + .content.parts[0].text'\n```\n\n</section>\n\n## Control output length\n\nYou can control the length of the model's response using max_output_tokens.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\n# Define a prompt that could generate a long response\nlong_prompt = \"Explain the history and impact of the internet.\"\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Generate responses with different max output tokens\nfor tokens in [50, 200, 500]:\n    print(f\"\\nMax output tokens: {tokens}\")\n    response = model.generate_content(\n        long_prompt,\n        generation_config={\"max_output_tokens\": tokens}\n    )\n    print(response.text)\n    print(f\"Character count: {len(response.text)}\")\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```js\nasync function testOutputLength() {\n  // Define a prompt that could generate a long response\n  const longPrompt = \"Explain the history and impact of the internet.\";\n  \n  // Initialize the model\n  const model = vertex_ai.getGenerativeModel({\n    model: \"gemini-1.5-pro\",\n  });\n  \n  // Generate responses with different max output tokens\n  const tokenLimits = [50, 200, 500];\n  \n  for (const tokens of tokenLimits) {\n    console.log(`\\nMax output tokens: ${tokens}`);\n    \n    const response = await model.generateContent({\n      contents: [{ role: \"user\", parts: [{ text: longPrompt }] }],\n      generationConfig: {\n        maxOutputTokens: tokens\n      }\n    });\n    \n    const text = response.response.text();\n    console.log(text);\n    console.log(`Character count: ${text.length}`);\n  }\n}\n\ntestOutputLength();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Define the prompt\nLONG_PROMPT=\"Explain the history and impact of the internet.\"\n\n# Test different token limits\nfor TOKENS in 50 200 500; do\n  echo -e \"\\nMax output tokens: $TOKENS\"\n  \n  response=$(curl -s -X POST \\\n    -H \"Authorization: Bearer $TOKEN\" \\\n    -H \"Content-Type: application/json\" \\\n    \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-1.5-pro:generateContent\" \\\n    -d \"{\n      \\\"contents\\\": [{\n        \\\"role\\\": \\\"user\\\",\n        \\\"parts\\\": [{\\\"text\\\": \\\"$LONG_PROMPT\\\"}]\n      }],\n      \\\"generationConfig\\\": {\n        \\\"maxOutputTokens\\\": $TOKENS\n      }\n    }\")\n  \n  text=$(echo \"$response\" | jq -r '.candidates[0].content.parts[0].text')\n  echo \"$text\"\n  echo \"Character count: $(echo -n \"$text\" | wc -c)\"\ndone\n```\n\n</section>\n\n## Experiment with system instructions\n\nSystem instructions help set the tone, style, and behavior of the model.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\n# Define a prompt\nprompt = \"Explain the concept of neural networks.\"\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# With different system instructions\nsystem_instructions = [\n    \"You are a university professor explaining complex topics to graduate students.\",\n    \"You are teaching a class of 10-year-olds and need to make concepts simple and fun.\",\n    \"You are writing content for a technical blog and should be precise and include relevant technical details.\"\n]\n\nfor instruction in system_instructions:\n    print(f\"\\nSystem instruction: {instruction}\")\n    response = model.generate_content(\n        [instruction, prompt]\n    )\n    print(response.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```js\nasync function testSystemInstructions() {\n  // Define a prompt\n  const prompt = \"Explain the concept of neural networks.\";\n  \n  // Initialize the model\n  const model = vertex_ai.getGenerativeModel({\n    model: \"gemini-1.5-pro\",\n  });\n  \n  // With different system instructions\n  const systemInstructions = [\n    \"You are a university professor explaining complex topics to graduate students.\",\n    \"You are teaching a class of 10-year-olds and need to make concepts simple and fun.\",\n    \"You are writing content for a technical blog and should be precise and include relevant technical details.\"\n  ];\n  \n  for (const instruction of systemInstructions) {\n    console.log(`\\nSystem instruction: ${instruction}`);\n    \n    const response = await model.generateContent({\n      contents: [\n        { role: \"system\", parts: [{ text: instruction }] },\n        { role: \"user\", parts: [{ text: prompt }] }\n      ]\n    });\n    \n    console.log(response.response.text());\n  }\n}\n\ntestSystemInstructions();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Define the prompt\nPROMPT=\"Explain the concept of neural networks.\"\n\n# Array of system instructions\nINSTRUCTIONS=(\n  \"You are a university professor explaining complex topics to graduate students.\"\n  \"You are teaching a class of 10-year-olds and need to make concepts simple and fun.\"\n  \"You are writing content for a technical blog and should be precise and include relevant technical details.\"\n)\n\n# Test different system instructions\nfor INSTRUCTION in \"${INSTRUCTIONS[@]}\"; do\n  echo -e \"\\nSystem instruction: $INSTRUCTION\"\n  \n  curl -s -X POST \\\n    -H \"Authorization: Bearer $TOKEN\" \\\n    -H \"Content-Type: application/json\" \\\n    \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-1.5-pro:generateContent\" \\\n    -d \"{\n      \\\"contents\\\": [\n        {\n          \\\"role\\\": \\\"system\\\",\n          \\\"parts\\\": [{\\\"text\\\": \\\"$INSTRUCTION\\\"}]\n        },\n        {\n          \\\"role\\\": \\\"user\\\",\n          \\\"parts\\\": [{\\\"text\\\": \\\"$PROMPT\\\"}]\n        }\n      ]\n    }\" | jq -r '.candidates[0].content.parts[0].text'\ndone\n```\n\n</section>\n\n## Test with context\n\nAdding context to your prompts can significantly improve the relevance and accuracy of responses.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\n# Define a prompt that needs context\nprompt = \"Summarize the main points.\"\n\n# Define context\ncontext = \"\"\"\nArtificial General Intelligence (AGI) refers to highly autonomous systems that outperform humans at most economically valuable work. \nUnlike narrow AI systems designed for specific tasks, AGI would possess a broad range of capabilities similar to human intelligence, \nincluding reasoning, problem-solving, and the ability to learn new tasks without explicit programming. \nKey challenges in developing AGI include creating systems that can generalize knowledge across domains, maintain safety alignments with human values, \nand address the profound ethical implications of such technology. While some experts believe AGI could be developed within decades, \nothers argue that human-level artificial general intelligence may be much further away or even unattainable.\n\"\"\"\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Without context\nresponse_no_context = model.generate_content(prompt)\n\n# With context\nresponse_with_context = model.generate_content([context, prompt])\n\nprint(\"Without context:\")\nprint(response_no_context.text)\nprint(\"\\nWith context:\")\nprint(response_with_context.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```js\nasync function testWithContext() {\n  // Define a prompt that needs context\n  const prompt = \"Summarize the main points.\";\n  \n  // Define context\n  const context = `\n  Artificial General Intelligence (AGI) refers to highly autonomous systems that outperform humans at most economically valuable work. \n  Unlike narrow AI systems designed for specific tasks, AGI would possess a broad range of capabilities similar to human intelligence, \n  including reasoning, problem-solving, and the ability to learn new tasks without explicit programming. \n  Key challenges in developing AGI include creating systems that can generalize knowledge across domains, maintain safety alignments with human values, \n  and address the profound ethical implications of such technology. While some experts believe AGI could be developed within decades, \n  others argue that human-level artificial general intelligence may be much further away or even unattainable.\n  `;\n  \n  // Initialize the model\n  const model = vertex_ai.getGenerativeModel({\n    model: \"gemini-1.5-pro\",\n  });\n  \n  // Without context\n  const responseNoContext = await model.generateContent({\n    contents: [{ role: \"user\", parts: [{ text: prompt }] }]\n  });\n  \n  // With context\n  const responseWithContext = await model.generateContent({\n    contents: [{ \n      role: \"user\", \n      parts: [\n        { text: context },\n        { text: prompt }\n      ] \n    }]\n  });\n  \n  console.log(\"Without context:\");\n  console.log(responseNoContext.response.text());\n  console.log(\"\\nWith context:\");\n  console.log(responseWithContext.response.text());\n}\n\ntestWithContext();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Define the prompt and context\nPROMPT=\"Summarize the main points.\"\nCONTEXT=\"Artificial General Intelligence (AGI) refers to highly autonomous systems that outperform humans at most economically valuable work. Unlike narrow AI systems designed for specific tasks, AGI would possess a broad range of capabilities similar to human intelligence, including reasoning, problem-solving, and the ability to learn new tasks without explicit programming. Key challenges in developing AGI include creating systems that can generalize knowledge across domains, maintain safety alignments with human values, and address the profound ethical implications of such technology. While some experts believe AGI could be developed within decades, others argue that human-level artificial general intelligence may be much further away or even unattainable.\"\n\n# Without context\necho -e \"\\nWithout context:\"\ncurl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-1.5-pro:generateContent\" \\\n  -d \"{\n    \\\"contents\\\": [{\n      \\\"role\\\": \\\"user\\\",\n      \\\"parts\\\": [{\\\"text\\\": \\\"$PROMPT\\\"}]\n    }]\n  }\" | jq -r '.candidates[0].content.parts[0].text'\n\n# With context\necho -e \"\\nWith context:\"\ncurl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-1.5-pro:generateContent\" \\\n  -d \"{\n    \\\"contents\\\": [{\n      \\\"role\\\": \\\"user\\\",\n      \\\"parts\\\": [\n        {\\\"text\\\": \\\"$CONTEXT\\\"},\n        {\\\"text\\\": \\\"$PROMPT\\\"}\n      ]\n    }]\n  }\" | jq -r '.candidates[0].content.parts[0].text'\n```\n\n</section>\n\n## Observation and conclusions\n\nThrough these experiments, you've observed how:\n\n1. Different models (Gemini 1.5 Pro vs Gemini 1.5 Flash) have varying capabilities and response styles.\n2. Temperature affects creativity and randomness in responses.\n3. Top-k and top-p parameters influence token selection and response diversity.\n4. Multiple candidates provide alternative responses to the same prompt.\n5. Max output tokens control response length.\n6. System instructions shape the tone and style of responses.\n7. Adding context improves response relevance and accuracy.\n\nUnderstanding these parameters helps you fine-tune Gemini models for your specific use cases, whether you need factual responses, creative content, or specialized information.\n\n## Next steps\n\n- Try combining different parameters to achieve the optimal response quality for your specific use case.\n- Experiment with more complex prompts and contexts to see how the models handle different scenarios.\n- Explore other models in the Vertex AI portfolio for specific tasks.\n- Implement these findings in your applications to improve user experience and response quality.\n\u0005End File\u0006# site/en/vertex/tutorials/generate-image-from-text.md\n# Generate images from text with Gemini\n\nThis tutorial demonstrates how to use Gemini models to generate images from text descriptions.\n\n## Set up the environment\n\nFirst, let's set up the environment and import the necessary libraries.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Get the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro-vision\",\n});\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Set your project ID and location\nexport PROJECT_ID=\"YOUR_PROJECT_ID\"\nexport LOCATION=\"us-central1\"\n\n# Get authentication token\nexport TOKEN=$(gcloud auth print-access-token)\n```\n\n</section>\n\n## Generate a simple image\n\nLet's start by generating a simple image from a text description.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro-vision\")\n\n# Create a text description for the image\nprompt = \"A serene landscape with mountains and a lake at sunset\"\n\n# Generate the image\nresponse = model.generate_content(\n    prompt,\n    generation_config={\n        \"response_mime_type\": \"image/png\"\n    }\n)\n\n# Display the image\nif response.candidates[0].content.parts[0].mime_type.startswith(\"image/\"):\n    image_data = response.candidates[0].content.parts[0].data\n    \n    # Save the image to a file\n    with open(\"generated_landscape.png\", \"wb\") as f:\n        f.write(image_data)\n    \n    # Display the image (if in a notebook)\n    try:\n        from IPython.display import Image, display\n        display(Image(data=image_data))\n    except ImportError:\n        print(\"Image saved as 'generated_landscape.png'\")\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nasync function generateImage() {\n  const prompt = \"A serene landscape with mountains and a lake at sunset\";\n\n  const result = await generativeModel.generateContent({\n    contents: [{ role: \"user\", parts: [{ text: prompt }] }],\n    generation_config: {\n      response_mime_type: \"image/png\"\n    }\n  });\n\n  const response = result.response;\n  \n  // Check if the response contains an image\n  if (response.candidates[0].content.parts[0].inline_data) {\n    const imageData = response.candidates[0].content.parts[0].inline_data.data;\n    const mimeType = response.candidates[0].content.parts[0].inline_data.mime_type;\n    \n    // Save the image to a file\n    const fs = require('fs');\n    fs.writeFileSync('generated_landscape.png', Buffer.from(imageData, 'base64'));\n    \n    console.log(\"Image generated and saved as 'generated_landscape.png'\");\n  } else {\n    console.log(\"No image was generated\");\n  }\n}\n\ngenerateImage();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Define the prompt\nPROMPT=\"A serene landscape with mountains and a lake at sunset\"\n\n# Generate image\nresponse=$(curl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-1.5-pro-vision:generateContent\" \\\n  -d \"{\n    \\\"contents\\\": [{\n      \\\"role\\\": \\\"user\\\",\n      \\\"parts\\\": [{\\\"text\\\": \\\"$PROMPT\\\"}]\n    }],\n    \\\"generation_config\\\": {\n      \\\"response_mime_type\\\": \\\"image/png\\\"\n    }\n  }\")\n\n# Extract the base64 image data\nimage_data=$(echo $response | jq -r '.candidates[0].content.parts[0].inline_data.data')\n\n# Save the image if it exists\nif [ \"$image_data\" != \"null\" ]; then\n  echo $image_data | base64 -d > generated_landscape.png\n  echo \"Image saved as generated_landscape.png\"\nelse\n  echo \"No image was generated\"\n  echo $response\nfi\n```\n\n</section>\n\n## Generate detailed images with specific parameters\n\nYou can create more detailed images by providing a more descriptive prompt and adjusting the generation parameters.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro-vision\")\n\n# Create a detailed text description\ndetailed_prompt = \"\"\"\nCreate a digital illustration of a futuristic city with:\n- Flying cars between tall skyscrapers\n- Holographic advertisements floating in the air\n- Green parks integrated into the buildings\n- Solar panels on rooftops\n- People walking on elevated pedestrian paths\n- Style: Vibrant colors with a sci-fi aesthetic\n\"\"\"\n\n# Generate the image with specific parameters\nresponse = model.generate_content(\n    detailed_prompt,\n    generation_config={\n        \"response_mime_type\": \"image/png\",\n        \"temperature\": 0.4,  # Lower temperature for more deterministic results\n    }\n)\n\n# Display the image\nif response.candidates[0].content.parts[0].mime_type.startswith(\"image/\"):\n    image_data = response.candidates[0].content.parts[0].data\n    \n    # Save the image to a file\n    with open(\"futuristic_city.png\", \"wb\") as f:\n        f.write(image_data)\n    \n    # Display the image (if in a notebook)\n    try:\n        from IPython.display import Image, display\n        display(Image(data=image_data))\n    except ImportError:\n        print(\"Image saved as 'futuristic_city.png'\")\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nasync function generateDetailedImage() {\n  const detailedPrompt = `\nCreate a digital illustration of a futuristic city with:\n- Flying cars between tall skyscrapers\n- Holographic advertisements floating in the air\n- Green parks integrated into the buildings\n- Solar panels on rooftops\n- People walking on elevated pedestrian paths\n- Style: Vibrant colors with a sci-fi aesthetic\n`;\n\n  const result = await generativeModel.generateContent({\n    contents: [{ role: \"user\", parts: [{ text: detailedPrompt }] }],\n    generation_config: {\n      response_mime_type: \"image/png\",\n      temperature: 0.4 // Lower temperature for more deterministic results\n    }\n  });\n\n  const response = result.response;\n  \n  // Check if the response contains an image\n  if (response.candidates[0].content.parts[0].inline_data) {\n    const imageData = response.candidates[0].content.parts[0].inline_data.data;\n    \n    // Save the image to a file\n    const fs = require('fs');\n    fs.writeFileSync('futuristic_city.png', Buffer.from(imageData, 'base64'));\n    \n    console.log(\"Image generated and saved as 'futuristic_city.png'\");\n  } else {\n    console.log(\"No image was generated\");\n  }\n}\n\ngenerateDetailedImage();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Define the detailed prompt\nDETAILED_PROMPT=\"Create a digital illustration of a futuristic city with:\n- Flying cars between tall skyscrapers\n- Holographic advertisements floating in the air\n- Green parks integrated into the buildings\n- Solar panels on rooftops\n- People walking on elevated pedestrian paths\n- Style: Vibrant colors with a sci-fi aesthetic\"\n\n# Generate image with specific parameters\nresponse=$(curl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-1.5-pro-vision:generateContent\" \\\n  -d \"{\n    \\\"contents\\\": [{\n      \\\"role\\\": \\\"user\\\",\n      \\\"parts\\\": [{\\\"text\\\": \\\"$DETAILED_PROMPT\\\"}]\n    }],\n    \\\"generation_config\\\": {\n      \\\"response_mime_type\\\": \\\"image/png\\\",\n      \\\"temperature\\\": 0.4\n    }\n  }\")\n\n# Extract the base64 image data\nimage_data=$(echo $response | jq -r '.candidates[0].content.parts[0].inline_data.data')\n\n# Save the image if it exists\nif [ \"$image_data\" != \"null\" ]; then\n  echo $image_data | base64 -d > futuristic_city.png\n  echo \"Image saved as futuristic_city.png\"\nelse\n  echo \"No image was generated\"\n  echo $response\nfi\n```\n\n</section>\n\n## Generate multiple image variations\n\nYou can generate multiple variations of an image by setting the candidate count parameter.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro-vision\")\n\n# Create a text description\nprompt = \"A whimsical treehouse in an enchanted forest, fantasy style\"\n\n# Generate multiple image variations\nresponse = model.generate_content(\n    prompt,\n    generation_config={\n        \"response_mime_type\": \"image/png\",\n        \"temperature\": 0.8,  # Higher temperature for more variation\n        \"candidate_count\": 3  # Generate 3 different variations\n    }\n)\n\n# Save and display all image variations\nfor i, candidate in enumerate(response.candidates):\n    if candidate.content.parts[0].mime_type.startswith(\"image/\"):\n        image_data = candidate.content.parts[0].data\n        \n        # Save the image to a file\n        with open(f\"treehouse_variation_{i+1}.png\", \"wb\") as f:\n            f.write(image_data)\n        \n        # Display the image (if in a notebook)\n        try:\n            from IPython.display import Image, display\n            print(f\"Variation {i+1}:\")\n            display(Image(data=image_data))\n        except ImportError:\n            print(f\"Variation {i+1} saved as 'treehouse_variation_{i+1}.png'\")\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nasync function generateMultipleVariations() {\n  const prompt = \"A whimsical treehouse in an enchanted forest, fantasy style\";\n\n  const result = await generativeModel.generateContent({\n    contents: [{ role: \"user\", parts: [{ text: prompt }] }],\n    generation_config: {\n      response_mime_type: \"image/png\",\n      temperature: 0.8, // Higher temperature for more variation\n      candidate_count: 3 // Generate 3 different variations\n    }\n  });\n\n  const response = result.response;\n  const fs = require('fs');\n  \n  // Process each candidate\n  response.candidates.forEach((candidate, i) => {\n    if (candidate.content.parts[0].inline_data) {\n      const imageData = candidate.content.parts[0].inline_data.data;\n      \n      // Save the image to a file\n      fs.writeFileSync(`treehouse_variation_${i+1}.png`, Buffer.from(imageData, 'base64'));\n      console.log(`Variation ${i+1} saved as 'treehouse_variation_${i+1}.png'`);\n    }\n  });\n}\n\ngenerateMultipleVariations();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Define the prompt\nPROMPT=\"A whimsical treehouse in an enchanted forest, fantasy style\"\n\n# Generate multiple image variations\nresponse=$(curl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-1.5-pro-vision:generateContent\" \\\n  -d \"{\n    \\\"contents\\\": [{\n      \\\"role\\\": \\\"user\\\",\n      \\\"parts\\\": [{\\\"text\\\": \\\"$PROMPT\\\"}]\n    }],\n    \\\"generation_config\\\": {\n      \\\"response_mime_type\\\": \\\"image/png\\\",\n      \\\"temperature\\\": 0.8,\n      \\\"candidate_count\\\": 3\n    }\n  }\")\n\n# Extract and save each image variation\nfor i in $(seq 0 2); do\n  image_data=$(echo $response | jq -r \".candidates[$i].content.parts[0].inline_data.data\")\n  \n  if [ \"$image_data\" != \"null\" ]; then\n    echo $image_data | base64 -d > \"treehouse_variation_$((i+1)).png\"\n    echo \"Variation $((i+1)) saved as 'treehouse_variation_$((i+1)).png'\"\n  fi\ndone\n```\n\n</section>\n\n## Generate an image based on a style reference\n\nYou can generate images in a specific style by providing a reference image and a description.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro-vision\")\n\n# Load a reference image (a painting in an artistic style)\n# Replace 'path_to_reference_image.jpg' with the path to your reference image\nimport base64\nfrom PIL import Image\nimport io\n\n# Function to load and encode image\ndef load_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return image_file.read()\n\n# Path to your reference image\nreference_image_path = \"path_to_reference_image.jpg\"  # Replace with your image path\nreference_image_data = load_image(reference_image_path)\n\n# Create a prompt with the reference image\nprompt = \"Create an image of a coastal village in the same artistic style as this reference image.\"\n\n# Generate the styled image\nresponse = model.generate_content(\n    [\n        prompt,\n        {\"mime_type\": \"image/jpeg\", \"data\": reference_image_data}\n    ],\n    generation_config={\n        \"response_mime_type\": \"image/png\"\n    }\n)\n\n# Display the generated image\nif response.candidates[0].content.parts[0].mime_type.startswith(\"image/\"):\n    image_data = response.candidates[0].content.parts[0].data\n    \n    # Save the image to a file\n    with open(\"styled_coastal_village.png\", \"wb\") as f:\n        f.write(image_data)\n    \n    # Display the image (if in a notebook)\n    try:\n        from IPython.display import Image, display\n        display(Image(data=image_data))\n    except ImportError:\n        print(\"Image saved as 'styled_coastal_village.png'\")\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst fs = require('fs');\n\nasync function generateStyleBasedImage() {\n  // Load a reference image (a painting in an artistic style)\n  // Replace 'path_to_reference_image.jpg' with the path to your reference image\n  const referenceImagePath = 'path_to_reference_image.jpg';\n  const referenceImageData = fs.readFileSync(referenceImagePath);\n  const base64Image = referenceImageData.toString('base64');\n\n  // Create a prompt with the reference image\n  const prompt = \"Create an image of a coastal village in the same artistic style as this reference image.\";\n\n  const result = await generativeModel.generateContent({\n    contents: [{ \n      role: \"user\", \n      parts: [\n        { text: prompt },\n        { \n          inline_data: {\n            mime_type: \"image/jpeg\",\n            data: base64Image\n          }\n        }\n      ] \n    }],\n    generation_config: {\n      response_mime_type: \"image/png\"\n    }\n  });\n\n  const response = result.response;\n  \n  // Check if the response contains an image\n  if (response.candidates[0].content.parts[0].inline_data) {\n    const imageData = response.candidates[0].content.parts[0].inline_data.data;\n    \n    // Save the image to a file\n    fs.writeFileSync('styled_coastal_village.png', Buffer.from(imageData, 'base64'));\n    \n    console.log(\"Image generated and saved as 'styled_coastal_village.png'\");\n  } else {\n    console.log(\"No image was generated\");\n  }\n}\n\ngenerateStyleBasedImage();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Define the prompt\nPROMPT=\"Create an image of a coastal village in the same artistic style as this reference image.\"\n\n# Path to reference image\nREFERENCE_IMAGE_PATH=\"path_to_reference_image.jpg\"  # Replace with your image path\n\n# Convert image to base64\nBASE64_IMAGE=$(base64 -w 0 $REFERENCE_IMAGE_PATH)\n\n# Generate image based on style reference\nresponse=$(curl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-1.5-pro-vision:generateContent\" \\\n  -d \"{\n    \\\"contents\\\": [{\n      \\\"role\\\": \\\"user\\\",\n      \\\"parts\\\": [\n        {\\\"text\\\": \\\"$PROMPT\\\"},\n        {\n          \\\"inline_data\\\": {\n            \\\"mime_type\\\": \\\"image/jpeg\\\",\n            \\\"data\\\": \\\"$BASE64_IMAGE\\\"\n          }\n        }\n      ]\n    }],\n    \\\"generation_config\\\": {\n      \\\"response_mime_type\\\": \\\"image/png\\\"\n    }\n  }\")\n\n# Extract the base64 image data\nimage_data=$(echo $response | jq -r '.candidates[0].content.parts[0].inline_data.data')\n\n# Save the image if it exists\nif [ \"$image_data\" != \"null\" ]; then\n  echo $image_data | base64 -d > styled_coastal_village.png\n  echo \"Image saved as styled_coastal_village.png\"\nelse\n  echo \"No image was generated\"\n  echo $response\nfi\n```\n\n</section>\n\n## Create images with specific dimensions\n\nYou can request images with specific dimensions by adjusting the generation config.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro-vision\")\n\n# Create a text description\nprompt = \"A colorful abstract painting with geometric shapes\"\n\n# Define different dimensions to try\ndimensions = [\n    {\"width\": 512, \"height\": 512},    # Square\n    {\"width\": 768, \"height\": 512},    # Landscape\n    {\"width\": 512, \"height\": 768}     # Portrait\n]\n\n# Generate images with different dimensions\nfor i, dim in enumerate(dimensions):\n    print(f\"Generating image with dimensions: {dim['width']}x{dim['height']}\")\n    \n    response = model.generate_content(\n        prompt,\n        generation_config={\n            \"response_mime_type\": \"image/png\",\n            \"image_generation_config\": {\n                \"width\": dim[\"width\"],\n                \"height\": dim[\"height\"]\n            }\n        }\n    )\n    \n    # Save and display the image\n    if response.candidates[0].content.parts[0].mime_type.startswith(\"image/\"):\n        image_data = response.candidates[0].content.parts[0].data\n        \n        # Save the image to a file\n        filename = f\"abstract_{dim['width']}x{dim['height']}.png\"\n        with open(filename, \"wb\") as f:\n            f.write(image_data)\n        \n        # Display the image (if in a notebook)\n        try:\n            from IPython.display import Image, display\n            display(Image(data=image_data))\n        except ImportError:\n            print(f\"Image saved as '{filename}'\")\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nasync function generateImagesWithDimensions() {\n  const prompt = \"A colorful abstract painting with geometric shapes\";\n\n  // Define different dimensions to try\n  const dimensions = [\n    {width: 512, height: 512},    // Square\n    {width: 768, height: 512},    // Landscape\n    {width: 512, height: 768}     // Portrait\n  ];\n\n  for (const dim of dimensions) {\n    console.log(`Generating image with dimensions: ${dim.width}x${dim.height}`);\n    \n    const result = await generativeModel.generateContent({\n      contents: [{ role: \"user\", parts: [{ text: prompt }] }],\n      generation_config: {\n        response_mime_type: \"image/png\",\n        image_generation_config: {\n          width: dim.width,\n          height: dim.height\n        }\n      }\n    });\n\n    const response = result.response;\n    \n    // Check if the response contains an image\n    if (response.candidates[0].content.parts[0].inline_data) {\n      const imageData = response.candidates[0].content.parts[0].inline_data.data;\n      \n      // Save the image to a file\n      const filename = `abstract_${dim.width}x${dim.height}.png`;\n      const fs = require('fs');\n      fs.writeFileSync(filename, Buffer.from(imageData, 'base64'));\n      \n      console.log(`Image saved as '${filename}'`);\n    } else {\n      console.log(\"No image was generated\");\n    }\n  }\n}\n\ngenerateImagesWithDimensions();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Define the prompt\nPROMPT=\"A colorful abstract painting with geometric shapes\"\n\n# Define dimensions arrays\nWIDTHS=(512 768 512)\nHEIGHTS=(512 512 768)\n\n# Generate images with different dimensions\nfor i in {0..2}; do\n  WIDTH=${WIDTHS[$i]}\n  HEIGHT=${HEIGHTS[$i]}\n  \n  echo \"Generating image with dimensions: ${WIDTH}x${HEIGHT}\"\n  \n  response=$(curl -s -X POST \\\n    -H \"Authorization: Bearer $TOKEN\" \\\n    -H \"Content-Type: application/json\" \\\n    \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-1.5-pro-vision:generateContent\" \\\n    -d \"{\n      \\\"contents\\\": [{\n        \\\"role\\\": \\\"user\\\",\n        \\\"parts\\\": [{\\\"text\\\": \\\"$PROMPT\\\"}]\n      }],\n      \\\"generation_config\\\": {\n        \\\"response_mime_type\\\": \\\"image/png\\\",\n        \\\"image_generation_config\\\": {\n          \\\"width\\\": $WIDTH,\n          \\\"height\\\": $HEIGHT\n        }\n      }\n    }\")\n\n  # Extract the base64 image data\n  image_data=$(echo $response | jq -r '.candidates[0].content.parts[0].inline_data.data')\n\n  # Save the image if it exists\n  if [ \"$image_data\" != \"null\" ]; then\n    filename=\"abstract_${WIDTH}x${HEIGHT}.png\"\n    echo $image_data | base64 -d > $filename\n    echo \"Image saved as $filename\"\n  else\n    echo \"No image was generated\"\n  fi\ndone\n```\n\n</section>\n\n## Create an image and get an explanation\n\nYou can generate an image and request an explanation about what was created.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro-vision\")\n\n# Create a prompt for image generation and explanation\nprompt = \"\"\"\nGenerate an image of a character that would fit in a fantasy RPG game.\nAfter creating the image, explain:\n1. The character's features and appearance\n2. Their potential abilities or role in a game\n3. The visual style you used for the character\n\"\"\"\n\n# Generate the image and explanation\nresponse = model.generate_content(\n    prompt,\n    generation_config={\n        \"response_mime_type\": \"image/png,text/plain\"  # Request both image and text\n    }\n)\n\n# Process the response parts\nimage_data = None\nexplanation_text = None\n\nfor part in response.candidates[0].content.parts:\n    if part.mime_type.startswith(\"image/\"):\n        image_data = part.data\n    elif part.mime_type == \"text/plain\":\n        explanation_text = part.text\n\n# Save and display the image\nif image_data:\n    with open(\"fantasy_character.png\", \"wb\") as f:\n        f.write(image_data)\n    \n    # Display the image (if in a notebook)\n    try:\n        from IPython.display import Image, display\n        display(Image(data=image_data))\n    except ImportError:\n        print(\"Image saved as 'fantasy_character.png'\")\n\n# Display the explanation\nif explanation_text:\n    print(\"\\nCharacter Explanation:\")\n    print(explanation_text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nasync function generateImageWithExplanation() {\n  const prompt = `\nGenerate an image of a character that would fit in a fantasy RPG game.\nAfter creating the image, explain:\n1. The character's features and appearance\n2. Their potential abilities or role in a game\n3. The visual style you used for the character\n`;\n\n  const result = await generativeModel.generateContent({\n    contents: [{ role: \"user\", parts: [{ text: prompt }] }],\n    generation_config: {\n      response_mime_type: \"image/png,text/plain\"  // Request both image and text\n    }\n  });\n\n  const response = result.response;\n  let imageData = null;\n  let explanationText = null;\n  \n  // Process the response parts\n  for (const part of response.candidates[0].content.parts) {\n    if (part.inline_data && part.inline_data.mime_type.startsWith(\"image/\")) {\n      imageData = part.inline_data.data;\n    } else if (part.text) {\n      explanationText = part.text;\n    }\n  }\n  \n  // Save and display the image\n  if (imageData) {\n    const fs = require('fs');\n    fs.writeFileSync(\"fantasy_character.png\", Buffer.from(imageData, 'base64'));\n    console.log(\"Image saved as 'fantasy_character.png'\");\n  }\n  \n  // Display the explanation\n  if (explanationText) {\n    console.log(\"\\nCharacter Explanation:\");\n    console.log(explanationText);\n  }\n}\n\ngenerateImageWithExplanation();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Define the prompt\nPROMPT=\"Generate an image of a character that would fit in a fantasy RPG game.\nAfter creating the image, explain:\n1. The character's features and appearance\n2. Their potential abilities or role in a game\n3. The visual style you used for the character\"\n\n# Generate image with explanation\nresponse=$(curl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-1.5-pro-vision:generateContent\" \\\n  -d \"{\n    \\\"contents\\\": [{\n      \\\"role\\\": \\\"user\\\",\n      \\\"parts\\\": [{\\\"text\\\": \\\"$PROMPT\\\"}]\n    }],\n    \\\"generation_config\\\": {\n      \\\"response_mime_type\\\": \\\"image/png,text/plain\\\"\n    }\n  }\")\n\n# Process the parts of the response to extract image and text\nparts=$(echo $response | jq '.candidates[0].content.parts')\nnum_parts=$(echo $parts | jq 'length')\n\n# Find and save the image and text\nfor (( i=0; i<$num_parts; i++ )); do\n  part_type=$(echo $parts | jq -r \".[$i] | if .inline_data then .inline_data.mime_type else \\\"text\\\" end\")\n  \n  if [[ $part_type == image/* ]]; then\n    image_data=$(echo $parts | jq -r \".[$i].inline_data.data\")\n    echo $image_data | base64 -d > fantasy_character.png\n    echo \"Image saved as fantasy_character.png\"\n  elif [[ $part_type == \"text\" ]]; then\n    explanation=$(echo $parts | jq -r \".[$i].text\")\n    echo -e \"\\nCharacter Explanation:\"\n    echo \"$explanation\"\n  fi\ndone\n```\n\n</section>\n\n## Conclusion\n\nIn this tutorial, you've learned how to:\n\n1. Generate basic images from text descriptions\n2. Create detailed images with specific parameters\n3. Generate multiple image variations\n4. Create images based on a style reference\n5. Generate images with specific dimensions\n6. Create images with accompanying explanations\n\nThe Gemini model's image generation capabilities offer a powerful tool for creative applications, content creation, and visual storytelling. By adjusting prompts and parameters, you can fine-tune the output to match your specific requirements.\n\n## Next steps\n\n- Experiment with more complex prompts and generation parameters\n- Try combining text and image inputs to guide the generation process\n- Use generated images in creative projects, prototyping, or as part of larger applications\n- Explore how to integrate image generation into a conversational interface\n\u0005End File\u0006# site/en/vertex/reference/models.md\n# Model reference for Vertex AI Gemini\n\nThis document lists all the Gemini models available on Google Cloud through Vertex AI.\n\n## Gemini 1.5 models\n\nThe following Gemini 1.5 models are available:\n\n| Model name | Description | Context window | Input modalities | Output modalities |\n|------------|-------------|-----------------|-----------------|-------------------|\n| `gemini-1.5-pro` | Offers the best quality for the most demanding tasks and a 1 million token context window. | 1M tokens | Text, image, audio, video | Text, image |\n| `gemini-1.5-flash` | Designed for high performance with lower latency and cost. Most suitable for straightforward tasks or applications requiring quicker responses. | 1M tokens | Text, image, audio, video | Text, image |\n| `gemini-1.5-pro-vision` | Vision-specific model optimized for handling image and video inputs. | 1M tokens | Text, image, audio, video | Text, image |\n| `gemini-1.5-flash-vision` | Faster, lower-cost version of the vision model for simpler image and video tasks. | 1M tokens | Text, image, audio, video | Text, image |\n\n## Gemini 1.0 models\n\nThe following Gemini 1.0 models are available:\n\n| Model name | Description | Context window | Input modalities | Output modalities |\n|------------|-------------|-----------------|-----------------|-------------------|\n| `gemini-1.0-pro` | Offers the best quality for highly complex tasks. Most suitable for demanding applications requiring deep reasoning, intricate content generation, or complex coding tasks. | 32K tokens | Text | Text |\n| `gemini-1.0-pro-vision` | Gemini Pro Vision can understand images and videos in addition to text. | 16K tokens for text, 16 images | Text, image, video | Text |\n| `gemini-1.0-ultra` | Most capable model for highly complex tasks. | 8K tokens | Text | Text |\n| `gemini-1.0-ultra-vision` | Most capable multimodal model for complex visual understanding tasks. | 8K tokens for text, 16 images | Text, image, video | Text |\n\n## Choosing the right model\n\nWhen choosing a Gemini model, consider the following factors:\n\n1. **Task complexity**: For complex reasoning or creative tasks, use Pro or Ultra models. For simpler tasks, Flash models may be sufficient.\n\n2. **Multimodal needs**: If your application involves images or videos, use the Vision variants.\n\n3. **Context window**: If you need to process large documents or maintain longer conversations, Gemini 1.5 models with their 1M token context window offer significant advantages.\n\n4. **Cost and performance**: Flash models are more cost-effective and have lower latency than Pro models.\n\n5. **Features**: Consider any specific capabilities you might need, such as system instructions or function calling.\n\n## Endpoints\n\nAll Gemini models on Vertex AI are accessible through the same endpoint. The base URL for API calls is:\n\n```\nhttps://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_NAME:METHOD\n```\n\nWhere:\n- `PROJECT_ID` is your Google Cloud project ID\n- `LOCATION` is the region (e.g., `us-central1`)\n- `MODEL_NAME` is one of the model names listed above\n- `METHOD` is the API method (e.g., `generateContent`, `streamGenerateContent`, `countTokens`)\n\n## Usage examples\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Generate content\nresponse = model.generate_content(\"Explain quantum computing in simple terms.\")\nprint(response.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n});\n\nasync function generateContent() {\n  const request = {\n    contents: [{ role: \"user\", parts: [{ text: \"Explain quantum computing in simple terms.\" }] }],\n  };\n\n  const response = await generativeModel.generateContent(request);\n  console.log(response.response.text());\n}\n\ngenerateContent();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Set environment variables\nexport PROJECT_ID=\"YOUR_PROJECT_ID\"\nexport LOCATION=\"us-central1\"\nexport MODEL=\"gemini-1.5-pro\"\n\n# Get an access token\nexport TOKEN=$(gcloud auth print-access-token)\n\n# Make the API request\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"Explain quantum computing in simple terms.\"}]\n      }\n    ]\n  }'\n```\n\n</section>\n\n## Next steps\n\n- Learn more about [using Gemini models](../tutorials/introduction.md)\n- Explore [API reference documentation](./api-reference.md)\n- Try out the [model tuning capabilities](../tutorials/tuning-models.md)\n\u0005End File\u0006# google/generative-ai-docs\n# System instructions for Gemini\n\nSystem instructions allow you to customize how Gemini models respond to user queries. This guide explains how to effectively use system instructions with Gemini models in Vertex AI.\n\n## What are system instructions?\n\nSystem instructions are special directives given to the model before user queries. They set the context, define behavior guidelines, establish a persona, or provide additional information that shapes how the model responds.\n\nUnlike user queries, system instructions are not part of the conversation itself but rather serve as a \"behind-the-scenes\" configuration for the model.\n\n## Supported models\n\nThe following Vertex AI Gemini models support system instructions:\n\n- `gemini-1.5-pro`\n- `gemini-1.5-flash` \n- `gemini-1.5-pro-vision`\n- `gemini-1.5-flash-vision`\n- `gemini-1.0-pro`\n- `gemini-1.0-pro-vision`\n- `gemini-1.0-ultra`\n- `gemini-1.0-ultra-vision`\n\n## How to use system instructions\n\nYou can add system instructions when interacting with Gemini models through the API. Here's how to do it in different programming languages:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Define the system instruction\nsystem_instruction = \"You are a helpful customer service agent for a furniture store. Keep your responses brief and friendly. Don't ask for personal information.\"\n\n# Create a chat session with the system instruction\nchat = model.start_chat(system_instruction=system_instruction)\n\n# Get a response\nresponse = chat.send_message(\"Do you have any blue sofas in stock?\")\nprint(response.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n});\n\nasync function chatWithSystemInstruction() {\n  // Define the system instruction\n  const systemInstruction = \"You are a helpful customer service agent for a furniture store. Keep your responses brief and friendly. Don't ask for personal information.\";\n  \n  // Create a chat session\n  const chat = generativeModel.startChat({\n    systemInstruction: systemInstruction,\n  });\n  \n  // Send a message and get a response\n  const result = await chat.sendMessage(\"Do you have any blue sofas in stock?\");\n  console.log(result.response.text());\n}\n\nchatWithSystemInstruction();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Set environment variables\nexport PROJECT_ID=\"YOUR_PROJECT_ID\"\nexport LOCATION=\"us-central1\"\nexport MODEL=\"gemini-1.5-pro\"\n\n# Get an access token\nexport TOKEN=$(gcloud auth print-access-token)\n\n# Make the API request with a system instruction\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"system\",\n        \"parts\": [{\"text\": \"You are a helpful customer service agent for a furniture store. Keep your responses brief and friendly. Don'\\''t ask for personal information.\"}]\n      },\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"Do you have any blue sofas in stock?\"}]\n      }\n    ]\n  }'\n```\n\n</section>\n\n## Best practices for system instructions\n\n### Do:\n\n1. **Be specific and clear**: Provide concrete guidelines rather than vague directions.\n2. **Keep it concise**: Focus on the most important aspects of how the model should behave.\n3. **Test and iterate**: Try different system instructions to see which works best for your use case.\n4. **Set clear boundaries**: Specify what the model should and shouldn't do.\n5. **Specify the persona**: If appropriate, define the character or role the model should adopt.\n\n### Don't:\n\n1. **Don't be inconsistent**: Avoid contradictory instructions that could confuse the model.\n2. **Don't be too verbose**: Excessively long instructions might dilute the important points.\n3. **Don't override ethical guidelines**: The model will still refuse harmful requests regardless of system instructions.\n4. **Don't try to \"jailbreak\"**: Instructions attempting to bypass safety features will be ignored.\n\n## Examples of effective system instructions\n\n### For a customer support bot:\n\n```\nYou are a customer support specialist for a software company. Your responses should be:\n1. Professional but friendly\n2. Concise (no more than 3-4 sentences)\n3. Focused on solving the customer's problem\n4. Free of technical jargon unless absolutely necessary\n\nWhen you don't know an answer, direct the customer to email support@example.com instead of guessing.\n```\n\n### For a creative writing assistant:\n\n```\nYou are a creative writing coach helping authors develop their ideas. \n- Ask thought-provoking questions about characters, plot, and setting\n- Provide specific suggestions rather than general advice\n- Offer examples when helpful\n- Be encouraging and constructive in your feedback\n- Adapt your guidance based on the writer's genre (sci-fi, romance, thriller, etc.)\n```\n\n### For a factual Q&A system:\n\n```\nYou are an educational assistant providing factual information.\n- Present information accurately and objectively\n- Cite sources when possible\n- Acknowledge uncertainty when a question doesn't have a clear answer\n- Provide context to help users understand complex topics\n- Use simple language accessible to high school students\n- When appropriate, offer different perspectives on controversial topics\n```\n\n## System instructions vs. user messages\n\n| Aspect | System Instructions | User Messages |\n|--------|---------------------|---------------|\n| **Purpose** | Configure model behavior, set context | Ask questions, provide inputs |\n| **Visibility** | Not shown to users in UI | Visible part of the conversation |\n| **Persistence** | Applies to entire conversation | Specific to that exchange |\n| **Format** | Directives to the model | Natural language queries |\n| **Best for** | Setting behavioral guidelines, defining roles | Specific questions, tasks, or requests |\n\n## Common use cases\n\n1. **Role-playing**: \"You are a medieval historian specializing in 12th century Europe.\"\n2. **Format control**: \"Format your responses as bullet points, keeping each point to one sentence.\"\n3. **Language style**: \"Respond in the style of a friendly kindergarten teacher explaining concepts to 5-year-olds.\"\n4. **Domain specificity**: \"You are a financial advisor helping people understand investment options. Use simple terms and avoid financial jargon.\"\n5. **Response length**: \"Keep all responses under 100 words and focus on the most important information.\"\n\n## Combining with other parameters\n\nSystem instructions work in conjunction with other model parameters. For example, you can combine system instructions with temperature settings:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Define the system instruction\nsystem_instruction = \"You are a poet who writes short, evocative poems. Each poem should be exactly 4 lines long.\"\n\n# Create a chat session with the system instruction\nchat = model.start_chat(system_instruction=system_instruction)\n\n# Get a response with higher creativity (temperature)\nresponse = chat.send_message(\n    \"Write a poem about autumn leaves\",\n    generation_config={\"temperature\": 0.9}\n)\nprint(response.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n});\n\nasync function combinedParameters() {\n  // Define the system instruction\n  const systemInstruction = \"You are a poet who writes short, evocative poems. Each poem should be exactly 4 lines long.\";\n  \n  // Create a chat session\n  const chat = generativeModel.startChat({\n    systemInstruction: systemInstruction,\n  });\n  \n  // Send a message with additional parameters\n  const result = await chat.sendMessage(\"Write a poem about autumn leaves\", {\n    generationConfig: {\n      temperature: 0.9\n    }\n  });\n  \n  console.log(result.response.text());\n}\n\ncombinedParameters();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Set environment variables\nexport PROJECT_ID=\"YOUR_PROJECT_ID\"\nexport LOCATION=\"us-central1\"\nexport MODEL=\"gemini-1.5-pro\"\n\n# Get an access token\nexport TOKEN=$(gcloud auth print-access-token)\n\n# Make the API request with system instruction and temperature\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"system\",\n        \"parts\": [{\"text\": \"You are a poet who writes short, evocative poems. Each poem should be exactly 4 lines long.\"}]\n      },\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"Write a poem about autumn leaves\"}]\n      }\n    ],\n    \"generation_config\": {\n      \"temperature\": 0.9\n    }\n  }'\n```\n\n</section>\n\n## Limitations\n\n- System instructions are guidelines, not absolute controls. The model may not always follow instructions perfectly.\n- Safety filters will override system instructions when necessary to prevent harmful outputs.\n- Very complex or contradictory system instructions may lead to unpredictable results.\n- System instructions count against your token limits.\n\n## Troubleshooting\n\nIf your system instructions aren't working as expected:\n\n1. **Simplify and clarify**: Break down complex instructions into simpler ones.\n2. **Check for contradictions**: Make sure your instructions don't conflict with each other.\n3. **Test with different phrasings**: Try rephrasing your instructions to be more direct.\n4. **Validate token count**: Ensure your system instructions aren't too long.\n5. **Start fresh**: Sometimes clearing the conversation and starting over can help.\n\n## Conclusion\n\nSystem instructions are a powerful tool for customizing Gemini model behavior to better suit your specific needs. By providing clear, specific guidelines through system instructions, you can create more tailored and effective AI interactions.\n\nFor more information, see the [API reference documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini).\n\u0005End File\u0006# google/generative-ai-docs\n# Setting Gemini system instructions\n\nSystem instructions help you shape how Gemini responds to user queries. They provide global context or guidance for the model to follow throughout a conversation. \n\n## How to use system instructions\n\nYou can use a `role: \"system\"` message to set instructions for the model. You can add system instructions when initializing a chat or include them in any message to the model.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model with system instructions\nmodel = GenerativeModel(\"gemini-1.5-pro\")\nchat = model.start_chat(\n    system_instruction=\"You're a customer service agent for a furniture store. Keep responses brief and friendly.\"\n)\n\n# Send a message to the chat\nresponse = chat.send_message(\"Do you have any blue sofas in stock?\")\nprint(response.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n});\n\nasync function runChat() {\n  const chat = generativeModel.startChat({\n    systemInstruction: \"You're a customer service agent for a furniture store. Keep responses brief and friendly.\"\n  });\n\n  const result = await chat.sendMessage(\"Do you have any blue sofas in stock?\");\n  console.log(result.response.text());\n}\n\nrunChat();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Set environment variables\nexport PROJECT_ID=\"YOUR_PROJECT_ID\"\nexport LOCATION=\"us-central1\"\nexport MODEL=\"gemini-1.5-pro\"\n\n# Get an access token\nexport TOKEN=$(gcloud auth print-access-token)\n\n# Make the API request\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"system\",\n        \"parts\": [{\"text\": \"You'\\''re a customer service agent for a furniture store. Keep responses brief and friendly.\"}]\n      },\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"Do you have any blue sofas in stock?\"}]\n      }\n    ]\n  }'\n```\n\n</section>\n\nYou can also include a system instruction in a single request without starting a chat:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Create content parts with system instruction and user message\nresponse = model.generate_content(\n    [\n        {\"role\": \"system\", \"parts\": [\"You are a math tutor helping with homework.\"]},\n        {\"role\": \"user\", \"parts\": [\"Explain how to solve quadratic equations.\"]}\n    ]\n)\n\nprint(response.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n});\n\nasync function generateWithSystemInstruction() {\n  const result = await generativeModel.generateContent({\n    contents: [\n      {\n        role: \"system\",\n        parts: [{ text: \"You are a math tutor helping with homework.\" }]\n      },\n      {\n        role: \"user\",\n        parts: [{ text: \"Explain how to solve quadratic equations.\" }]\n      }\n    ]\n  });\n\n  console.log(result.response.text());\n}\n\ngenerateWithSystemInstruction();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Make the API request\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"system\",\n        \"parts\": [{\"text\": \"You are a math tutor helping with homework.\"}]\n      },\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"Explain how to solve quadratic equations.\"}]\n      }\n    ]\n  }'\n```\n\n</section>\n\n## Best practices\n\nFor effective system instructions:\n\n1. **Be specific**: Provide clear guidance on the model's role and how it should respond.\n2. **Set boundaries**: Specify what topics to focus on or avoid.\n3. **Define formatting**: If you need a specific response format, include that in your instructions.\n4. **Keep it concise**: Overly verbose instructions can be less effective.\n\nHere are some example system instructions for different use cases:\n\n### For customer support:\n\n```\nYou are a customer support representative for a software company. \nBe helpful, concise, and friendly. \nWhen you don't know something, direct the customer to email support@example.com.\nFormat your responses in clear paragraphs with bullet points when listing options.\n```\n\n### For educational content:\n\n```\nYou are an educational assistant for high school students.\nExplain concepts in simple language suitable for teenagers.\nInclude examples to illustrate your explanations.\nWhen appropriate, suggest further resources for learning.\n```\n\n### For creative writing:\n\n```\nYou are a creative writing coach specializing in fiction.\nProvide constructive feedback on writing samples.\nOffer specific suggestions for improvement rather than general advice.\nBe encouraging but honest about areas that need work.\n```\n\n## Limitations\n\n- System instructions provide guidance, but the model may not follow them perfectly in every case.\n- Safety filters will override system instructions if they would lead to harmful content.\n- Very complex or contradictory instructions may lead to unpredictable results.\n\n## Next steps\n\n- Experiment with different system instructions to find what works best for your use case.\n- Try combining system instructions with other features like function calling or structured outputs.\n- For more advanced control, consider [model tuning](../tutorials/tuning-models.md) for your specific needs.\n\u0005End File\u0006# google/generative-ai-docs\n# Embeddings for text with Vertex AI\n\nText embeddings convert text into numerical vector representations, capturing semantic meaning that can be used for various applications. This guide will help you understand and use text embeddings with Vertex AI.\n\n## What are text embeddings?\n\nText embeddings are dense vector representations of text that capture semantic relationships. When text is converted to embeddings:\n\n- Similar texts have similar vectors (closer in vector space)\n- Relationships and meanings are preserved\n- These numerical representations can be used for machine learning tasks\n\n## Use cases for text embeddings\n\nText embeddings power many AI applications:\n\n- **Semantic search**: Find documents based on meaning, not just keywords\n- **Recommendation systems**: Suggest related content\n- **Classification**: Organize text into categories\n- **Clustering**: Group similar documents\n- **Question answering**: Match questions to potential answers\n- **Anomaly detection**: Identify unusual text patterns\n\n## Available embedding models\n\nVertex AI offers the following text embedding models:\n\n| Model | Description | Dimensions | Max tokens | Best for |\n|-------|-------------|------------|------------|----------|\n| `textembedding-gecko` | General purpose text embedding model | 768 | 3,072 | General text embedding tasks |\n| `textembedding-gecko-multilingual` | Supports 100+ languages | 768 | 3,072 | Multilingual applications |\n| `text-embedding-004` | Advanced embedding model | 768 or configurable | 8,192 | High-quality embeddings with longer context |\n\n## Getting text embeddings\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.language_models import TextEmbeddingModel\n\n# Initialize the model\nmodel = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n\n# Get embeddings for a single text\nembeddings = model.get_embeddings([\"How does neural machine translation work?\"])\n\n# Print the dimensionality\nprint(f\"Embedding dimension: {len(embeddings[0].values)}\")\n\n# Print the first few values\nprint(f\"First few values: {embeddings[0].values[:5]}\")\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst embeddingModel = 'text-embedding-004';\n\nasync function getEmbeddings() {\n  const request = {\n    model: embeddingModel,\n    texts: [\"How does neural machine translation work?\"],\n  };\n\n  const model = vertex_ai.preview.getTextEmbeddingModel(request);\n  const result = await model.batchEmbedTexts(request);\n  \n  const embedding = result.embeddings[0].values;\n  \n  console.log(`Embedding dimension: ${embedding.length}`);\n  console.log(`First few values: ${embedding.slice(0, 5)}`);\n}\n\ngetEmbeddings();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Set environment variables\nexport PROJECT_ID=\"YOUR_PROJECT_ID\"\nexport LOCATION=\"us-central1\"\nexport MODEL=\"text-embedding-004\"\n\n# Get an access token\nexport TOKEN=$(gcloud auth print-access-token)\n\n# Make the API request\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:predict\" \\\n  -d '{\n    \"instances\": [\n      { \"content\": \"How does neural machine translation work?\" }\n    ]\n  }'\n```\n\n</section>\n\n## Getting embeddings for multiple texts\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.language_models import TextEmbeddingModel\n\n# Initialize the model\nmodel = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n\n# Define multiple texts\ntexts = [\n    \"Machine learning algorithms build mathematical models based on sample data\",\n    \"Deep learning is a subset of machine learning\",\n    \"Natural language processing deals with interactions between computers and humans\"\n]\n\n# Get embeddings for multiple texts\nembeddings = model.get_embeddings(texts)\n\n# Print information about the embeddings\nfor i, embedding in enumerate(embeddings):\n    print(f\"Text {i+1} embedding dimension: {len(embedding.values)}\")\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst embeddingModel = 'text-embedding-004';\n\nasync function getBatchEmbeddings() {\n  // Define multiple texts\n  const texts = [\n    \"Machine learning algorithms build mathematical models based on sample data\",\n    \"Deep learning is a subset of machine learning\",\n    \"Natural language processing deals with interactions between computers and humans\"\n  ];\n\n  const request = {\n    model: embeddingModel,\n    texts: texts,\n  };\n\n  const model = vertex_ai.preview.getTextEmbeddingModel(request);\n  const result = await model.batchEmbedTexts(request);\n  \n  // Print information about the embeddings\n  result.embeddings.forEach((embedding, i) => {\n    console.log(`Text ${i+1} embedding dimension: ${embedding.values.length}`);\n  });\n}\n\ngetBatchEmbeddings();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Make the API request for multiple texts\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:predict\" \\\n  -d '{\n    \"instances\": [\n      { \"content\": \"Machine learning algorithms build mathematical models based on sample data\" },\n      { \"content\": \"Deep learning is a subset of machine learning\" },\n      { \"content\": \"Natural language processing deals with interactions between computers and humans\" }\n    ]\n  }'\n```\n\n</section>\n\n## Calculating similarity between embeddings\n\nOnce you have embeddings, you can calculate similarity between them using cosine similarity:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nimport numpy as np\nfrom vertexai.language_models import TextEmbeddingModel\n\n# Initialize the model\nmodel = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n\n# Define texts to compare\ntext1 = \"Machine learning is a field of artificial intelligence\"\ntext2 = \"AI includes the study of machine learning algorithms\"\ntext3 = \"The Eiffel Tower is located in Paris, France\"\n\n# Get embeddings\nembeddings = model.get_embeddings([text1, text2, text3])\n\n# Extract the vectors\nvector1 = np.array(embeddings[0].values)\nvector2 = np.array(embeddings[1].values)\nvector3 = np.array(embeddings[2].values)\n\n# Calculate cosine similarity\ndef cosine_similarity(a, b):\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\n# Compare similarities\nsim_1_2 = cosine_similarity(vector1, vector2)\nsim_1_3 = cosine_similarity(vector1, vector3)\nsim_2_3 = cosine_similarity(vector2, vector3)\n\nprint(f\"Similarity between text1 and text2: {sim_1_2:.4f}\")\nprint(f\"Similarity between text1 and text3: {sim_1_3:.4f}\")\nprint(f\"Similarity between text2 and text3: {sim_2_3:.4f}\")\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst embeddingModel = 'text-embedding-004';\n\n// Calculate cosine similarity between two vectors\nfunction cosineSimilarity(a, b) {\n  // Dot product\n  let dotProduct = 0;\n  for (let i = 0; i < a.length; i++) {\n    dotProduct += a[i] * b[i];\n  }\n  \n  // Magnitudes\n  let magA = 0;\n  let magB = 0;\n  for (let i = 0; i < a.length; i++) {\n    magA += a[i] * a[i];\n    magB += b[i] * b[i];\n  }\n  \n  magA = Math.sqrt(magA);\n  magB = Math.sqrt(magB);\n  \n  return dotProduct / (magA * magB);\n}\n\nasync function compareSimilarity() {\n  // Define texts to compare\n  const text1 = \"Machine learning is a field of artificial intelligence\";\n  const text2 = \"AI includes the study of machine learning algorithms\";\n  const text3 = \"The Eiffel Tower is located in Paris, France\";\n\n  const request = {\n    model: embeddingModel,\n    texts: [text1, text2, text3],\n  };\n\n  const model = vertex_ai.preview.getTextEmbeddingModel(request);\n  const result = await model.batchEmbedTexts(request);\n  \n  // Extract vectors\n  const vector1 = result.embeddings[0].values;\n  const vector2 = result.embeddings[1].values;\n  const vector3 = result.embeddings[2].values;\n  \n  // Compare similarities\n  const sim_1_2 = cosineSimilarity(vector1, vector2);\n  const sim_1_3 = cosineSimilarity(vector1, vector3);\n  const sim_2_3 = cosineSimilarity(vector2, vector3);\n  \n  console.log(`Similarity between text1 and text2: ${sim_1_2.toFixed(4)}`);\n  console.log(`Similarity between text1 and text3: ${sim_1_3.toFixed(4)}`);\n  console.log(`Similarity between text2 and text3: ${sim_2_3.toFixed(4)}`);\n}\n\ncompareSimilarity();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\nTo calculate similarity with REST API, you need to:\n1. Get embeddings for all texts\n2. Extract the embedding values\n3. Calculate cosine similarity in your application code\n\n```bash\n# Get embeddings for multiple texts\nresponse=$(curl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:predict\" \\\n  -d '{\n    \"instances\": [\n      { \"content\": \"Machine learning is a field of artificial intelligence\" },\n      { \"content\": \"AI includes the study of machine learning algorithms\" },\n      { \"content\": \"The Eiffel Tower is located in Paris, France\" }\n    ]\n  }')\n\n# Extract embeddings (in a real application, you would parse the JSON response)\necho \"Got embeddings. You'll need to extract and process them in your application code to calculate similarity.\"\n```\n\n</section>\n\n## Using embeddings for semantic search\n\nHere's a basic example of using embeddings for semantic search:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nimport numpy as np\nfrom vertexai.language_models import TextEmbeddingModel\n\n# Initialize the model\nmodel = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n\n# Define a document collection\ndocuments = [\n    \"Machine learning is a subset of artificial intelligence\",\n    \"Natural language processing focuses on interactions between computers and human language\",\n    \"Computer vision is concerned with how computers can gain understanding from digital images\",\n    \"Reinforcement learning is training algorithms using rewards and punishments\",\n    \"Neural networks are computing systems inspired by biological neural networks\"\n]\n\n# Convert documents to embeddings\ndocument_embeddings = model.get_embeddings(documents)\ndocument_vectors = [np.array(embedding.values) for embedding in document_embeddings]\n\n# Define a search query\nquery = \"How do computers understand images?\"\n\n# Get query embedding\nquery_embedding = model.get_embeddings([query])[0]\nquery_vector = np.array(query_embedding.values)\n\n# Calculate similarity between query and each document\nsimilarities = []\nfor i, doc_vector in enumerate(document_vectors):\n    similarity = np.dot(query_vector, doc_vector) / (np.linalg.norm(query_vector) * np.linalg.norm(doc_vector))\n    similarities.append((i, similarity))\n\n# Sort by similarity (highest first)\nsimilarities.sort(key=lambda x: x[1], reverse=True)\n\n# Display results\nprint(\"Search results for:\", query)\nfor doc_idx, similarity in similarities:\n    print(f\"Score: {similarity:.4f} - {documents[doc_idx]}\")\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst embeddingModel = 'text-embedding-004';\n\n// Calculate cosine similarity\nfunction cosineSimilarity(a, b) {\n  let dotProduct = 0;\n  let magA = 0;\n  let magB = 0;\n  \n  for (let i = 0; i < a.length; i++) {\n    dotProduct += a[i] * b[i];\n    magA += a[i] * a[i];\n    magB += b[i] * b[i];\n  }\n  \n  return dotProduct / (Math.sqrt(magA) * Math.sqrt(magB));\n}\n\nasync function semanticSearch() {\n  // Define a document collection\n  const documents = [\n    \"Machine learning is a subset of artificial intelligence\",\n    \"Natural language processing focuses on interactions between computers and human language\",\n    \"Computer vision is concerned with how computers can gain understanding from digital images\",\n    \"Reinforcement learning is training algorithms using rewards and punishments\",\n    \"Neural networks are computing systems inspired by biological neural networks\"\n  ];\n\n  const model = vertex_ai.preview.getTextEmbeddingModel({\n    model: embeddingModel,\n  });\n  \n  // Get document embeddings\n  const docResult = await model.batchEmbedTexts({\n    texts: documents\n  });\n  \n  const documentVectors = docResult.embeddings.map(emb => emb.values);\n  \n  // Define a search query\n  const query = \"How do computers understand images?\";\n  \n  // Get query embedding\n  const queryResult = await model.batchEmbedTexts({\n    texts: [query]\n  });\n  \n  const queryVector = queryResult.embeddings[0].values;\n  \n  // Calculate similarity between query and each document\n  const similarities = documentVectors.map((docVector, i) => {\n    const similarity = cosineSimilarity(queryVector, docVector);\n    return { index: i, similarity };\n  });\n  \n  // Sort by similarity (highest first)\n  similarities.sort((a, b) => b.similarity - a.similarity);\n  \n  // Display results\n  console.log(\"Search results for:\", query);\n  similarities.forEach(({ index, similarity }) => {\n    console.log(`Score: ${similarity.toFixed(4)} - ${documents[index]}`);\n  });\n}\n\nsemanticSearch();\n```\n\n</section>\n\n## Customizing embedding dimensions\n\nWith `text-embedding-004`, you can customize the embedding dimensions:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.language_models import TextEmbeddingModel\n\n# Initialize the model\nmodel = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n\n# Get embeddings with default dimension (768)\ndefault_embeddings = model.get_embeddings([\"This is a test sentence.\"])\nprint(f\"Default embedding dimension: {len(default_embeddings[0].values)}\")\n\n# Get embeddings with custom dimensions\ndimensions = [128, 256, 512]\n\nfor dim in dimensions:\n    embeddings = model.get_embeddings(\n        [\"This is a test sentence.\"],\n        output_dimensionality=dim\n    )\n    print(f\"Custom embedding dimension ({dim}): {len(embeddings[0].values)}\")\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst embeddingModel = 'text-embedding-004';\n\nasync function customizeDimensions() {\n  const text = \"This is a test sentence.\";\n  const model = vertex_ai.preview.getTextEmbeddingModel({\n    model: embeddingModel,\n  });\n  \n  // Get embeddings with default dimension (768)\n  const defaultResult = await model.batchEmbedTexts({\n    texts: [text]\n  });\n  \n  console.log(`Default embedding dimension: ${defaultResult.embeddings[0].values.length}`);\n  \n  // Get embeddings with custom dimensions\n  const dimensions = [128, 256, 512];\n  \n  for (const dim of dimensions) {\n    const result = await model.batchEmbedTexts({\n      texts: [text],\n      outputDimensionality: dim\n    });\n    \n    console.log(`Custom embedding dimension (${dim}): ${result.embeddings[0].values.length}`);\n  }\n}\n\ncustomizeDimensions();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Get embeddings with default dimension (768)\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:predict\" \\\n  -d '{\n    \"instances\": [\n      { \"content\": \"This is a test sentence.\" }\n    ]\n  }'\n\n# Get embeddings with custom dimension (256)\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:predict\" \\\n  -d '{\n    \"instances\": [\n      { \"content\": \"This is a test sentence.\" }\n    ],\n    \"parameters\": {\n      \"outputDimensionality\": 256\n    }\n  }'\n```\n\n</section>\n\n## Best practices for text embeddings\n\n1. **Choose the right model**:\n   - Use `textembedding-gecko` for general purpose tasks\n   - Use `textembedding-gecko-multilingual` for multilingual applications\n   - Use `text-embedding-004` for higher quality and longer context\n\n2. **Preprocessing**:\n   - Remove unnecessary formatting (but keep important structure)\n   - Consider chunking long documents into smaller segments\n   - Maintain consistent text processing\n\n3. **Performance optimization**:\n   - Batch requests when embedding multiple texts\n   - Cache embeddings for frequently used texts\n   - Use appropriate dimensionality for your use case\n\n4. **Storage and retrieval**:\n   - Use vector databases like Pinecone, Weaviate, or Chroma for large collections\n   - Consider dimensionality reduction for very large datasets\n   - Index embeddings for faster similarity search\n\n## Next steps\n\n- [Explore vector databases](https://cloud.google.com/vertex-ai/docs/vector-search/overview) for storing and searching embeddings\n- Try building a [retrieval-augmented generation (RAG) system](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/rag) with embeddings\n- Experiment with different similarity metrics beyond cosine similarity\n- Use embeddings for classification, clustering, or recommendation systems\n\u0005End File\u0006# google/generative-ai-docs\n# Embedding models on Vertex AI\n\nEmbedding models convert text, images, or other content into numerical vector representations that capture semantic meaning. This guide provides an overview of embedding models available on Vertex AI and how to use them.\n\n## What are embeddings?\n\nEmbeddings are dense vector representations that encode semantic information about the input. Similar content is mapped to similar points in the embedding space, enabling semantic search, clustering, classification, and other machine learning tasks.\n\n## Available embedding models\n\nVertex AI offers several embedding models for different use cases:\n\n### Text embedding models\n\n| Model | Description | Dimensions | Max input tokens |\n|-------|-------------|------------|------------------|\n| `textembedding-gecko` | General purpose text embedding model | 768 | 3,072 |\n| `textembedding-gecko-multilingual` | Supports 100+ languages | 768 | 3,072 |\n| `text-embedding-004` | Advanced text embedding model | Configurable (up to 768) | 8,192 |\n\n### Multimodal embedding models\n\n| Model | Description | Input modalities | Dimensions |\n|-------|-------------|------------------|------------|\n| `multimodalembedding` | Generates embeddings for text and images | Text, images | 1,408 |\n\n## Using text embeddings\n\nText embeddings are useful for:\n- Semantic search\n- Document similarity\n- Classification\n- Clustering\n- Information retrieval\n- Recommendation systems\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.language_models import TextEmbeddingModel\n\n# Initialize the model\nmodel = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n\n# Get embeddings for text\ntexts = [\n    \"The cat sat on the mat\",\n    \"The dog played in the yard\"\n]\n\nembeddings = model.get_embeddings(texts)\n\n# Print embedding dimensions\nprint(f\"Number of embeddings: {len(embeddings)}\")\nprint(f\"Embedding dimension: {len(embeddings[0].values)}\")\n\n# Print first few values of the first embedding\nprint(f\"First few values: {embeddings[0].values[:5]}\")\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\nasync function getTextEmbeddings() {\n  const embeddingModel = vertex_ai.preview.getTextEmbeddingModel({\n    model: \"text-embedding-004\",\n  });\n  \n  const texts = [\n    \"The cat sat on the mat\",\n    \"The dog played in the yard\"\n  ];\n  \n  const response = await embeddingModel.batchEmbedTexts({\n    texts: texts\n  });\n  \n  console.log(`Number of embeddings: ${response.embeddings.length}`);\n  console.log(`Embedding dimension: ${response.embeddings[0].values.length}`);\n  console.log(`First few values: ${response.embeddings[0].values.slice(0, 5)}`);\n}\n\ngetTextEmbeddings();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Set environment variables\nexport PROJECT_ID=\"YOUR_PROJECT_ID\"\nexport LOCATION=\"us-central1\"\nexport MODEL=\"text-embedding-004\"\n\n# Get an access token\nexport TOKEN=$(gcloud auth print-access-token)\n\n# Make the API request\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:predict\" \\\n  -d '{\n    \"instances\": [\n      { \"content\": \"The cat sat on the mat\" },\n      { \"content\": \"The dog played in the yard\" }\n    ]\n  }'\n```\n\n</section>\n\n### Customizing embedding dimensions\n\nWith `text-embedding-004`, you can specify the output dimensionality:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.language_models import TextEmbeddingModel\n\n# Initialize the model\nmodel = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n\n# Get embeddings with custom dimensions\ntext = \"This is a sample text for embedding\"\ncustom_dim_embedding = model.get_embeddings([text], output_dimensionality=256)\n\nprint(f\"Custom embedding dimension: {len(custom_dim_embedding[0].values)}\")\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\nasync function getCustomDimensionEmbeddings() {\n  const embeddingModel = vertex_ai.preview.getTextEmbeddingModel({\n    model: \"text-embedding-004\",\n  });\n  \n  const text = \"This is a sample text for embedding\";\n  \n  const response = await embeddingModel.batchEmbedTexts({\n    texts: [text],\n    outputDimensionality: 256\n  });\n  \n  console.log(`Custom embedding dimension: ${response.embeddings[0].values.length}`);\n}\n\ngetCustomDimensionEmbeddings();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Get embeddings with custom dimensions\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:predict\" \\\n  -d '{\n    \"instances\": [\n      { \"content\": \"This is a sample text for embedding\" }\n    ],\n    \"parameters\": {\n      \"outputDimensionality\": 256\n    }\n  }'\n```\n\n</section>\n\n## Using multimodal embeddings\n\nMultimodal embeddings can encode both text and images into the same embedding space, enabling cross-modal retrieval and similarity.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nimport base64\nfrom vertexai.vision_models import MultiModalEmbeddingModel\n\n# Initialize the model\nmodel = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding@001\")\n\n# Function to load image data\ndef load_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return image_file.read()\n\n# Get text embedding\ntext_embedding = model.get_embeddings(\n    text=\"A beautiful sunset over the ocean\"\n)\nprint(f\"Text embedding dimension: {len(text_embedding.text_embedding)}\")\n\n# Get image embedding (replace with your image path)\nimage_data = load_image(\"path/to/sunset_image.jpg\")\nimage_embedding = model.get_embeddings(\n    image=image_data\n)\nprint(f\"Image embedding dimension: {len(image_embedding.image_embedding)}\")\n\n# Get multimodal embedding with both text and image\nmultimodal_embedding = model.get_embeddings(\n    text=\"A beautiful sunset over the ocean\",\n    image=image_data\n)\nprint(f\"Multimodal embedding dimensions:\")\nprint(f\"- Text: {len(multimodal_embedding.text_embedding)}\")\nprint(f\"- Image: {len(multimodal_embedding.image_embedding)}\")\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst fs = require('fs');\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\nasync function getMultimodalEmbeddings() {\n  const embeddingModel = vertex_ai.preview.getMultimodalEmbeddingModel({\n    model: \"multimodalembedding@001\",\n  });\n  \n  // Get text embedding\n  const textResponse = await embeddingModel.embedContent({\n    text: \"A beautiful sunset over the ocean\"\n  });\n  \n  console.log(`Text embedding dimension: ${textResponse.textEmbedding.values.length}`);\n  \n  // Get image embedding (replace with your image path)\n  const imageData = fs.readFileSync('path/to/sunset_image.jpg');\n  const base64Image = imageData.toString('base64');\n  \n  const imageResponse = await embeddingModel.embedContent({\n    imageBytes: base64Image\n  });\n  \n  console.log(`Image embedding dimension: ${imageResponse.imageEmbedding.values.length}`);\n  \n  // Get multimodal embedding with both text and image\n  const multimodalResponse = await embeddingModel.embedContent({\n    text: \"A beautiful sunset over the ocean\",\n    imageBytes: base64Image\n  });\n  \n  console.log('Multimodal embedding dimensions:');\n  console.log(`- Text: ${multimodalResponse.textEmbedding.values.length}`);\n  console.log(`- Image: ${multimodalResponse.imageEmbedding.values.length}`);\n}\n\ngetMultimodalEmbeddings();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Set environment variables for multimodal model\nexport MM_MODEL=\"multimodalembedding@001\"\n\n# Read and encode image\nIMAGE_PATH=\"path/to/sunset_image.jpg\"\nIMAGE_B64=$(base64 -w 0 $IMAGE_PATH)\n\n# Get text embedding\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MM_MODEL}:predict\" \\\n  -d '{\n    \"instances\": [\n      {\n        \"text\": \"A beautiful sunset over the ocean\"\n      }\n    ]\n  }'\n\n# Get image embedding\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MM_MODEL}:predict\" \\\n  -d '{\n    \"instances\": [\n      {\n        \"image\": {\"bytesBase64Encoded\": \"'\"$IMAGE_B64\"'\"}\n      }\n    ]\n  }'\n\n# Get multimodal embedding\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MM_MODEL}:predict\" \\\n  -d '{\n    \"instances\": [\n      {\n        \"text\": \"A beautiful sunset over the ocean\",\n        \"image\": {\"bytesBase64Encoded\": \"'\"$IMAGE_B64\"'\"}\n      }\n    ]\n  }'\n```\n\n</section>\n\n## Working with embedding similarity\n\nOnce you have embeddings, you can calculate similarity between them to identify related content:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nimport numpy as np\nfrom vertexai.language_models import TextEmbeddingModel\n\n# Function to calculate cosine similarity\ndef cosine_similarity(a, b):\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\n# Initialize the model\nmodel = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n\n# Define texts to compare\ntexts = [\n    \"Machine learning involves computers learning from data\",\n    \"AI systems can learn from examples and improve over time\",\n    \"Paris is the capital of France and known for the Eiffel Tower\"\n]\n\n# Get embeddings\nembeddings = model.get_embeddings(texts)\nvectors = [np.array(embedding.values) for embedding in embeddings]\n\n# Calculate similarities\nprint(\"Similarity between text 1 and text 2:\", cosine_similarity(vectors[0], vectors[1]))\nprint(\"Similarity between text 1 and text 3:\", cosine_similarity(vectors[0], vectors[2]))\nprint(\"Similarity between text 2 and text 3:\", cosine_similarity(vectors[1], vectors[2]))\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Function to calculate cosine similarity\nfunction cosineSimilarity(a, b) {\n  let dotProduct = 0;\n  let normA = 0;\n  let normB = 0;\n  \n  for (let i = 0; i < a.length; i++) {\n    dotProduct += a[i] * b[i];\n    normA += a[i] * a[i];\n    normB += b[i] * b[i];\n  }\n  \n  return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));\n}\n\nasync function calculateSimilarities() {\n  const embeddingModel = vertex_ai.preview.getTextEmbeddingModel({\n    model: \"text-embedding-004\",\n  });\n  \n  // Define texts to compare\n  const texts = [\n    \"Machine learning involves computers learning from data\",\n    \"AI systems can learn from examples and improve over time\",\n    \"Paris is the capital of France and known for the Eiffel Tower\"\n  ];\n  \n  // Get embeddings\n  const response = await embeddingModel.batchEmbedTexts({\n    texts: texts\n  });\n  \n  const vectors = response.embeddings.map(emb => emb.values);\n  \n  // Calculate similarities\n  console.log(\"Similarity between text 1 and text 2:\", \n    cosineSimilarity(vectors[0], vectors[1]));\n  console.log(\"Similarity between text 1 and text 3:\", \n    cosineSimilarity(vectors[0], vectors[2]));\n  console.log(\"Similarity between text 2 and text 3:\", \n    cosineSimilarity(vectors[1], vectors[2]));\n}\n\ncalculateSimilarities();\n```\n\n</section>\n\n## Storing and retrieving embeddings\n\nFor production applications, you'll want to store embeddings in a vector database for efficient retrieval:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\n# Example using Google Cloud Vertex AI Vector Search\nfrom google.cloud import aiplatform\nfrom vertexai.language_models import TextEmbeddingModel\nimport uuid\n\n# Initialize Vector Search\naiplatform.init(project='YOUR_PROJECT_ID', location='us-central1')\n\n# Initialize the embedding model\nmodel = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n\n# Sample documents\ndocuments = [\n    \"Machine learning algorithms build mathematical models based on sample data\",\n    \"Deep learning is a subset of machine learning\",\n    \"Natural language processing deals with interactions between computers and humans\"\n]\n\n# Get embeddings\nembeddings = model.get_embeddings(documents)\nvectors = [embedding.values for embedding in embeddings]\n\n# Create Vector Search index (first time only)\n# index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n#    display_name=\"document_search_index\",\n#    dimensions=768,\n#    approximate_neighbors_count=10,\n# )\n\n# Use existing index\nindex = aiplatform.MatchingEngineIndex(\"YOUR_INDEX_ID\")\n\n# Create an index endpoint (first time only)\n# index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n#    display_name=\"document_search_endpoint\",\n#    public_endpoint_enabled=True,\n# )\n\n# Use existing endpoint\nindex_endpoint = aiplatform.MatchingEngineIndexEndpoint(\"YOUR_ENDPOINT_ID\")\n\n# Deploy index to endpoint (if not already deployed)\n# deployed_index = index_endpoint.deploy_index(\n#    index=index,\n#    deployed_index_id=\"document_search_deployed_index\",\n# )\n\n# Prepare data for upload\ndatapoints = []\nfor i, (text, vector) in enumerate(zip(documents, vectors)):\n    datapoint = {\n        \"id\": str(uuid.uuid4()),\n        \"embedding\": vector,\n        \"restricts\": {\"text\": text}\n    }\n    datapoints.append(datapoint)\n\n# Upload vectors to the index\nindex.upsert_datapoints(datapoints=datapoints)\n\n# Query the index\nquery = \"How do computers understand human language?\"\nquery_embedding = model.get_embeddings([query])[0].values\n\n# Search for similar documents\nresults = index_endpoint.find_neighbors(\n    deployed_index_id=\"document_search_deployed_index\",\n    queries=[query_embedding],\n    num_neighbors=2\n)\n\n# Display results\nfor match in results[0]:\n    print(f\"Score: {match.distance}, Document: {match.restricts['text']}\")\n```\n\n</section>\n\n## Best practices for embeddings\n\n1. **Choose the right model**:\n   - Text-only tasks: Use `text-embedding-004` for high quality or `textembedding-gecko` for general use\n   - Multilingual needs: Use `textembedding-gecko-multilingual`\n   - Cross-modal tasks: Use `multimodalembedding`\n\n2. **Optimize for your use case**:\n   - For `text-embedding-004`, choose appropriate dimensions based on your needs\n   - Batch requests when embedding multiple items\n   - Cache embeddings for frequently used content\n\n3. **Data preparation**:\n   - Clean and normalize text before embedding\n   - Break long documents into meaningful chunks\n   - Consider task-specific preprocessing\n\n4. **Storage and retrieval**:\n   - Use vector databases or Vertex AI Vector Search for large collections\n   - Create appropriate indexes for fast similarity search\n   - Consider approximate nearest neighbor search for large collections\n\n## Common applications\n\n- **Semantic search**: Find documents based on meaning, not just keywords\n- **Content recommendation**: Suggest similar items based on embedding similarity\n- **Document clustering**: Group documents by semantic similarity\n- **Classification**: Categorize text or images based on their embeddings\n- **Cross-modal retrieval**: Find images matching text descriptions or vice versa\n- **Question answering**: Match questions to potential answers\n- **Anomaly detection**: Identify outliers in embedding space\n\n## Next steps\n\n- Explore [Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview) for efficient embedding storage and retrieval\n- Try building a [retrieval-augmented generation (RAG)](https://cloud.google.com/vertex-ai/docs/generative-ai/rag/rag-overview) application using embeddings\n- Experiment with multimodal applications combining text and image embeddings\n- Learn more about [similarity metrics](https://en.wikipedia.org/wiki/Cosine_similarity) beyond cosine similarity\n\u0005End File\u0006# Safety settings for Vertex AI Gemini models\n\nSafety settings in Vertex AI Gemini models help you control the content that the model generates. This reference guide explains the safety settings and how to use them with Gemini models.\n\n## Understanding safety settings\n\nSafety settings let you specify:\n\n1. **Safety categories**: Different types of potentially harmful content\n2. **Threshold levels**: How strictly to filter content in each category\n\nBy adjusting these settings, you can configure Gemini models to align with your application's requirements while maintaining appropriate safeguards.\n\n## Safety categories\n\nGemini models on Vertex AI support the following safety categories:\n\n| Safety Category | Description |\n|-----------------|-------------|\n| `HARM_CATEGORY_HATE_SPEECH` | Content that is rude, disrespectful, or profane, or that expresses, incites, or promotes hate based on identity. |\n| `HARM_CATEGORY_DANGEROUS_CONTENT` | Content that promotes, facilitates, or encourages harmful activities. |\n| `HARM_CATEGORY_HARASSMENT` | Content that refers to or directs identity-based harassment toward an individual. |\n| `HARM_CATEGORY_SEXUALLY_EXPLICIT` | Content meant to arouse sexual excitement, such as descriptions of sexual activity, or that promotes sexual services (except for educational content). |\n\n## Threshold levels\n\nFor each safety category, you can set one of the following threshold levels:\n\n| Threshold | Description |\n|-----------|-------------|\n| `BLOCK_NONE` | No content filtering in this category. All content allowed regardless of safety classification. |\n| `BLOCK_ONLY_HIGH` | Filters only content that has a HIGH probability of violating safety policies. |\n| `BLOCK_MEDIUM_AND_ABOVE` | Filters content that has MEDIUM or HIGH probability of violating safety policies. |\n| `BLOCK_LOW_AND_ABOVE` | Strictest setting. Filters content that has LOW, MEDIUM, or HIGH probability of violating safety policies. |\n\n## Default safety settings\n\nIf you don't specify safety settings, Gemini models use these defaults:\n\n| Safety Category | Default Threshold |\n|-----------------|-------------------|\n| `HARM_CATEGORY_HATE_SPEECH` | `BLOCK_MEDIUM_AND_ABOVE` |\n| `HARM_CATEGORY_DANGEROUS_CONTENT` | `BLOCK_MEDIUM_AND_ABOVE` |\n| `HARM_CATEGORY_HARASSMENT` | `BLOCK_MEDIUM_AND_ABOVE` |\n| `HARM_CATEGORY_SEXUALLY_EXPLICIT` | `BLOCK_MEDIUM_AND_ABOVE` |\n\n## How to set safety settings\n\nYou can configure safety settings when making API calls to Gemini models:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\nfrom vertexai.generative_models import HarmCategory, HarmBlockThreshold\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Define custom safety settings\nsafety_settings = {\n    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH\n}\n\n# Generate content with custom safety settings\nresponse = model.generate_content(\n    \"Write a story about an adventure\",\n    safety_settings=safety_settings\n)\n\nprint(response.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI, HarmCategory, HarmBlockThreshold} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n  safetySettings: [\n    {\n      category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n      threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n    },\n    {\n      category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n      threshold: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE\n    },\n    {\n      category: HarmCategory.HARM_CATEGORY_HARASSMENT,\n      threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n    },\n    {\n      category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n      threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH\n    }\n  ]\n});\n\nasync function generateWithSafetySettings() {\n  const result = await generativeModel.generateContent({\n    contents: [{ role: \"user\", parts: [{ text: \"Write a story about an adventure\" }] }]\n  });\n\n  console.log(result.response.text());\n}\n\ngenerateWithSafetySettings();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Set environment variables\nexport PROJECT_ID=\"YOUR_PROJECT_ID\"\nexport LOCATION=\"us-central1\"\nexport MODEL=\"gemini-1.5-pro\"\n\n# Get an access token\nexport TOKEN=$(gcloud auth print-access-token)\n\n# Make the API request with safety settings\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"Write a story about an adventure\"}]\n      }\n    ],\n    \"safety_settings\": [\n      {\n        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"threshold\": \"BLOCK_LOW_AND_ABOVE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"threshold\": \"BLOCK_ONLY_HIGH\"\n      }\n    ]\n  }'\n```\n\n</section>\n\n## Safety settings in chat\n\nYou can also apply safety settings to chat sessions:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\nfrom vertexai.generative_models import HarmCategory, HarmBlockThreshold\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Define custom safety settings\nsafety_settings = {\n    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n}\n\n# Start a chat session with custom safety settings\nchat = model.start_chat(safety_settings=safety_settings)\n\n# Send a message\nresponse = chat.send_message(\"Tell me about internet safety for children\")\nprint(response.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI, HarmCategory, HarmBlockThreshold} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model with safety settings\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n  safetySettings: [\n    {\n      category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n      threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n    },\n    {\n      category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n      threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n    },\n    {\n      category: HarmCategory.HARM_CATEGORY_HARASSMENT,\n      threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n    },\n    {\n      category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n      threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n    }\n  ]\n});\n\nasync function chatWithSafetySettings() {\n  // Start a chat session\n  const chat = generativeModel.startChat();\n  \n  // Send a message\n  const result = await chat.sendMessage(\"Tell me about internet safety for children\");\n  console.log(result.response.text());\n}\n\nchatWithSafetySettings();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Create a session\nexport SESSION_ID=$(uuidgen)\n\n# First message with safety settings\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"Tell me about internet safety for children\"}]\n      }\n    ],\n    \"safety_settings\": [\n      {\n        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n      }\n    ]\n  }'\n```\n\n</section>\n\n## Handling safety feedback\n\nWhen content is blocked due to safety settings, you'll receive a safety feedback response. Here's how to check for safety feedback:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\nfrom vertexai.generative_models import HarmCategory, HarmBlockThreshold\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Set very restrictive safety settings\nsafety_settings = {\n    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE\n}\n\n# Generate content that might trigger safety filters\ntry:\n    response = model.generate_content(\n        \"Write detailed instructions for how to hack into a computer\",\n        safety_settings=safety_settings\n    )\n    print(response.text)\nexcept Exception as e:\n    print(f\"Content was blocked: {e}\")\n    \n    # If you want to access the safety ratings from the exception\n    # (This is example code - actual implementation depends on the exception structure)\n    # if hasattr(e, 'response') and hasattr(e.response, 'safety_ratings'):\n    #     for rating in e.response.safety_ratings:\n    #         print(f\"Category: {rating.category}, Probability: {rating.probability}\")\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI, HarmCategory, HarmBlockThreshold} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model with restrictive safety settings\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n  safetySettings: [\n    {\n      category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n      threshold: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE\n    },\n    {\n      category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n      threshold: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE\n    },\n    {\n      category: HarmCategory.HARM_CATEGORY_HARASSMENT,\n      threshold: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE\n    },\n    {\n      category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n      threshold: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE\n    }\n  ]\n});\n\nasync function handleSafetyFeedback() {\n  try {\n    const result = await generativeModel.generateContent({\n      contents: [{ \n        role: \"user\", \n        parts: [{ text: \"Write detailed instructions for how to hack into a computer\" }] \n      }]\n    });\n    \n    console.log(result.response.text());\n  } catch (error) {\n    console.log(\"Content was blocked:\", error.message);\n    \n    // If you want to access the safety ratings from the error\n    if (error.response && error.response.promptFeedback && error.response.promptFeedback.safetyRatings) {\n      error.response.promptFeedback.safetyRatings.forEach(rating => {\n        console.log(`Category: ${rating.category}, Probability: ${rating.probability}`);\n      });\n    }\n  }\n}\n\nhandleSafetyFeedback();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Make a request that might trigger safety filters\nresponse=$(curl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"Write detailed instructions for how to hack into a computer\"}]\n      }\n    ],\n    \"safety_settings\": [\n      {\n        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n        \"threshold\": \"BLOCK_LOW_AND_ABOVE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"threshold\": \"BLOCK_LOW_AND_ABOVE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n        \"threshold\": \"BLOCK_LOW_AND_ABOVE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"threshold\": \"BLOCK_LOW_AND_ABOVE\"\n      }\n    ]\n  }')\n\n# Check for safety feedback\necho $response | jq '.promptFeedback'\n```\n\n</section>\n\n## Use case examples\n\n### Educational content\n\nFor applications focused on education, you might want to allow educational discussion of sensitive topics while still blocking explicit content:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\nfrom vertexai.generative_models import HarmCategory, HarmBlockThreshold\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Educational content settings - slightly more permissive\nsafety_settings = {\n    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n}\n\n# Generate educational content about a sensitive topic\nresponse = model.generate_content(\n    \"Explain how viruses and malware work for a cybersecurity class\",\n    safety_settings=safety_settings\n)\n\nprint(response.text)\n```\n\n</section>\n\n### Child-friendly content\n\nFor applications designed for children, you might want the strictest safety settings:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\nfrom vertexai.generative_models import HarmCategory, HarmBlockThreshold\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Child-friendly content settings - most restrictive\nsafety_settings = {\n    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE\n}\n\n# Generate child-friendly content\nresponse = model.generate_content(\n    \"Write a short story about space exploration for children\",\n    safety_settings=safety_settings\n)\n\nprint(response.text)\n```\n\n</section>\n\n### Professional content\n\nFor business applications, you might want to block harmful content while allowing professional discussion:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\nfrom vertexai.generative_models import HarmCategory, HarmBlockThreshold\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Professional content settings\nsafety_settings = {\n    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE\n}\n\n# Generate professional content\nresponse = model.generate_content(\n    \"Write a professional email about a project delay\",\n    safety_settings=safety_settings\n)\n\nprint(response.text)\n```\n\n</section>\n\n## Best practices\n\n1. **Start with default settings**: Begin with the default settings and adjust only if needed.\n\n2. **Match to your audience**: Use stricter settings for applications aimed at children or sensitive audiences.\n\n3. **Consider your use case**: Educational or professional applications might require different settings than entertainment.\n\n4. **Test thoroughly**: Test your safety settings with various prompts to ensure they're working as expected.\n\n5. **Handle rejections gracefully**: Provide helpful feedback to users when content is blocked.\n\n6. **Balance safety and utility**: Find the right balance between safety and the functionality of your application.\n\n## Limitations\n\n- Safety settings are not perfect and might occasionally block harmless content or miss harmful content.\n- Different models might interpret the same safety settings slightly differently.\n- Safety settings work in conjunction with other built-in model safety measures.\n- Some topics might be blocked regardless of safety settings due to model policies.\n\n## Next steps\n\n- Experiment with different safety settings for your specific use case\n- Implement proper error handling for blocked content\n- Consider combining safety settings with system instructions for better control\n- Review [Vertex AI responsible AI practices](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai)\n\u0005End File\u0006# google/generative-ai-docs\n# site/en/vertex/reference/multimodal-input.md\n# Using multimodal inputs with Gemini\n\nGemini models can accept multiple types of inputs, including text, images, audio, and video. This capability enables rich, contextual interactions that combine different forms of information. This guide explains how to work with multimodal inputs in Gemini models on Vertex AI.\n\n## Supported modalities\n\nThe following input modalities are supported by Gemini models:\n\n| Modality | Description | Supported Formats |\n|----------|-------------|-------------------|\n| Text | Written content | Plain text, markdown |\n| Images | Static visual content | PNG, JPEG, WEBP, HEIC, HEIF |\n| Video | Moving visual content | MP4, MOV, AVI, MPEG |\n| Audio | Sound content | MP3, WAV, FLAC, M4A |\n\n## Supported models\n\nNot all Gemini models support all modalities. Here's a breakdown of the multimodal capabilities by model:\n\n| Model | Text | Images | Video | Audio |\n|-------|------|--------|-------|-------|\n| `gemini-1.5-pro` | ✅ | ✅ | ✅ | ✅ |\n| `gemini-1.5-flash` | ✅ | ✅ | ✅ | ✅ |\n| `gemini-1.5-pro-vision` | ✅ | ✅ | ✅ | ✅ |\n| `gemini-1.5-flash-vision` | ✅ | ✅ | ✅ | ✅ |\n| `gemini-1.0-pro` | ✅ | ❌ | ❌ | ❌ |\n| `gemini-1.0-pro-vision` | ✅ | ✅ | ✅ | ❌ |\n| `gemini-1.0-ultra` | ✅ | ❌ | ❌ | ❌ |\n| `gemini-1.0-ultra-vision` | ✅ | ✅ | ✅ | ❌ |\n\n## Working with text and images\n\nThe most common multimodal combination is text and images. Here's how to send both to Gemini:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nimport base64\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro-vision\")\n\n# Function to load and encode an image\ndef load_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return image_file.read()\n\n# Load image\nimage_data = load_image(\"path/to/your/image.jpg\")\n\n# Create a prompt with text and image\nprompt = \"What's in this image? Provide a detailed description.\"\n\n# Generate content with text and image\nresponse = model.generate_content(\n    [\n        prompt,\n        {\"mime_type\": \"image/jpeg\", \"data\": image_data}\n    ]\n)\n\nprint(response.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst fs = require('fs');\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro-vision\",\n});\n\nasync function generateWithTextAndImage() {\n  // Load image\n  const imagePath = \"path/to/your/image.jpg\";\n  const imageData = fs.readFileSync(imagePath);\n  const base64EncodedImage = imageData.toString('base64');\n  \n  // Create a prompt with text and image\n  const prompt = \"What's in this image? Provide a detailed description.\";\n  \n  const result = await generativeModel.generateContent({\n    contents: [\n      {\n        role: \"user\",\n        parts: [\n          { text: prompt },\n          { \n            inline_data: {\n              mime_type: \"image/jpeg\",\n              data: base64EncodedImage\n            }\n          }\n        ]\n      }\n    ],\n  });\n  \n  console.log(result.response.text());\n}\n\ngenerateWithTextAndImage();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Set environment variables\nexport PROJECT_ID=\"YOUR_PROJECT_ID\"\nexport LOCATION=\"us-central1\"\nexport MODEL=\"gemini-1.5-pro-vision\"\n\n# Get an access token\nexport TOKEN=$(gcloud auth print-access-token)\n\n# Encode image to base64\nexport IMAGE_PATH=\"path/to/your/image.jpg\"\nexport BASE64_IMAGE=$(base64 -w 0 $IMAGE_PATH)\n\n# Make the API request with text and image\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [\n          {\n            \"text\": \"What'\\''s in this image? Provide a detailed description.\"\n          },\n          {\n            \"inline_data\": {\n              \"mime_type\": \"image/jpeg\",\n              \"data\": \"'\"$BASE64_IMAGE\"'\"\n            }\n          }\n        ]\n      }\n    ]\n  }'\n```\n\n</section>\n\n## Working with multiple images\n\nYou can send multiple images in a single request:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro-vision\")\n\n# Function to load an image\ndef load_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return image_file.read()\n\n# Load multiple images\nimage1_data = load_image(\"path/to/first_image.jpg\")\nimage2_data = load_image(\"path/to/second_image.jpg\")\n\n# Create a prompt with text and multiple images\nprompt = \"Compare these two images and tell me the differences between them.\"\n\n# Generate content with text and multiple images\nresponse = model.generate_content(\n    [\n        prompt,\n        {\"mime_type\": \"image/jpeg\", \"data\": image1_data},\n        {\"mime_type\": \"image/jpeg\", \"data\": image2_data}\n    ]\n)\n\nprint(response.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst fs = require('fs');\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro-vision\",\n});\n\nasync function generateWithMultipleImages() {\n  // Load multiple images\n  const imagePath1 = \"path/to/first_image.jpg\";\n  const imagePath2 = \"path/to/second_image.jpg\";\n  \n  const imageData1 = fs.readFileSync(imagePath1);\n  const imageData2 = fs.readFileSync(imagePath2);\n  \n  const base64EncodedImage1 = imageData1.toString('base64');\n  const base64EncodedImage2 = imageData2.toString('base64');\n  \n  // Create a prompt with text and multiple images\n  const prompt = \"Compare these two images and tell me the differences between them.\";\n  \n  const result = await generativeModel.generateContent({\n    contents: [\n      {\n        role: \"user\",\n        parts: [\n          { text: prompt },\n          { \n            inline_data: {\n              mime_type: \"image/jpeg\",\n              data: base64EncodedImage1\n            }\n          },\n          { \n            inline_data: {\n              mime_type: \"image/jpeg\",\n              data: base64EncodedImage2\n            }\n          }\n        ]\n      }\n    ],\n  });\n  \n  console.log(result.response.text());\n}\n\ngenerateWithMultipleImages();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Encode multiple images to base64\nexport IMAGE_PATH1=\"path/to/first_image.jpg\"\nexport IMAGE_PATH2=\"path/to/second_image.jpg\"\nexport BASE64_IMAGE1=$(base64 -w 0 $IMAGE_PATH1)\nexport BASE64_IMAGE2=$(base64 -w 0 $IMAGE_PATH2)\n\n# Make the API request with text and multiple images\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [\n          {\n            \"text\": \"Compare these two images and tell me the differences between them.\"\n          },\n          {\n            \"inline_data\": {\n              \"mime_type\": \"image/jpeg\",\n              \"data\": \"'\"$BASE64_IMAGE1\"'\"\n            }\n          },\n          {\n            \"inline_data\": {\n              \"mime_type\": \"image/jpeg\",\n              \"data\": \"'\"$BASE64_IMAGE2\"'\"\n            }\n          }\n        ]\n      }\n    ]\n  }'\n```\n\n</section>\n\n## Working with video\n\nGemini can analyze video content. Here's how to send a video to the model:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro-vision\")\n\n# Function to load a video file\ndef load_video(video_path):\n    with open(video_path, \"rb\") as video_file:\n        return video_file.read()\n\n# Load video\nvideo_data = load_video(\"path/to/your/video.mp4\")\n\n# Create a prompt with text and video\nprompt = \"What's happening in this video? Provide a detailed description.\"\n\n# Generate content with text and video\nresponse = model.generate_content(\n    [\n        prompt,\n        {\"mime_type\": \"video/mp4\", \"data\": video_data}\n    ]\n)\n\nprint(response.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst fs = require('fs');\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro-vision\",\n});\n\nasync function generateWithVideo() {\n  // Load video\n  const videoPath = \"path/to/your/video.mp4\";\n  const videoData = fs.readFileSync(videoPath);\n  const base64EncodedVideo = videoData.toString('base64');\n  \n  // Create a prompt with text and video\n  const prompt = \"What's happening in this video? Provide a detailed description.\";\n  \n  const result = await generativeModel.generateContent({\n    contents: [\n      {\n        role: \"user\",\n        parts: [\n          { text: prompt },\n          { \n            inline_data: {\n              mime_type: \"video/mp4\",\n              data: base64EncodedVideo\n            }\n          }\n        ]\n      }\n    ],\n  });\n  \n  console.log(result.response.text());\n}\n\ngenerateWithVideo();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Encode video to base64\nexport VIDEO_PATH=\"path/to/your/video.mp4\"\nexport BASE64_VIDEO=$(base64 -w 0 $VIDEO_PATH)\n\n# Make the API request with text and video\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [\n          {\n            \"text\": \"What'\\''s happening in this video? Provide a detailed description.\"\n          },\n          {\n            \"inline_data\": {\n              \"mime_type\": \"video/mp4\",\n              \"data\": \"'\"$BASE64_VIDEO\"'\"\n            }\n          }\n        ]\n      }\n    ]\n  }'\n```\n\n</section>\n\n## Working with audio\n\nGemini 1.5 models can also process audio input:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Function to load an audio file\ndef load_audio(audio_path):\n    with open(audio_path, \"rb\") as audio_file:\n        return audio_file.read()\n\n# Load audio\naudio_data = load_audio(\"path/to/your/audio.mp3\")\n\n# Create a prompt with text and audio\nprompt = \"Transcribe this audio and tell me what language it's in.\"\n\n# Generate content with text and audio\nresponse = model.generate_content(\n    [\n        prompt,\n        {\"mime_type\": \"audio/mp3\", \"data\": audio_data}\n    ]\n)\n\nprint(response.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst fs = require('fs');\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n});\n\nasync function generateWithAudio() {\n  // Load audio\n  const audioPath = \"path/to/your/audio.mp3\";\n  const audioData = fs.readFileSync(audioPath);\n  const base64EncodedAudio = audioData.toString('base64');\n  \n  // Create a prompt with text and audio\n  const prompt = \"Transcribe this audio and tell me what language it's in.\";\n  \n  const result = await generativeModel.generateContent({\n    contents: [\n      {\n        role: \"user\",\n        parts: [\n          { text: prompt },\n          { \n            inline_data: {\n              mime_type: \"audio/mp3\",\n              data: base64EncodedAudio\n            }\n          }\n        ]\n      }\n    ],\n  });\n  \n  console.log(result.response.text());\n}\n\ngenerateWithAudio();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Encode audio to base64\nexport AUDIO_PATH=\"path/to/your/audio.mp3\"\nexport BASE64_AUDIO=$(base64 -w 0 $AUDIO_PATH)\n\n# Make the API request with text and audio\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-1.5-pro:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [\n          {\n            \"text\": \"Transcribe this audio and tell me what language it'\\''s in.\"\n          },\n          {\n            \"inline_data\": {\n              \"mime_type\": \"audio/mp3\",\n              \"data\": \"'\"$BASE64_AUDIO\"'\"\n            }\n          }\n        ]\n      }\n    ]\n  }'\n```\n\n</section>\n\n## Combining multiple modalities\n\nYou can combine multiple different modalities in a single request:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Functions to load different media types\ndef load_image(path):\n    with open(path, \"rb\") as file:\n        return file.read()\n\ndef load_audio(path):\n    with open(path, \"rb\") as file:\n        return file.read()\n\n# Load media\nimage_data = load_image(\"path/to/your/image.jpg\")\naudio_data = load_audio(\"path/to/your/audio.mp3\")\n\n# Create a prompt with text, image, and audio\nprompt = \"Describe the image and transcribe the audio. Are they related to each other?\"\n\n# Generate content with multiple modalities\nresponse = model.generate_content(\n    [\n        prompt,\n        {\"mime_type\": \"image/jpeg\", \"data\": image_data},\n        {\"mime_type\": \"audio/mp3\", \"data\": audio_data}\n    ]\n)\n\nprint(response.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst fs = require('fs');\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n});\n\nasync function generateWithMultipleModalities() {\n  // Load media\n  const imagePath = \"path/to/your/image.jpg\";\n  const audioPath = \"path/to/your/audio.mp3\";\n  \n  const imageData = fs.readFileSync(imagePath);\n  const audioData = fs.readFileSync(audioPath);\n  \n  const base64EncodedImage = imageData.toString('base64');\n  const base64EncodedAudio = audioData.toString('base64');\n  \n  // Create a prompt with text, image, and audio\n  const prompt = \"Describe the image and transcribe the audio. Are they related to each other?\";\n  \n  const result = await generativeModel.generateContent({\n    contents: [\n      {\n        role: \"user\",\n        parts: [\n          { text: prompt },\n          { \n            inline_data: {\n              mime_type: \"image/jpeg\",\n              data: base64EncodedImage\n            }\n          },\n          { \n            inline_data: {\n              mime_type: \"audio/mp3\",\n              data: base64EncodedAudio\n            }\n          }\n        ]\n      }\n    ],\n  });\n  \n  console.log(result.response.text());\n}\n\ngenerateWithMultipleModalities();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Encode media to base64\nexport IMAGE_PATH=\"path/to/your/image.jpg\"\nexport AUDIO_PATH=\"path/to/your/audio.mp3\"\nexport BASE64_IMAGE=$(base64 -w 0 $IMAGE_PATH)\nexport BASE64_AUDIO=$(base64 -w 0 $AUDIO_PATH)\n\n# Make the API request with multiple modalities\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-1.5-pro:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [\n          {\n            \"text\": \"Describe the image and transcribe the audio. Are they related to each other?\"\n          },\n          {\n            \"inline_data\": {\n              \"mime_type\": \"image/jpeg\",\n              \"data\": \"'\"$BASE64_IMAGE\"'\"\n            }\n          },\n          {\n            \"inline_data\": {\n              \"mime_type\": \"audio/mp3\",\n              \"data\": \"'\"$BASE64_AUDIO\"'\"\n            }\n          }\n        ]\n      }\n    ]\n  }'\n```\n\n</section>\n\n## Multimodal input in a conversation\n\nYou can also use multimodal inputs in a conversation context:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro-vision\")\n\n# Function to load an image\ndef load_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return image_file.read()\n\n# Start a chat session\nchat = model.start_chat()\n\n# First message: text only\nresponse1 = chat.send_message(\"I'm going to show you some images. Please analyze them carefully.\")\nprint(\"Bot:\", response1.text)\n\n# Second message: text + image\nimage_data = load_image(\"path/to/your/image.jpg\")\nresponse2 = chat.send_message(\n    [\n        \"What can you tell me about this image?\",\n        {\"mime_type\": \"image/jpeg\", \"data\": image_data}\n    ]\n)\nprint(\"Bot:\", response2.text)\n\n# Third message: follow-up question\nresponse3 = chat.send_message(\"Are there any people in the image? If so, what are they doing?\")\nprint(\"Bot:\", response3.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst fs = require('fs');\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro-vision\",\n});\n\nasync function multimodalChat() {\n  // Start a chat session\n  const chat = generativeModel.startChat();\n  \n  // First message: text only\n  const response1 = await chat.sendMessage(\"I'm going to show you some images. Please analyze them carefully.\");\n  console.log(\"Bot:\", response1.response.text());\n  \n  // Second message: text + image\n  const imagePath = \"path/to/your/image.jpg\";\n  const imageData = fs.readFileSync(imagePath);\n  const base64EncodedImage = imageData.toString('base64');\n  \n  const response2 = await chat.sendMessage({\n    contents: [\n      {\n        role: \"user\",\n        parts: [\n          { text: \"What can you tell me about this image?\" },\n          { \n            inline_data: {\n              mime_type: \"image/jpeg\",\n              data: base64EncodedImage\n            }\n          }\n        ]\n      }\n    ]\n  });\n  console.log(\"Bot:\", response2.response.text());\n  \n  // Third message: follow-up question\n  const response3 = await chat.sendMessage(\"Are there any people in the image? If so, what are they doing?\");\n  console.log(\"Bot:\", response3.response.text());\n}\n\nmultimodalChat();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\nFor REST API, you need to maintain the conversation history manually by including previous messages in each request.\n\n```bash\n# Create a unique session ID\nexport SESSION_ID=$(uuidgen)\n\n# First message: text only\nresponse1=$(curl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-1.5-pro-vision:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"I'\\''m going to show you some images. Please analyze them carefully.\"}]\n      }\n    ]\n  }')\n\n# Extract bot response\nbot_response1=$(echo $response1 | jq -r '.candidates[0].content.parts[0].text')\necho \"Bot: $bot_response1\"\n\n# Encode image to base64\nexport IMAGE_PATH=\"path/to/your/image.jpg\"\nexport BASE64_IMAGE=$(base64 -w 0 $IMAGE_PATH)\n\n# Second message: text + image (including conversation history)\nresponse2=$(curl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-1.5-pro-vision:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"I'\\''m going to show you some images. Please analyze them carefully.\"}]\n      },\n      {\n        \"role\": \"model\",\n        \"parts\": [{\"text\": \"'\"$bot_response1\"'\"}]\n      },\n      {\n        \"role\": \"user\",\n        \"parts\": [\n          {\"text\": \"What can you tell me about this image?\"},\n          {\n            \"inline_data\": {\n              \"mime_type\": \"image/jpeg\",\n              \"data\": \"'\"$BASE64_IMAGE\"'\"\n            }\n          }\n        ]\n      }\n    ]\n  }')\n\n# Extract bot response\nbot_response2=$(echo $response2 | jq -r '.candidates[0].content.parts[0].text')\necho \"Bot: $bot_response2\"\n\n# Third message: follow-up question (including conversation history)\nresponse3=$(curl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-1.5-pro-vision:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"I'\\''m going to show you some images. Please analyze them carefully.\"}]\n      },\n      {\n        \"role\": \"model\",\n        \"parts\": [{\"text\": \"'\"$bot_response1\"'\"}]\n      },\n      {\n        \"role\": \"user\",\n        \"parts\": [\n          {\"text\": \"What can you tell me about this image?\"},\n          {\n            \"inline_data\": {\n              \"mime_type\": \"image/jpeg\",\n              \"data\": \"'\"$BASE64_IMAGE\"'\"\n            }\n          }\n        ]\n      },\n      {\n        \"role\": \"model\",\n        \"parts\": [{\"text\": \"'\"$bot_response2\"'\"}]\n      },\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"Are there any people in the image? If so, what are they doing?\"}]\n      }\n    ]\n  }')\n\n# Extract bot response\nbot_response3=$(echo $response3 | jq -r '.candidates[0].content.parts[0].text')\necho \"Bot: $bot_response3\"\n```\n\n</section>\n\n## Media optimization tips\n\nWhen working with media files in Gemini, consider these optimization tips:\n\n### Images\n- **Resolution**: Aim for 1024x1024 pixels for optimal balance between quality and performance\n- **Format**: JPEG/JPG for photos, PNG for graphics with transparency\n- **File size**: Keep below 20MB; compress if necessary\n- **Content**: Ensure key elements are clearly visible\n\n### Videos\n- **Duration**: Keep under 2 minutes when possible\n- **Resolution**: 720p or 1080p is sufficient\n- **Format**: MP4 with H.264 encoding is widely compatible\n- **File size**: Keep below 200MB; compress if necessary\n\n### Audio\n- **Duration**: Keep under 5 minutes for optimal processing\n- **Format**: MP3 or WAV formats work well\n- **Quality**: 128-192 kbps bitrate is sufficient for speech\n- **Clarity**: Ensure audio is clear with minimal background noise\n\n## Best practices\n\n1. **Be specific in your prompts**: Clearly indicate what you want the model to focus on when analyzing media.\n\n2. **Use appropriate modalities**: Choose the right format for your content (e.g., don't use video when an image would suffice).\n\n3. **Optimize file sizes**: Large files slow down processing and may hit API limits.\n\n4. **Follow up with clarifying questions**: If the model misses something in the media, ask specific follow-up questions.\n\n5. **Combine modalities strategically**: Different modalities can complement each other (e.g., images + text description).\n\n6. **Test with different models**: Different Gemini models may have varying capabilities with multimodal content.\n\n7. **Handle failures gracefully**: Implement proper error handling for cases where media processing fails.\n\n## Limitations\n\n- **File size limits**: There are maximum file size limits for each modality\n- **Content restrictions**: Safety filters apply to all modality types\n- **Processing complexity**: Complex or ambiguous visual/audio content may result in less accurate interpretations\n- **Language support**: Non-English audio may have varying levels of transcription accuracy\n- **Processing time**: Multimodal inputs typically take longer to process than text-only inputs\n\n## Next steps\n\n- Experiment with different combinations of modalities for your specific use case\n- Try the same content with different Gemini models to compare performance\n- Explore more complex applications like document analysis or multimedia content creation\n- Consider implementing a feedback loop to improve multimodal interactions\n\u0005End File\u0006# google/generative-ai-docs\n# site/en/vertex/reference/streaming.md\n# Streaming responses with Gemini\n\nThis guide explains how to stream responses from Gemini models in Vertex AI. Streaming allows you to receive model responses incrementally as they're generated, rather than waiting for the complete response at once.\n\n## Why use streaming?\n\nStreaming offers several advantages:\n\n1. **Improved user experience**: Users see responses appear progressively, making the interaction feel more responsive.\n2. **Faster initial display**: The first part of the response appears immediately, reducing perceived latency.\n3. **Better for long responses**: For lengthy outputs, streaming avoids making users wait until the entire response is generated.\n4. **Resource efficiency**: Your application can start processing or displaying content before the full response arrives.\n\n## Supported models\n\nAll Gemini models in Vertex AI support streaming:\n\n- `gemini-1.5-pro`\n- `gemini-1.5-flash`\n- `gemini-1.5-pro-vision`\n- `gemini-1.5-flash-vision`\n- `gemini-1.0-pro`\n- `gemini-1.0-pro-vision`\n- `gemini-1.0-ultra`\n- `gemini-1.0-ultra-vision`\n\n## Basic streaming implementation\n\nHere's how to implement streaming with different SDKs:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Create a prompt\nprompt = \"Write a short story about a robot learning to paint.\"\n\n# Stream the response\nfor chunk in model.generate_content(prompt, stream=True):\n    print(chunk.text, end=\"\", flush=True)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n});\n\nasync function streamResponse() {\n  const prompt = \"Write a short story about a robot learning to paint.\";\n  \n  const streamingResponse = await generativeModel.generateContentStream({\n    contents: [{ role: \"user\", parts: [{ text: prompt }] }],\n  });\n\n  // Process the streaming response\n  for await (const chunk of streamingResponse.stream) {\n    // Print the text part of the chunk if it exists\n    if (chunk.candidates[0].content.parts[0].text) {\n      process.stdout.write(chunk.candidates[0].content.parts[0].text);\n    }\n  }\n  \n  console.log(); // New line at the end\n}\n\nstreamResponse();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Set environment variables\nexport PROJECT_ID=\"YOUR_PROJECT_ID\"\nexport LOCATION=\"us-central1\"\nexport MODEL=\"gemini-1.5-pro\"\n\n# Get an access token\nexport TOKEN=$(gcloud auth print-access-token)\n\n# Make the streaming API request\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:streamGenerateContent\" \\\n  --data '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"Write a short story about a robot learning to paint.\"}]\n      }\n    ]\n  }' \\\n  --output -\n```\n\n</section>\n\n## Streaming with chat\n\nYou can also stream responses in a chat conversation:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model and start a chat\nmodel = GenerativeModel(\"gemini-1.5-pro\")\nchat = model.start_chat()\n\n# Send a message and stream the response\nprompt = \"Tell me about the solar system.\"\nprint(\"User: \" + prompt)\nprint(\"Bot: \", end=\"\", flush=True)\n\nfor chunk in chat.send_message(prompt, stream=True):\n    print(chunk.text, end=\"\", flush=True)\nprint(\"\\n\")\n\n# Continue the conversation with streaming\nprompt = \"What is the largest planet?\"\nprint(\"User: \" + prompt)\nprint(\"Bot: \", end=\"\", flush=True)\n\nfor chunk in chat.send_message(prompt, stream=True):\n    print(chunk.text, end=\"\", flush=True)\nprint(\"\\n\")\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n});\n\nasync function streamChat() {\n  // Start a chat session\n  const chat = generativeModel.startChat();\n  \n  // First message with streaming\n  const prompt1 = \"Tell me about the solar system.\";\n  console.log(\"User: \" + prompt1);\n  process.stdout.write(\"Bot: \");\n  \n  const streamingResponse1 = await chat.sendMessageStream(prompt1);\n  for await (const chunk of streamingResponse1.stream) {\n    if (chunk.candidates[0].content.parts[0].text) {\n      process.stdout.write(chunk.candidates[0].content.parts[0].text);\n    }\n  }\n  console.log(\"\\n\");\n  \n  // Second message with streaming\n  const prompt2 = \"What is the largest planet?\";\n  console.log(\"User: \" + prompt2);\n  process.stdout.write(\"Bot: \");\n  \n  const streamingResponse2 = await chat.sendMessageStream(prompt2);\n  for await (const chunk of streamingResponse2.stream) {\n    if (chunk.candidates[0].content.parts[0].text) {\n      process.stdout.write(chunk.candidates[0].content.parts[0].text);\n    }\n  }\n  console.log(\"\\n\");\n}\n\nstreamChat();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Create a unique session ID\nexport SESSION_ID=$(uuidgen)\n\n# First message with streaming\necho \"User: Tell me about the solar system.\"\necho -n \"Bot: \"\n\nresponse1=$(curl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:streamGenerateContent\" \\\n  --data '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"Tell me about the solar system.\"}]\n      }\n    ]\n  }')\n\n# Store the response for the chat history\nbot_response1=$(echo \"$response1\" | grep -o '\"text\": \"[^\"]*\"' | sed 's/\"text\": \"\\(.*\\)\"/\\1/' | tr -d '\\n')\necho -e \"\\n\"\n\n# Second message with streaming, including conversation history\necho \"User: What is the largest planet?\"\necho -n \"Bot: \"\n\ncurl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:streamGenerateContent\" \\\n  --data '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"Tell me about the solar system.\"}]\n      },\n      {\n        \"role\": \"model\",\n        \"parts\": [{\"text\": \"'\"$bot_response1\"'\"}]\n      },\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"What is the largest planet?\"}]\n      }\n    ]\n  }'\n\necho -e \"\\n\"\n```\n\n</section>\n\n## Processing streaming responses\n\nIn real applications, you'll likely want to process streaming responses more carefully:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Function to process streaming response\ndef process_streaming_response(stream):\n    # Buffer to collect the complete response\n    full_response = \"\"\n    \n    # Process each chunk as it arrives\n    for chunk in stream:\n        # Get the text content from this chunk\n        chunk_text = chunk.text\n        \n        # Update the full response\n        full_response += chunk_text\n        \n        # Process this chunk (in this example, just print it)\n        print(chunk_text, end=\"\", flush=True)\n        \n        # You could perform additional processing here:\n        # - Check for specific keywords\n        # - Update UI elements\n        # - Analyze sentiment as the response develops\n        # - etc.\n    \n    print()  # Add a newline at the end\n    return full_response\n\n# Example usage\nprompt = \"Explain the concept of machine learning in simple terms.\"\nprint(\"Generating response...\\n\")\n\nstream = model.generate_content(prompt, stream=True)\ncomplete_response = process_streaming_response(stream)\n\nprint(\"\\nFull response length:\", len(complete_response))\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n});\n\n// Function to process streaming response\nasync function processStreamingResponse(streamingResponse) {\n  // Buffer to collect the complete response\n  let fullResponse = \"\";\n  \n  // Process each chunk as it arrives\n  for await (const chunk of streamingResponse.stream) {\n    // Get the text content from this chunk\n    const chunkText = chunk.candidates[0].content.parts[0].text || \"\";\n    \n    // Update the full response\n    fullResponse += chunkText;\n    \n    // Process this chunk (in this example, just print it)\n    process.stdout.write(chunkText);\n    \n    // You could perform additional processing here:\n    // - Check for specific keywords\n    // - Update UI elements\n    // - Analyze sentiment as the response develops\n    // - etc.\n  }\n  \n  console.log();  // Add a newline at the end\n  return fullResponse;\n}\n\nasync function main() {\n  const prompt = \"Explain the concept of machine learning in simple terms.\";\n  console.log(\"Generating response...\\n\");\n  \n  const streamingResponse = await generativeModel.generateContentStream({\n    contents: [{ role: \"user\", parts: [{ text: prompt }] }],\n  });\n  \n  const completeResponse = await processStreamingResponse(streamingResponse);\n  \n  console.log(\"\\nFull response length:\", completeResponse.length);\n}\n\nmain();\n```\n\n</section>\n\n## Handling errors in streams\n\nIt's important to handle errors that might occur during streaming:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Function to handle streaming with error handling\ndef stream_with_error_handling(prompt):\n    try:\n        stream = model.generate_content(prompt, stream=True)\n        \n        # Process the stream\n        for chunk in stream:\n            try:\n                print(chunk.text, end=\"\", flush=True)\n            except Exception as chunk_error:\n                print(f\"\\nError processing chunk: {chunk_error}\")\n                \n        print()  # Add a newline at the end\n        \n    except Exception as e:\n        print(f\"\\nError initiating or during stream: {e}\")\n        \n        # You might want to handle different types of errors differently\n        if \"safety\" in str(e).lower():\n            print(\"Content was filtered due to safety concerns.\")\n        elif \"quota\" in str(e).lower():\n            print(\"You've exceeded your API quota. Please try again later.\")\n        else:\n            print(\"An unexpected error occurred. Please try again.\")\n\n# Example usage\nprompt = \"Explain quantum computing in simple terms.\"\nprint(\"Generating response...\\n\")\nstream_with_error_handling(prompt)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n});\n\n// Function to handle streaming with error handling\nasync function streamWithErrorHandling(prompt) {\n  try {\n    const streamingResponse = await generativeModel.generateContentStream({\n      contents: [{ role: \"user\", parts: [{ text: prompt }] }],\n    });\n    \n    // Process the stream\n    for await (const chunk of streamingResponse.stream) {\n      try {\n        if (chunk.candidates[0].content.parts[0].text) {\n          process.stdout.write(chunk.candidates[0].content.parts[0].text);\n        }\n      } catch (chunkError) {\n        console.log(`\\nError processing chunk: ${chunkError.message}`);\n      }\n    }\n    \n    console.log();  // Add a newline at the end\n    \n  } catch (e) {\n    console.log(`\\nError initiating or during stream: ${e.message}`);\n    \n    // You might want to handle different types of errors differently\n    if (e.message.toLowerCase().includes('safety')) {\n      console.log(\"Content was filtered due to safety concerns.\");\n    } else if (e.message.toLowerCase().includes('quota')) {\n      console.log(\"You've exceeded your API quota. Please try again later.\");\n    } else {\n      console.log(\"An unexpected error occurred. Please try again.\");\n    }\n  }\n}\n\n// Example usage\nasync function main() {\n  const prompt = \"Explain quantum computing in simple terms.\";\n  console.log(\"Generating response...\\n\");\n  await streamWithErrorHandling(prompt);\n}\n\nmain();\n```\n\n</section>\n\n## Advanced streaming techniques\n\n### Handling multimodal responses\n\nWhen working with multimodal responses (like text and images), you'll need to handle different content types in the stream:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro-vision\")\n\n# Create a prompt that might generate multimodal content\nprompt = \"Generate a short story about space exploration.\"\n\n# Stream the response and handle different content types\nfor chunk in model.generate_content(\n    prompt,\n    stream=True,\n    generation_config={\"response_mime_type\": \"text/plain,image/png\"}\n):\n    # Check the mime type of this chunk's content\n    if hasattr(chunk, 'text') and chunk.text:\n        # Text content\n        print(chunk.text, end=\"\", flush=True)\n    elif hasattr(chunk, 'parts'):\n        # Process each part\n        for part in chunk.parts:\n            if part.mime_type.startswith(\"image/\"):\n                print(\"\\n[Image generated - saving image...]\\n\")\n                \n                # Save the image\n                with open(\"generated_image.png\", \"wb\") as f:\n                    f.write(part.data)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst fs = require('fs');\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro-vision\",\n});\n\nasync function streamMultimodalResponse() {\n  // Create a prompt that might generate multimodal content\n  const prompt = \"Generate a short story about space exploration.\";\n  \n  const streamingResponse = await generativeModel.generateContentStream({\n    contents: [{ role: \"user\", parts: [{ text: prompt }] }],\n    generation_config: {\n      response_mime_type: \"text/plain,image/png\"\n    }\n  });\n  \n  // Process the streaming response\n  for await (const chunk of streamingResponse.stream) {\n    // Process each candidate\n    for (const candidate of chunk.candidates) {\n      // Process each part in the content\n      for (const part of candidate.content.parts) {\n        if (part.text) {\n          // Text content\n          process.stdout.write(part.text);\n        } else if (part.inline_data && part.inline_data.mime_type.startsWith(\"image/\")) {\n          console.log(\"\\n[Image generated - saving image...]\\n\");\n          \n          // Save the image\n          fs.writeFileSync(\n            \"generated_image.png\", \n            Buffer.from(part.inline_data.data, 'base64')\n          );\n        }\n      }\n    }\n  }\n  \n  console.log(); // New line at the end\n}\n\nstreamMultimodalResponse();\n```\n\n</section>\n\n### Real-time analysis during streaming\n\nYou can analyze the content as it streams, for example to detect certain keywords or sentiment:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Function to analyze stream in real-time\ndef analyze_stream(prompt, keywords_to_detect=None):\n    if keywords_to_detect is None:\n        keywords_to_detect = [\"important\", \"key\", \"significant\", \"critical\"]\n    \n    stream = model.generate_content(prompt, stream=True)\n    \n    # Initialize variables\n    full_response = \"\"\n    keyword_found = False\n    \n    # Process each chunk\n    for chunk in stream:\n        chunk_text = chunk.text\n        full_response += chunk_text\n        \n        # Print the chunk\n        print(chunk_text, end=\"\", flush=True)\n        \n        # Check for keywords in this chunk\n        for keyword in keywords_to_detect:\n            if keyword.lower() in chunk_text.lower() and not keyword_found:\n                print(f\"\\n\\n[ALERT: Keyword '{keyword}' detected!]\\n\\n\", end=\"\", flush=True)\n                keyword_found = True\n                break\n    \n    print(\"\\n\")\n    \n    # Final analysis\n    word_count = len(full_response.split())\n    print(f\"Response complete: {word_count} words\")\n    \n    if keyword_found:\n        print(\"Keywords were detected in this response.\")\n    else:\n        print(\"No monitored keywords were detected.\")\n    \n    return full_response\n\n# Example usage\nprompt = \"Explain why cybersecurity is important for businesses.\"\nprint(\"Analyzing response as it streams...\\n\")\nanalyze_stream(prompt)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n});\n\n// Function to analyze stream in real-time\nasync function analyzeStream(prompt, keywordsToDetect = null) {\n  if (keywordsToDetect === null) {\n    keywordsToDetect = [\"important\", \"key\", \"significant\", \"critical\"];\n  }\n  \n  const streamingResponse = await generativeModel.generateContentStream({\n    contents: [{ role: \"user\", parts: [{ text: prompt }] }],\n  });\n  \n  // Initialize variables\n  let fullResponse = \"\";\n  let keywordFound = false;\n  \n  // Process each chunk\n  for await (const chunk of streamingResponse.stream) {\n    const chunkText = chunk.candidates[0].content.parts[0].text || \"\";\n    fullResponse += chunkText;\n    \n    // Print the chunk\n    process.stdout.write(chunkText);\n    \n    // Check for keywords in this chunk\n    for (const keyword of keywordsToDetect) {\n      if (chunkText.toLowerCase().includes(keyword.toLowerCase()) && !keywordFound) {\n        process.stdout.write(`\\n\\n[ALERT: Keyword '${keyword}' detected!]\\n\\n`);\n        keywordFound = true;\n        break;\n      }\n    }\n  }\n  \n  console.log(\"\\n\");\n  \n  // Final analysis\n  const wordCount = fullResponse.split(/\\s+/).length;\n  console.log(`Response complete: ${wordCount} words`);\n  \n  if (keywordFound) {\n    console.log(\"Keywords were detected in this response.\");\n  } else {\n    console.log(\"No monitored keywords were detected.\");\n  }\n  \n  return fullResponse;\n}\n\n// Example usage\nasync function main() {\n  const prompt = \"Explain why cybersecurity is important for businesses.\";\n  console.log(\"Analyzing response as it streams...\\n\");\n  await analyzeStream(prompt);\n}\n\nmain();\n```\n\n</section>\n\n## Best practices\n\n1. **Handle errors gracefully**: Implement proper error handling for stream interruptions or API issues.\n\n2. **Buffer carefully**: Decide if you need to maintain the full response or can process chunks independently.\n\n3. **Update UI progressively**: In web applications, update the UI as each chunk arrives for a responsive experience.\n\n4. **Monitor token usage**: Track token usage to avoid unexpectedly reaching API limits.\n\n5. **Test network conditions**: Test your streaming implementation under various network conditions.\n\n6. **Process efficiently**: Keep chunk processing lightweight to avoid blocking the stream.\n\n7. **Provide feedback**: Give users visual feedback that content is being generated (e.g., typing indicators).\n\n## Common use cases\n\n- **Chatbots**: Create responsive conversational interfaces\n- **Content generation**: Show progress for long-form content generation\n- **Real-time analysis**: Process content as it's generated\n- **Document creation**: Build documents progressively with user feedback\n- **Interactive education**: Generate explanations that users can begin reading immediately\n\n## Limitations\n\n- **Network interruptions**: Streaming is more vulnerable to network issues than single requests\n- **Complexity**: Streaming adds complexity to error handling and state management\n- **Partial responses**: You need to handle cases where streams are interrupted\n- **Limited browser support**: Some older browsers have limited support for streaming\n\n## Next steps\n\n- Implement streaming in your Gemini-powered applications\n- Explore more advanced streaming patterns for your specific use case\n- Consider adding user feedback options during streaming (like the ability to stop generation)\n- Test your streaming implementation across different network conditions\n\u0005End File\u0006# site/en/vertex/reference/generation-parameters.md\n# Generation parameters for Vertex AI Gemini models\n\nGeneration parameters allow you to control how Gemini models generate content. This reference guide explains the available parameters and how to use them effectively.\n\n## Available parameters\n\nThe following generation parameters are available for Vertex AI Gemini models:\n\n| Parameter | Description | Type | Range | Default |\n|-----------|-------------|------|-------|---------|\n| `temperature` | Controls randomness in generation | Float | 0.0 - 1.0 | 0.7 |\n| `top_p` | Controls diversity via nucleus sampling | Float | 0.0 - 1.0 | 0.95 |\n| `top_k` | Limits token selection to top k options | Integer | 1 - 100 | 40 |\n| `max_output_tokens` | Maximum tokens to generate | Integer | 1 - varies by model | model-dependent |\n| `candidate_count` | Number of response candidates to generate | Integer | 1 - 8 | 1 |\n| `stop_sequences` | List of strings that stop generation when encountered | List of strings | - | [] |\n| `response_mime_type` | MIME type for response format | String | \"text/plain\", \"image/png\", etc. | \"text/plain\" |\n\n## Setting generation parameters\n\nYou can set generation parameters when making API calls to Gemini models:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\n# Generate content with custom parameters\nresponse = model.generate_content(\n    \"Write a short poem about the ocean\",\n    generation_config={\n        \"temperature\": 0.9,\n        \"top_p\": 0.8,\n        \"top_k\": 40,\n        \"max_output_tokens\": 200,\n        \"candidate_count\": 1\n    }\n)\n\nprint(response.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n});\n\nasync function generateWithParameters() {\n  const result = await generativeModel.generateContent({\n    contents: [{ role: \"user\", parts: [{ text: \"Write a short poem about the ocean\" }] }],\n    generationConfig: {\n      temperature: 0.9,\n      topP: 0.8,\n      topK: 40,\n      maxOutputTokens: 200,\n      candidateCount: 1\n    }\n  });\n\n  console.log(result.response.text());\n}\n\ngenerateWithParameters();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Set environment variables\nexport PROJECT_ID=\"YOUR_PROJECT_ID\"\nexport LOCATION=\"us-central1\"\nexport MODEL=\"gemini-1.5-pro\"\n\n# Get an access token\nexport TOKEN=$(gcloud auth print-access-token)\n\n# Make the API request with generation parameters\ncurl -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"Write a short poem about the ocean\"}]\n      }\n    ],\n    \"generation_config\": {\n      \"temperature\": 0.9,\n      \"top_p\": 0.8,\n      \"top_k\": 40,\n      \"max_output_tokens\": 200,\n      \"candidate_count\": 1\n    }\n  }'\n```\n\n</section>\n\n## Understanding temperature\n\nTemperature controls the randomness of the model's output. Higher values make the output more random and creative, while lower values make it more deterministic and focused.\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\nprompt = \"Write a tagline for a coffee shop\"\n\n# Low temperature (deterministic)\nlow_temp_response = model.generate_content(\n    prompt,\n    generation_config={\"temperature\": 0.1}\n)\nprint(\"Temperature 0.1:\")\nprint(low_temp_response.text)\nprint()\n\n# Medium temperature (balanced)\nmed_temp_response = model.generate_content(\n    prompt,\n    generation_config={\"temperature\": 0.5}\n)\nprint(\"Temperature 0.5:\")\nprint(med_temp_response.text)\nprint()\n\n# High temperature (creative)\nhigh_temp_response = model.generate_content(\n    prompt,\n    generation_config={\"temperature\": 0.9}\n)\nprint(\"Temperature 0.9:\")\nprint(high_temp_response.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n});\n\nasync function testTemperatures() {\n  const prompt = \"Write a tagline for a coffee shop\";\n  \n  // Low temperature (deterministic)\n  const lowTempResult = await generativeModel.generateContent({\n    contents: [{ role: \"user\", parts: [{ text: prompt }] }],\n    generationConfig: { temperature: 0.1 }\n  });\n  console.log(\"Temperature 0.1:\");\n  console.log(lowTempResult.response.text());\n  console.log();\n  \n  // Medium temperature (balanced)\n  const medTempResult = await generativeModel.generateContent({\n    contents: [{ role: \"user\", parts: [{ text: prompt }] }],\n    generationConfig: { temperature: 0.5 }\n  });\n  console.log(\"Temperature 0.5:\");\n  console.log(medTempResult.response.text());\n  console.log();\n  \n  // High temperature (creative)\n  const highTempResult = await generativeModel.generateContent({\n    contents: [{ role: \"user\", parts: [{ text: prompt }] }],\n    generationConfig: { temperature: 0.9 }\n  });\n  console.log(\"Temperature 0.9:\");\n  console.log(highTempResult.response.text());\n}\n\ntestTemperatures();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Test with low temperature (deterministic)\necho \"Temperature 0.1:\"\ncurl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"Write a tagline for a coffee shop\"}]\n      }\n    ],\n    \"generation_config\": {\n      \"temperature\": 0.1\n    }\n  }' | jq -r '.candidates[0].content.parts[0].text'\necho\n\n# Test with medium temperature (balanced)\necho \"Temperature 0.5:\"\ncurl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"Write a tagline for a coffee shop\"}]\n      }\n    ],\n    \"generation_config\": {\n      \"temperature\": 0.5\n    }\n  }' | jq -r '.candidates[0].content.parts[0].text'\necho\n\n# Test with high temperature (creative)\necho \"Temperature 0.9:\"\ncurl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"Write a tagline for a coffee shop\"}]\n      }\n    ],\n    \"generation_config\": {\n      \"temperature\": 0.9\n    }\n  }' | jq -r '.candidates[0].content.parts[0].text'\n```\n\n</section>\n\n## Using top_p and top_k\n\nTop-p (nucleus sampling) and top-k sampling are alternative ways to control randomness:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\nprompt = \"Give me ideas for a science fiction story\"\n\n# Using top_p\ntop_p_response = model.generate_content(\n    prompt,\n    generation_config={\n        \"temperature\": 0.7,\n        \"top_p\": 0.3  # Only consider tokens in the top 30% of probability mass\n    }\n)\nprint(\"Using top_p=0.3:\")\nprint(top_p_response.text)\nprint()\n\n# Using top_k\ntop_k_response = model.generate_content(\n    prompt,\n    generation_config={\n        \"temperature\": 0.7,\n        \"top_k\": 10  # Only consider the top 10 token options\n    }\n)\nprint(\"Using top_k=10:\")\nprint(top_k_response.text)\nprint()\n\n# Using both\ncombined_response = model.generate_content(\n    prompt,\n    generation_config={\n        \"temperature\": 0.7,\n        \"top_p\": 0.3,\n        \"top_k\": 10\n    }\n)\nprint(\"Using top_p=0.3 and top_k=10:\")\nprint(combined_response.text)\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n});\n\nasync function testTopPandTopK() {\n  const prompt = \"Give me ideas for a science fiction story\";\n  \n  // Using top_p\n  const topPResult = await generativeModel.generateContent({\n    contents: [{ role: \"user\", parts: [{ text: prompt }] }],\n    generationConfig: {\n      temperature: 0.7,\n      topP: 0.3  // Only consider tokens in the top 30% of probability mass\n    }\n  });\n  console.log(\"Using top_p=0.3:\");\n  console.log(topPResult.response.text());\n  console.log();\n  \n  // Using top_k\n  const topKResult = await generativeModel.generateContent({\n    contents: [{ role: \"user\", parts: [{ text: prompt }] }],\n    generationConfig: {\n      temperature: 0.7,\n      topK: 10  // Only consider the top 10 token options\n    }\n  });\n  console.log(\"Using top_k=10:\");\n  console.log(topKResult.response.text());\n  console.log();\n  \n  // Using both\n  const combinedResult = await generativeModel.generateContent({\n    contents: [{ role: \"user\", parts: [{ text: prompt }] }],\n    generationConfig: {\n      temperature: 0.7,\n      topP: 0.3,\n      topK: 10\n    }\n  });\n  console.log(\"Using top_p=0.3 and top_k=10:\");\n  console.log(combinedResult.response.text());\n}\n\ntestTopPandTopK();\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"REST\">\n\n```bash\n# Test with top_p\necho \"Using top_p=0.3:\"\ncurl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"Give me ideas for a science fiction story\"}]\n      }\n    ],\n    \"generation_config\": {\n      \"temperature\": 0.7,\n      \"top_p\": 0.3\n    }\n  }' | jq -r '.candidates[0].content.parts[0].text'\necho\n\n# Test with top_k\necho \"Using top_k=10:\"\ncurl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"Give me ideas for a science fiction story\"}]\n      }\n    ],\n    \"generation_config\": {\n      \"temperature\": 0.7,\n      \"top_k\": 10\n    }\n  }' | jq -r '.candidates[0].content.parts[0].text'\necho\n\n# Test with both\necho \"Using top_p=0.3 and top_k=10:\"\ncurl -s -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL}:generateContent\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [{\"text\": \"Give me ideas for a science fiction story\"}]\n      }\n    ],\n    \"generation_config\": {\n      \"temperature\": 0.7,\n      \"top_p\": 0.3,\n      \"top_k\": 10\n    }\n  }' | jq -r '.candidates[0].content.parts[0].text'\n```\n\n</section>\n\n## Controlling output length\n\nYou can control the maximum length of the generated output:\n\n<section class=\"zippy\" data-name=\"Python\">\n\n```python\nfrom vertexai.generative_models import GenerativeModel\n\n# Initialize the model\nmodel = GenerativeModel(\"gemini-1.5-pro\")\n\nprompt = \"Write an essay about artificial intelligence\"\n\n# Short output\nshort_response = model.generate_content(\n    prompt,\n    generation_config={\"max_output_tokens\": 50}\n)\nprint(\"Short output (max 50 tokens):\")\nprint(short_response.text)\nprint(f\"Character count: {len(short_response.text)}\")\nprint()\n\n# Medium output\nmedium_response = model.generate_content(\n    prompt,\n    generation_config={\"max_output_tokens\": 200}\n)\nprint(\"Medium output (max 200 tokens):\")\nprint(medium_response.text)\nprint(f\"Character count: {len(medium_response.text)}\")\nprint()\n\n# Long output\nlong_response = model.generate_content(\n    prompt,\n    generation_config={\"max_output_tokens\": 800}\n)\nprint(\"Long output (max 800 tokens):\")\nprint(long_response.text)\nprint(f\"Character count: {len(long_response.text)}\")\n```\n\n</section>\n\n<section class=\"zippy\" data-name=\"Node.js\">\n\n```javascript\nconst {VertexAI} = require('@google-cloud/vertexai');\n\n// Initialize Vertex with your Cloud project and location\nconst vertex_ai = new VertexAI({project: 'YOUR_PROJECT_ID', location: 'us-central1'});\n\n// Initialize the model\nconst generativeModel = vertex_ai.getGenerativeModel({\n  model: \"gemini-1.5-pro\",\n});\n\nasync function testOutputLength() {\n  const prompt = \"Write an essay about artificial intelligence\";\n  \n  // Short output\n  const shortResult = await generativeModel.generateContent({\n    contents: [{ role: \"user\", parts: [{ text: prompt }] }],\n    generationConfig: { maxOutputTokens: 50 }\n  });\n  const shortText = shortResult.response.text();",
    "docsie-io-named-an-emerging-favorite-by-|category|1": "Produkthantering",
    "docsie-io-named-an-emerging-favorite-by-|category|2": "Produktuppdateringar\n\u0005End File\u0006# sse-lab/SemanticDrifts\n# data/prompts_text/SYS_PROMPT_summarize_LNG_zh_C42.prompt\nsystem: As a language translator, your task is to translate text from Chinese to English.\n\nPlease adhere to the following guidelines:\n1. Provide only the translated text, without explanations or additional context.\n2. Keep translations accurate, clear, and natural, preserving the tone of the original text.\n3. Maintain the same level of formality or informality as the original.\n4. Preserve paragraph structures, bullets, and other formatting elements as in the original text.\n5. Translate idioms to their natural English counterparts rather than literal translations.\n6. When translating content with multiple paragraphs, maintain the original structure.",
    "docsie-io-named-an-emerging-favorite-by-|category|3": "Bästa praxis",
    "docsie-io-named-an-emerging-favorite-by-|category|4": "Tekniskt skrivande",
    "docsie-io-named-an-emerging-favorite-by-|category|5": "API-dokumentation",
    "docsie-io-named-an-emerging-favorite-by-|category|6": "Docsie-pris"
}