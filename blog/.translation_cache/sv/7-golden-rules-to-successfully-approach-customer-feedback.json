{
    "__metadata__": {
        "original_categories": [
            "Product Documentation",
            "AI",
            "Product Management",
            "Documentation Portals",
            "Best Practices",
            "Technical Writing",
            "Product Documentation Tutorials",
            "Customer Feedback"
        ],
        "author_name": "Tal F.",
        "author_email": "tal@docsie.io",
        "author_info": "VP of Customer Success @ Docsie.io",
        "author_image": " https://cdn.docsie.io/user_profiles/15/logo_logo_QmXrbijvL0L2hFKNm6Q25DtjahujKdB6nu4pqBlLBgvtT.png",
        "header_image": "https://cdn.docsie.io/workspace_8D5W1pxgb7Jq3oZO7/doc_QpDdxIGnXpT0d02oQ/file_hG0WwhFWFhAWE8UZz/boo_XGfvRm3TVTFbV6HET/8898fddf-00fe-12c4-2c8e-c4ecec8ef0cbUntitled_1_min_1_2_3_4_5_6_7_8_9_10.jpg",
        "timestamp": "2021-07-06T16:10:25+00:00",
        "status": 1
    },
    "7-golden-rules-to-successfully-approach-|title": "7 gyllene regler för att framgångsrikt hantera kundåterkoppling",
    "7-golden-rules-to-successfully-approach-|summary": "Kundåterkoppling är i framkant för att skapa och optimera tilltalande och omfattande produktdokumentation som dina kunder kommer att förstå.",
    "7-golden-rules-to-successfully-approach-|markdown": "![](https://images.unsplash.com/photo-1516321318423-f06f85e504b3?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwzMTM3MXwwfDF8c2VhcmNofDQ0fHxsaXN0ZW4lMjB0byUyMGN1c3RvbWVyfGVufDB8fHx8MTYyNTYwMTA3Mg&ixlib=rb-1.2.1&q=80&w=1080)\n\nKundfeedback är grunden för att skapa och optimera tilltalande produktdokumentation som dina kunder förstår och använder för att hjälpa dem med dina SaaS- eller fysiska produkter.\n\nMånga företag är rädda för att få veta vad deras kunder verkligen tycker om dokumentationen, vilket skapar hinder för att ta reda på kundernas faktiska åsikter om produktdokumentationen.\n\nVarför är detta viktigt? Varför behöver vi veta vad våra kunder tycker om vår produktdokumentation?\n\nOm du driver ett eget företag anstränger du dig säkert för att glädja dina kunder, möta deras krav och behålla deras lojalitet. Men hur kan du vara säker på att dina ansträngningar ger önskat resultat? Du kommer aldrig att kunna ge dina kunder den optimala dokumentationsupplevelsen om du inte tar reda på vad de tycker om din service. Deras feedback om upplevelsen när de läser och bläddrar i din onlinedokumentation är värdefull information som du kan använda för att bättre anpassa dokumentationen efter deras behov.\n\nDu kan använda kundfeedback till din fördel på många sätt. I den här artikeln försöker jag visa varför insamling av kundfeedback om din dokumentation, oavsett om den är efterfrågad eller inte, är avgörande för kundnöjdhet, lojalitet, kundretention, produkt- och tjänsteförbättring och många andra aspekter av din verksamhet. Fortsätt läsa för att lära dig fördelarna med feedback på produktdokumentation.\n\nVad innebär feedback på produktdokumentation?\n\nKundfeedback är information från kunder om deras nöjdhet eller missnöje med produktdokumentation, användarmanual och all onlinedokumentation som företaget tillhandahåller. Deras feedback kan hjälpa dig förbättra kundupplevelsen och anpassa dina aktiviteter efter deras behov. Denna information kan samlas in genom olika typer av undersökningar (efterfrågad feedback), men du kan också hitta och samla in spontan feedback från dina kunder genom internetövervakningsverktyg, som Vocally – en funktion i Docsie som låter dig se hur dina kunder interagerar med din publicerade onlinedokumentation. Båda källorna är nödvändiga för att få en komplett bild av hur dina kunder ser på ditt varumärke.\n\nHögpresterande organisationer förstår vikten av kundfeedback i sin verksamhet. De lyssnar regelbundet på sina kunders röster. De söker inte bara efter tankar som kunder delar på sociala medier och recensioner på feedbacksajter (som TripAdvisor), utan de ber också direkt om feedback genom olika typer av enkäter. Om du vill hålla dig före konkurrenterna måste du alltid vara uppmärksam på kundfeedback, oavsett om den är positiv eller negativ, efterfrågad eller spontan.\n\n## Varför är det viktigt?\n\n![](https://images.unsplash.com/photo-1531537571171-a707bf2683da?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwzMTM3MXwwfDF8c2VhcmNofDF8fGxpc3RlbmluZ3xlbnwwfHx8fDE2MjU2MDEwMTg&ixlib=rb-1.2.1&q=80&w=1080)\n\nMånga SaaS- och ingenjörsprodukter har olika användarguider, manualer och andra typer av produktdokumentation för sina kunder. Denna dokumentation kan översättas till olika språk och finnas i många versioner. Så varför är det viktigt att veta vad dina kunder tänker när de tittar på ditt företags produktdokumentation? Enkelt. Produktdokumentation är utformad för att förklara hur din produkt fungerar, och om denna förklaring inte är begriplig för dina kunder kan det leda till avhopp och irriterade kunder. Feedback hjälper till att minska gapet mellan deras missförstånd och optimerar dokumentationen så att de kan förstå materialet bättre. Detta gör i slutändan dina kunder nöjdare eftersom de nu förstår hur de ska använda produktdokumentationen som den var tänkt, vilket skapar förtroende för ditt varumärke och formar ditt företags rykte på ett positivt sätt.\n\nFör det andra kan feedback ge produktchefer, produktägare, tekniska skribenter och till och med teknikteam information som de kan använda för att förbättra produktdokumentationens layout och innehåll för att öka förståelsen. Detta är alltid målet när man publicerar produktdokumentation.\n\nHär är de sju viktigaste anledningarna till varför kundfeedback är avgörande i affärsvärlden.\n\n### 1. Kundfeedback hjälper till att utveckla bättre produktdokumentation.\n\nNär du först lanserar en ny produkt, varumärke eller tjänst har du troligen en uppfattning om kundernas behov. Marknadsundersökningar före lanseringen kan hjälpa dig avgöra om potentiella kunder är villiga att köpa den och ge dig förslag på förbättringar. Efter att du lanserat dina produkter behöver du skapa lämpliga användarguider och onlinedokumentation genom att dela din kunskapsportal med dina kunder online. Du anlitar en teknisk skribent för att skriva information om din produkt. En översättare för att översätta den och får ditt teknikteam att visa den online. Men du kommer bara att höra om alla fördelar, fel och kundernas faktiska upplevelse efter att de använt dina produkter tillsammans med din produktdokumentation och gett dig och ditt team nödvändig feedback.\n\nDenna kundfeedback ger insikt i vilka aspekter av din produktdokumentation som fungerar effektivt och vilka förändringar som kan göras för att förbättra upplevelsen. Din yrkeskunskap kan vara den bästa i branschen där ditt företag verkar, men kundinsikter är alltid mer användbara för att finslipa produktdokumentation som dina kunder förstår. Eftersom du står nära dina produkter är det bra att få deras feedback som hjälper dig säkerställa att den slutliga produktdokumentationen möter deras förväntningar, löser deras problem, klargör eventuella missförstånd och uppfyller deras krav.\n\n### 2. Kundfeedback hjälper till att utvärdera kundnöjdhetsnivån.\n\nKundlojalitet och nöjdhet är viktiga faktorer för ett företags ekonomiska resultat. Det är kopplat till en rad fördelar, inklusive förbättrad marknadsandel, lägre kostnader och ökade intäkter. Flera studier har visat ett starkt samband mellan kundnöjdhet och företagets resultat. Det finns ingen tvekan om att du vill säkerställa att dina kunder är nöjda med din produktdokumentation och förstår hur de ska använda dina produkter effektivt. Att få deras feedback är förstås det bästa sättet att se om du har uppfyllt deras förväntningar. Genom att använda betygbaserade frågor kan du enkelt bedöma nöjdhetsnivån och därmed förutse ditt företags ekonomiska situation i framtiden.\n\nNPS är en av de mest exakta metoderna för att mäta, hantera och förbättra kundnöjdhet som används av många organisationer. Mätningen baseras på en enda fråga som frågar om en kund sannolikt skulle rekommendera ett varumärke till en vän. Lojalitetsfrågorna ger svarsalternativ från 0 till 5, där 0 är mycket negativt och 5 mycket positivt. Eftersom denna metod är både enkel och allmänt utbredd kan den användas i kundnöjdhetshantering av alla företag. Vocally använder också detta betygsystem och låter dessutom våra kunder spela in videor av hur deras kunder använder dokumentationen. Denna information är mycket viktig och ger företag konkurrensfördelen att analysera kundernas feedback och leverera framtida optimeringar av produktdokumentationen.\n\n### 3. Att samla in kundfeedback visar att du bryr dig om vad de har att säga.\n\nGenom att be om synpunkter från dina kunder visar du att du värdesätter deras input. Du inkluderar dem i utvecklingen av ditt företag, dina produkter och produktdokumentation/användarguider så att de känner sig mer knutna till det. Du kan stärka din relation med dem genom att lyssna på deras röst. Detta är det effektivaste sättet att rekrytera viktiga varumärkesambassadörer som hjälper dig sprida positiv ryktesspridning. Och jag är säker på att du vet att förlita sig på deras rekommendationer förmodligen är det mest framgångsrika och kostnadseffektiva sättet att få nya kunder och öka din trovärdighet i nuvarande och framtida kunders ögon.\n\nFolk uppskattar när du frågar om deras nöjdhet (eller missnöje) med hur ditt företag formulerat din produktdokumentation. Det visar att du värdesätter deras synpunkt och är här för att hjälpa dem, inte tvärtom. De tror att ditt huvudsyfte med produktdokumentation är att lösa deras missförstånd och hjälpa dem att använda dina produkter effektivt, vilket är ett bra sätt att behålla kunder.\n\n### 4. Kundfeedback hjälper till att skapa bästa möjliga kundupplevelse.\n\nDagens marknadsföring påverkas starkt av människors interaktion med produkter, tjänster och varumärken. Folk köper inte märkesvaror bara för att de är bra. De söker kvalitativ kundservice, användarupplevelse och en övergripande god förståelse för produkternas specifikationer och användningsområden som matchar deras behov. Kunder förblir lojala mot ditt varumärke om du fokuserar på att erbjuda den bästa kundupplevelsen vid varje kontaktpunkt. Och det mest effektiva sättet att ge kunderna en fantastisk upplevelse är att fråga dem vad de uppskattar med din produktdokumentation som visar hur dina produkter används och vad de tror kan förbättras. Detta gäller särskilt för SaaS-produkter som har många facetter och tekniska aspekter. Ju mer utbildade dina kunder är, desto bättre förstår de användningsområdena för dina produkter och desto mer vill de stanna kvar och förbli lojala.\n\n### 5. Kundfeedback hjälper till att behålla kunder.\n\nKunder som är nöjda med din service fortsätter att göra affärer med dig. Missnöjda kunder som inte förstår din produktdokumentation och ständigt blir förvirrade över hur de ska använda din produkt kommer i slutändan att söka ett bättre alternativ till ditt företag. Kundfeedback kan hjälpa dig att ta reda på var dina kunder blir förvirrade med din produktdokumentation och vad som gjorde dem nöjda med kunskapsportalen för dina produkter, samt identifiera områden där du kan förbättra. Du kan alltid ha fingret på pulsen om du regelbundet ber om feedback. När en missnöjd kund uttrycker sitt missnöje kan du reagera snabbt och hitta en lösning på deras problem. Detta är ett idealiskt tillfälle att återvinna en kund och kanske förbättra deras lojalitetsnivå. I många fall visar en missnöjd kund som hade problem med dina produkter, tjänster och/eller användarguider men fick problemet snabbt löst större lojalitet mot ditt varumärke än en kund som aldrig varit missnöjd med dina produkter eller tjänster.\n\n### 6. Kundfeedback är en pålitlig informationskälla för andra konsumenter.\n\nI marknadsföringsvärlden är det mycket viktigt att samla in användbar data och information för att optimera och fortsätta öka kundretention och minska avhopp. I sociala mediers tidsålder tror konsumenter inte längre på reklam eller expertråd. Numera är åsikter från andra konsumenter som har använt en produkt eller tjänst en mer trovärdig informationskälla. När du letar efter ett ställe att bo på i en stad du besöker eller en trevlig ny restaurang att äta på med vänner, kollar du recensioner först. När du vill köpa nya skor ber du om råd på Facebook eller läser en recension på en trovärdig sajt. Recensioner gäller för alla företag, särskilt SaaS-produkter. Många företag inkluderar nu ett recensionssystem som en del av sina tjänster och produkter. Detta säkerställer inte bara att deras varumärken kan formas av recensenterna utan också att de kan arbeta hårt för att vinna nya kunders förtroende och försöka behålla dem på lång sikt. Kundfeedback är lika viktig för ditt företag som för andra kunder, så se till att du och dina kunder enkelt kan komma åt kommentarer och utvärderingar.\n\n### 7. Kundfeedback ger dig information som hjälper dig att fatta affärsbeslut.\n\n![](https://images.unsplash.com/photo-1607000975509-de2f74eb8d36?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwzMTM3MXwwfDF8c2VhcmNofDE0fHx0cnVzdHxlbnwwfHx8fDE2MjU2MDEyNTY&ixlib=rb-1.2.1&q=80&w=1080)\n\nPå en mycket konkurrensutsatt marknad har affärsbeslut baserade på gissningar ingen plats. Framgångsrika företagare samlar in och hanterar en viss typ av information som hjälper till att utveckla framtida initiativ. Bara på detta sätt kan de anpassa sina produkter och tjänster för att exakt möta kundernas behov.\n\nKundfeedback är en av de mest pålitliga källorna till konkret data som kan användas för att fatta affärsbeslut. Kundinsikter kan hjälpa dig att bättre förstå din kundkrets och deras behov. Ta deras råd i beaktande, så kommer du att kunna räkna ut var du bör investera dina pengar för att få bästa möjliga avkastning. Du kanske upptäcker att i ditt fall är mer produktutveckling inte nödvändig, utan att du istället bör fokusera på att marknadsföra ditt varumärke för att få mer synlighet. Kundfeedback är en bra källa till sådan information, men du måste lära dig att lyssna på den och omvandla den till meningsfulla affärsinsikter.\n\nVocally (en Docsie-driven funktion) låter dig sätta dina kunder i centrum för din verksamhet och betrakta deras input som den viktigaste informationskällan i din organisation så att du kan hålla dig uppdaterad. Dina kunder är de som använder dina produkter och tjänster, därför är de de bästa personerna att berätta för dig hur du kan göra dem ännu nöjdare. Bortse aldrig från deras röst. Om du misslyckas med att uppfylla deras förväntningar kommer dina kunder att gå till ett annat företag som kan göra det bättre. Kundfeedback bör användas på alla nivåer i din organisation och i alla avdelningar. Insikter kommer att hjälpa till i utvecklingen av dina produkter, förbättra kundservice och hantera kundnöjdhet. Kundfeedback säkerställer att dina kunder stannar hos dig, förblir lojala mot ditt varumärke och därmed sprider positiv ryktesspridning om dig. Att ha engagerade varumärkesambassadörer är ovärderligt när det gäller att få ditt företag att växa!\n\nKom ihåg att kundfeedback kan hittas överallt. Lär dig hur du samlar in den med hjälp av Vocally. Använd feedbacken du får från dina kunder som en dörröppnare, så att du kan diskutera din produktdokumentation, dina produkter och tjänster och samla mer information om hur du kan gynna dina kunder och göra dem lojala för alltid mot din produkt och tjänster. Dina kunder kommer att uppskatta att du värdesätter deras input. Det är avgörande för att bygga djupa band med din publik. Kundfeedback är ovärderlig för ditt företag, så sluta aldrig lyssna!\n\nLåt oss övervinna rädslan för att höra feedback från våra kunder eftersom vi med deras feedback kan förbättra vår produktdokumentation, produkter och tjänster samt kundservice. Låt oss utforska hur Vocally – en Docsie-driven feedbackfunktion – kan hjälpa oss förstå hur våra kunder använder vår dokumentation och vad de gillar eller ogillar med den.\n\n### Komma igång med Docsie-feedback:\n\n### Här är en video med djupgående instruktioner om hur du konfigurerar och använder Vocally med dina Docsie-kunskapsportaler:\n\n<!-- blank line -->\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/F9SGMzCjLF8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n<!-- blank line -->\n\nNär du har publicerat din Docsie-portal och klickar på Docsies molnbaserade kunskapsportal kan du se detta längst ned:\n\n![](https://cdn.docsie.io/workspace_WxPJSQ5gsES8Bzjxy/doc_ydgtE07E6Rp4AMmKv/file_HNt1rBjiROJQ2H7lm/boo_tZD1ykTPGDgzextMB/9e6e8892-7334-d462-c42d-52b0fe4abae4Snag_d7a0c28.png)\n\n![](https://cdn.docsie.io/workspace_WxPJSQ5gsES8Bzjxy/doc_ydgtE07E6Rp4AMmKv/file_Geoc0jqHeoWcbthnO/boo_tZD1ykTPGDgzextMB/bff4d0cc-c94f-380e-b1c7-e1453ab12d3dSnag_d7a6cb7.png)\n\nNär våra kunder som tittar på vår dokumentation klickar på denna knapp och skickar oss feedback, samlas deras feedback automatiskt in och kan ses i vår webbanalys inom Docsie. Men det är inte allt! Vi kan också se en kort video av hur våra kunder interagerade med vår dokumentation!\n\nLåt mig visa dig hur!\n\nEfter att vår klient eller kund skickat den trevliga lilla feedbacken:\n\n![](https://cdn.docsie.io/workspace_WxPJSQ5gsES8Bzjxy/doc_ydgtE07E6Rp4AMmKv/file_yGdEfZA4ZQSxUAdDV/boo_tZD1ykTPGDgzextMB/add6328a-0f22-b9cb-dbf4-0ebfc0467ca9Snag_d7b9057.png)\n\nVi kan se den i Docsie. Först behöver vi gå till Docsie-plattformen och till vänster ser vi \"workspace, notification feed\" och under dessa flikar ser vi \"Web Analytics\" med en glob.\n\n![](https://cdn.docsie.io/workspace_WxPJSQ5gsES8Bzjxy/doc_ydgtE07E6Rp4AMmKv/file_Uwq3GfsNK7QCj9St4/boo_tZD1ykTPGDgzextMB/3ab0e698-c291-cb21-0199-d6ab57198128Snag_d7c1bed.png)\n\nOm vi trycker på den och öppnar Web analytics-knappen visas Docsies feedbackplattform:\n\n![](https://cdn.docsie.io/workspace_WxPJSQ5gsES8Bzjxy/doc_ydgtE07E6Rp4AMmKv/file_OjUjUzFHHGwlnoIdg/boo_tZD1ykTPGDgzextMB/4ba81e13-dbac-f279-0856-4052d5f14c4eSnag_d7cdda7.png)\n\n**Denna plattform visar oss allt vi behöver veta om feedbacken vi har samlat in, vår kund som skickade den, datumet de skickade den och vilken portal de hänvisar till.**\n\n1. Vi kan spela upp videon när vi vill med denna spela upp- och pausknapp.\n\n2. Detta låter oss ändra hastigheten på videon som spelas upp. Detta är särskilt praktiskt för stora dokumentationsportaler som förhandsgranskas av våra kunder så att vi snabbare kan identifiera var de hade problem.\n\n3. Detta berättar vilken portal de har skickat denna feedback från.\n\n4. Detta är deras kommentar.\n\n5. Detta visar hur mycket de gillade eller ogillade dokumentationen.\n\n6. Om du har tillåtit feedback kan Docsie visa endast vissa delar du behöver analysera baserat på datum.\n\n7. Konfigurationsknappen hjälper oss göra ändringar i CSS och andra aspekter av Vocallys feedbackverktyg.\n\nLåt oss nu titta på vilka funktioner Vocally erbjuder genom att klicka på \"Configuration\"-knappen:\n\n![](https://cdn.docsie.io/workspace_WxPJSQ5gsES8Bzjxy/doc_ydgtE07E6Rp4AMmKv/file_ItXaohEeulWngnN4y/boo_tZD1ykTPGDgzextMB/7ad80a03-ee84-3fbe-27f7-5c09dac0130eSnag_d7dbcdc.png)\n\n**Detta är vår konfigurationspanel. Det finns några olika ändringar som kan göras här.**\n\n1. Du kan koppla din feedback till e-post, Slack, Mattermost, Zendesk och Zapier.\n\n2. Du kan automatiskt spela in dina videor eller stänga av denna funktion.\n\n3. Detta är kodremsan så att du senare kan använda nummer 4 för att stilisera din feedback så att den matchar ditt företags varumärkesutseende. Att lägga till en kodremsa krävs bara när du publicerar Docsie via en kodremsa för din webbplats. Om du publicerar din Docsie-dokumentation via våra molnbaserade portaler kommer du automatiskt att ha Vocally sömlöst integrerat i alla dina publicerade kunskapsportaler.\n\n4. Detta är för att stilisera Vocally.\n\nNu när vi har tagit en djupdykning i vikten av varför feedback är nödvändig för förbättringar och optimeringar, för att bygga förtroende med dina kunder och hur Docsie ger en lösning på alla dessa frågor, är du välkommen att prova Vocally själv. Registrera dig på Docsie, skriv och publicera din första dokumentation, och se sedan hur du gillar det!\n\nFör mer information om Vocally – en Docsie-driven feedbackfunktion – besök: [https://help.docsie.io/?doc=/gather-feedback/](https://help.docsie.io/?doc=/gather-feedback/)",
    "7-golden-rules-to-successfully-approach-|category|0": "Produktdokumentation\n\u0005End File\u0006# docs/gemini/user_intent_detection.md\n# User Intent Detection with Gemini\n\nThis guide outlines strategies for effectively detecting user intent in prompt engineering with Gemini models.\n\n## Why Identify User Intent?\n\nUnderstanding a user's intent allows for:\n1. More precise and relevant responses\n2. Better handling of ambiguity and context\n3. Appropriate application of safety controls\n4. Improved user experience through appropriate response style\n\n## Intent Detection Techniques\n\n### Explicit Intent Recognition\n\nInclude recognition of explicit intent cues in your system instructions:\n\n```\nWhen users make requests that involve [specific action/task], \nidentify this intent and respond with [appropriate format/information].\n```\n\n**Example:**\n```\nWhen users ask for comparisons between items, identify this intent \nand structure your response as a clear table or list showing key differences and similarities.\n```\n\n### Use Explicit Intent Classification\n\nFor more complex systems, add explicit intent classification:\n\n```\nFor each user query, first classify their primary intent from these categories:\n- Information seeking: User wants factual information\n- Creative assistance: User wants creative content generation\n- Procedural help: User wants steps to complete a task\n- Decision support: User wants help making a choice\n- Troubleshooting: User needs help solving a problem\n\nThen tailor your response to match this intent.\n```\n\n### Identifying Ambiguous Intents\n\nInstruct the model to recognize when user intent is unclear:\n\n```\nIf the user's intent is ambiguous, identify this ambiguity and respond by:\n1. Noting the potential interpretations you've identified\n2. Providing information that addresses the most likely intent\n3. Asking a clarifying question to better understand their needs\n```\n\n## Example Applications\n\n### Customer Service Bot\n\n```\nAs a customer service assistant, identify these primary user intents:\n- Product information: Respond with detailed specifications and features\n- Troubleshooting: Provide step-by-step diagnostic procedures\n- Return/exchange: Explain the return policy and process\n- Complaint: Acknowledge the issue, express empathy, and offer solutions\n- Order status: Ask for order number if not provided, explain how to check status\n\nFor ambiguous queries, acknowledge the ambiguity and ask a specific clarifying question.\n```\n\n### Educational Assistant\n\n```\nAs an educational assistant, identify these user intents:\n- Factual question: Provide a clear, accurate answer with relevant context\n- Explanation request: Break down complex concepts with examples and analogies\n- Resource request: Suggest specific learning materials with brief descriptions\n- Problem-solving help: Guide through a solution process without providing complete answers\n- Definition request: Provide clear definition with examples of usage\n\nAdapt your language complexity to match the apparent knowledge level in the user's query.\n```\n\n## Best Practices\n\n1. **Start with core intents**: Begin by defining the most common intents for your application.\n2. **Use hierarchical intent structures**: Organize intents from general to specific.\n3. **Consider multi-intent queries**: Users often have multiple intents in a single query.\n4. **Iterate based on feedback**: Adjust your intent detection based on user interactions.\n5. **Include clarification strategies**: Define how to handle ambiguous intents.\n6. **Consider emotional context**: Sometimes detecting frustration or confusion is as important as the primary intent.\n\n## Implementing Intent Detection\n\nHere's a complete system instruction example:\n\n```\nYou are an AI assistant that helps users with financial planning questions.\n\nFor each user query, follow these steps:\n1. Identify the primary intent from these categories:\n   - Information seeking (about financial concepts)\n   - Account-specific questions\n   - Planning advice\n   - Tool/calculator requests\n   - Market analysis requests\n\n2. Also identify any secondary intents or emotional context.\n\n3. For information seeking, provide factual, concise explanations with examples.\n4. For account questions, explain you don't have access to personal account information.\n5. For planning advice, provide general guidelines with disclaimers about professional advice.\n6. For tool requests, describe relevant financial calculators or tools.\n7. For market analysis, provide general trends but avoid specific investment advice.\n\nIf intent is ambiguous, note the ambiguity and ask a clarifying question.\n\nAlways provide disclaimers about not being a financial advisor when appropriate.\n```\n\n## Conclusion\n\nEffective intent detection improves the precision and relevance of AI responses. By implementing explicit intent recognition, classification, and strategies for handling ambiguity, your Gemini implementation can better serve user needs across a variety of contexts.\n\u0005End File\u0006# Handling Harmful Requests with Gemini\n\nThis guide provides strategies for effectively handling potentially harmful requests when working with Gemini models.\n\n## Understanding Harmful Requests\n\nHarmful requests can include:\n\n- Requests for content that could physically, psychologically, or socially harm people\n- Instructions to bypass safety guardrails\n- Attempts to generate illegal, unethical, or deceptive content\n- Requests that could enable harm (e.g., disinformation, exploitation, harassment)\n\n## Core Response Strategies\n\n### 1. Clear Refusal with Education\n\nWhen refusing harmful requests, provide:\n- A clear statement of refusal\n- An explanation of why the request is problematic\n- Alternative, constructive suggestions when appropriate\n\n**Example System Instruction:**\n```\nWhen faced with requests to generate harmful content, clearly refuse and briefly explain why \nsuch content could be harmful. Where appropriate, offer constructive alternatives that address \nthe user's legitimate needs without causing harm.\n```\n\n### 2. Recognizing Intent vs. Content\n\nDistinguish between:\n- Requests with harmful intent but legitimate content needs\n- Requests with potentially harmful phrasing but legitimate intent\n- Genuinely harmful requests\n\n**Example System Instruction:**\n```\nDistinguish between harmful intent and potentially legitimate requests phrased problematically. \nFor questions about sensitive topics that appear to have educational or informational intent, \nprovide balanced, factual information while avoiding content that could enable harm.\n```\n\n### 3. Context-Aware Responses\n\nConsider the context of the request to determine appropriate responses:\n\n**Example System Instruction:**\n```\nConsider the full context of user requests before determining if they're harmful. \nFor academic, educational, literary, or creative contexts, provide appropriate information \nwhile still refusing to generate content that could directly enable harm.\n```\n\n## Implementation Examples\n\n### Academic/Educational Context\n\n```\nWhen users ask about sensitive or potentially harmful topics in an educational context:\n\n1. Acknowledge the academic nature of the inquiry\n2. Provide factual, educational information on the topic\n3. Focus on understanding, prevention, and historical/social context\n4. Avoid detailed instructions that could enable harmful actions\n5. Include relevant ethical considerations when appropriate\n\nFor example, you can discuss the science of explosives in general terms for educational purposes, \nbut should not provide specific instructions for creating dangerous devices.\n```\n\n### Creative Writing Context\n\n```\nFor creative writing requests that involve sensitive content:\n\n1. Recognize the fictional nature of the request\n2. For moderate content (e.g., fictional conflicts, general themes of conflict), provide appropriate creative assistance\n3. For more problematic content, suggest alternative approaches that achieve similar narrative goals without glorifying harm\n4. Decline to generate graphic depictions of violence, abuse, or content that demeans specific groups\n5. When refusing, briefly explain your reasoning and offer constructive alternatives\n\nFor example, you can help craft a crime thriller narrative without providing graphic depictions of violence or realistic criminal methodologies.\n```\n\n### Handling Edge Cases\n\n```\nFor ambiguous requests that might have harmful interpretations:\n\n1. First, assume a benign interpretation and respond accordingly\n2. If the request remains ambiguous, ask clarifying questions to understand the user's intent\n3. If harmful intent becomes clear after clarification, politely refuse\n4. If the user insists on harmful content after refusal, restate your refusal and explain alternatives\n\nFor example, if someone asks about \"breaking into systems,\" first assume they're asking about cybersecurity education, \nand respond with general information about the importance of security. If they clarify they want actual hacking instructions, \nexplain why you can't provide that specific assistance.\n```\n\n## Specific Content Categories\n\n### Medical Information\n\n```\nFor medical requests:\n1. Provide general, educational health information\n2. Always include disclaimers about consulting healthcare professionals\n3. Decline requests for specific medical diagnoses or treatment plans\n4. Refuse to generate content that could enable self-harm or harm to others\n5. For mental health topics, provide general information and resources for seeking help\n```\n\n### Legal Information\n\n```\nFor legal requests:\n1. Provide general educational information about legal concepts\n2. Include disclaimers about not providing legal advice\n3. Decline to generate content that could help with illegal activities\n4. For questions about laws, focus on factual information rather than application to specific cases\n5. Suggest consulting with legal professionals for specific situations\n```\n\n## Best Practices for System Instructions\n\n1. **Be specific about boundaries**: Clearly define what constitutes harmful content in your context\n2. **Provide alternative response paths**: Give the model constructive ways to respond to borderline requests\n3. **Include intent recognition guidance**: Help the model distinguish between educational, informational, or malicious intent\n4. **Define escalation thresholds**: Specify when to refuse outright versus when to provide limited information\n5. **Balance safety with helpfulness**: Aim to meet legitimate user needs while avoiding potential harm\n\n## Comprehensive Example\n\nHere's a complete example of system instructions for handling harmful requests:\n\n```\nYou are an AI assistant that provides helpful, accurate, and safe information. Follow these guidelines for handling potentially harmful requests:\n\nFor clearly harmful requests (instructions for illegal activities, content that demeans groups, etc.):\n- Politely refuse and briefly explain why you can't fulfill the request\n- Where appropriate, suggest constructive alternatives that address legitimate aspects of the query\n\nFor requests about sensitive topics with educational value:\n- Provide general educational information that promotes understanding\n- Focus on context, principles, and safety rather than specific harmful implementations\n- Include relevant ethical considerations and societal impact\n- Add appropriate disclaimers for medical, legal, or other specialized information\n\nFor ambiguous requests:\n- First interpret in the most benign way and respond accordingly\n- If ambiguity persists, ask clarifying questions about the user's intent\n- If clarification reveals harmful intent, politely decline\n\nFor creative or fictional scenarios:\n- Support creative writing and fictional scenarios within reasonable bounds\n- Decline to generate content that glorifies or provides detailed depictions of violence, abuse, or illegal activities\n- Suggest alternative approaches that achieve similar creative goals without potential harm\n\nAlways prioritize human wellbeing and safety while attempting to be as helpful as possible within these constraints.\n```\n\n## Conclusion\n\nEffective handling of potentially harmful requests requires balancing safety with helpfulness. By implementing clear guidelines that consider context, intent, and appropriate limitations, your Gemini implementation can respond constructively to users while maintaining appropriate safety boundaries.\n\u0005End File\u0006# docs/gemini/providing_factual_responses.md\n# Providing Factual Responses with Gemini\n\nThis guide outlines strategies for creating factually accurate responses with Gemini models, with emphasis on addressing uncertainty and handling limitations.\n\n## Core Principles for Factual Responses\n\n### 1. Clear Signaling of Confidence Levels\n\nInstruct the model to explicitly indicate its level of confidence in the information provided:\n\n```\nWhen providing factual information, clearly indicate your level of confidence:\n- For well-established facts, present information directly\n- For information with some uncertainty, use qualifying language like \"typically,\" \"generally,\" or \"according to [source]\"\n- For areas of significant uncertainty, explicitly acknowledge limitations and present multiple perspectives\n```\n\n### 2. Explicit Uncertainty Handling\n\nDefine how the model should handle uncertainty:\n\n```\nWhen faced with uncertainty about facts:\n1. Acknowledge the uncertainty directly\n2. Explain the nature of the uncertainty (conflicting sources, evolving knowledge, limited available information)\n3. Provide the most reliable information available while noting its limitations\n4. When appropriate, present multiple perspectives or explanations\n```\n\n### 3. Knowledge Boundary Awareness\n\nEnsure the model clearly recognizes and communicates its knowledge boundaries:\n\n```\nAcknowledge the boundaries of your knowledge:\n- For questions about very recent events (after your training cutoff), explain that you don't have information beyond [cutoff date]\n- For highly specialized or obscure topics, indicate if you have limited information\n- Never fabricate information to appear knowledgeable - explicitly state when you don't know something\n```\n\n## Implementation Strategies\n\n### Factual Domain-Specific Guidance\n\nFor domains where factual accuracy is particularly important, provide specific guidance:\n\n**Scientific Information:**\n```\nWhen providing scientific information:\n1. Distinguish between well-established scientific consensus and emerging research\n2. For well-established principles, provide clear explanations\n3. For evolving areas, note the preliminary nature of findings and avoid presenting them as settled\n4. For controversial topics, present the mainstream scientific view while acknowledging significant alternative perspectives\n5. Use precise terminology appropriate to the user's apparent knowledge level\n```\n\n**Historical Information:**\n```\nWhen discussing historical events:\n1. Focus on widely accepted historical facts\n2. Acknowledge when historical accounts may vary or be contested\n3. Present major interpretations of contested historical events\n4. Avoid making definitive claims about motivations or details that are not well-documented\n5. Consider cultural and geographical context in your explanations\n```\n\n### Response Formatting for Clarity\n\nStructure responses to clearly delineate different types of information:\n\n```\nStructure factual responses with:\n1. Direct answer to the query with appropriate confidence indicators\n2. Supporting context or explanation\n3. Any necessary caveats or limitations\n4. Alternative viewpoints when relevant\n5. Sources or types of evidence when helpful (without fabricating specific citations)\n```\n\n### Source Handling\n\nProvide guidance on how to reference sources without fabricating specific citations:\n\n```\nWhen discussing sources of information:\n- Refer to general categories of sources (e.g., \"peer-reviewed research,\" \"historical documents,\" \"expert consensus\")\n- Avoid fabricating specific citations, publication dates, or article titles\n- If specific named sources are necessary, only mention widely recognized sources you're confident about\n- When appropriate, note that additional research would be needed for more detailed source information\n```\n\n## Example System Instructions\n\n### General Knowledge Assistant\n\n```\nYou are a helpful AI assistant that provides factual information with appropriate nuance and accuracy.\n\nFollow these principles when responding to factual queries:\n\n1. Directly answer questions when you have high confidence in the information\n2. Use appropriate qualifying language for information with moderate certainty\n3. Explicitly acknowledge uncertainty when present\n4. Clearly state when you don't have information (especially for recent events past your training cutoff)\n5. Present multiple perspectives for contested topics\n6. Structure responses to distinguish between:\n   - Core answer\n   - Supporting explanation\n   - Limitations or caveats\n   - Alternative viewpoints (when relevant)\n\nNever fabricate information, specific sources, or precise statistics to appear knowledgeable. \nIf you don't know something, simply acknowledge the limitation.\n```\n\n### Educational Assistant\n\n```\nYou are an educational assistant designed to provide accurate, nuanced information appropriate for learning contexts.\n\nWhen responding to factual questions:\n\n1. Tailor information depth to the apparent knowledge level of the question\n2. Explain complex concepts using clear, accessible language\n3. Distinguish clearly between:\n   - Foundational, well-established knowledge\n   - Current scientific understanding that may evolve\n   - Areas of active research or debate\n4. Acknowledge knowledge limitations explicitly\n5. When discussing contested topics, present major viewpoints fairly\n6. Include relevant context that helps build a complete understanding\n7. For mathematical or scientific content, ensure precise technical accuracy\n\nPrioritize educational value by explaining underlying principles rather than just stating facts.\n```\n\n## Handling Specific Factual Challenges\n\n### Recent or Evolving Information\n\n```\nFor topics that may have changed since your training data cutoff:\n1. Provide the most up-to-date information you have\n2. Explicitly note your training cutoff date\n3. Explain what aspects might have changed since then\n4. When appropriate, suggest what kind of updated information the user might want to research\n```\n\n### Contested or Politically Sensitive Topics\n\n```\nFor contested or politically sensitive topics:\n1. Acknowledge the contested nature of the topic\n2. Present mainstream views on different sides of the issue\n3. Focus on verifiable facts rather than interpretations when possible\n4. Avoid language that suggests your personal endorsement of any particular view\n5. Maintain a balanced, educational tone throughout\n```\n\n### Specialized Technical Information\n\n```\nFor specialized technical information:\n1. Provide accurate information within your knowledge boundaries\n2. Be explicit about the limits of your technical knowledge in the specific domain\n3. Use precise technical terminology appropriate to the domain\n4. Acknowledge when a question requires expertise beyond your capabilities\n5. When appropriate, explain the general principles while noting that specific implementation details may require specialized expertise\n```\n\n## Best Practices for System Instructions\n\n1. **Define confidence levels**: Clearly explain how the model should express different levels of certainty\n2. **Establish knowledge boundaries**: Explicitly instruct on handling knowledge cutoff limitations\n3. **Set clear guidelines for unknowns**: Provide specific language for acknowledging uncertainty\n4. **Define domain-specific approaches**: Create specialized guidance for different knowledge domains\n5. **Include perspective-handling**: Specify how to present contested information or multiple viewpoints\n6. **Provide structural guidance**: Suggest how to organize information for maximum clarity\n\n## Conclusion\n\nCreating factually accurate responses with Gemini requires explicit guidance on handling uncertainty, acknowledging limitations, and structuring information clearly. By implementing these strategies, you can help ensure that responses are both helpful and appropriately nuanced in their presentation of factual information.\n\u0005End File\u0006# google/gemini-api-cookbook\n# docs/gemini/system_instructions_for_chat.md\n# System Instructions for Chat with Gemini\n\nThis guide outlines strategies for creating effective system instructions to customize Gemini's chat behavior.\n\n## What are System Instructions?\n\nSystem instructions provide configuration and guidance to the model about how to respond to user queries in a chat session. They allow you to:\n\n1. Define a specific role, personality, or character for the model\n2. Set constraints on how the model should respond\n3. Establish specific formats for answers\n4. Define rules for handling certain types of questions\n5. Provide specialized knowledge for specific contexts\n\n## Core Elements of Effective System Instructions\n\n### 1. Clear Role Definition\n\nStart with a concise description of the model's role or persona:\n\n```\nYou are a helpful, concise technical support specialist focusing on Python programming problems.\n```\n\nThe role definition should include:\n- The primary function (what the assistant helps with)\n- Key behavioral characteristics (concise, detailed, friendly, etc.)\n- Subject matter focus if applicable\n\n### 2. Response Style Guidelines\n\nDefine how responses should be structured and presented:\n\n```\nFormat your responses using these guidelines:\n- Use clear, direct language with minimal technical jargon\n- Structure complex answers with numbered steps or bullet points\n- Include brief code examples when relevant\n- End each response with a simple question to confirm if the issue is resolved\n```\n\n### 3. Boundaries and Constraints\n\nClearly define what the model should and shouldn't do:\n\n```\nImportant limitations:\n- Don't provide specific advice about company policies\n- If asked about topics outside of Python programming, politely redirect to Python topics\n- Don't make assumptions about the user's operating system; ask for clarification if needed\n- Never provide solutions that could compromise security or enable harmful actions\n```\n\n### 4. Specialized Knowledge and Resources\n\nInclude any specific knowledge base or resources the model should reference:\n\n```\nWhen answering Python questions:\n- Focus on Python 3.x solutions unless specifically asked about other versions\n- Recommend official documentation at docs.python.org when appropriate\n- For package questions, reference PyPI and the package's official documentation\n- Suggest common debugging tools like pdb and logging modules for troubleshooting issues\n```\n\n## Example System Instructions\n\n### Technical Support Specialist\n\n```\nYou are a helpful technical support specialist focusing on Mac OS issues.\n\nFormat your responses with:\n1. A direct answer to the user's question\n2. Step-by-step instructions when providing solutions\n3. Explanations of why the issue might be occurring\n4. Preventative advice to avoid the problem in the future\n\nImportant guidelines:\n- Use clear, non-technical language unless the user demonstrates technical knowledge\n- Ask clarifying questions if the user's issue is vague\n- Recommend official Apple support resources for complex hardware issues\n- If a solution requires Terminal commands, provide clear warnings about potential risks\n- For issues you can't resolve, suggest appropriate professional support options\n\nFocus on recent Mac OS versions (10.14 Mojave through current) and common applications.\n```\n\n### Math Tutor\n\n```\nYou are a helpful mathematics tutor for high school and early college students.\n\nYour approach to tutoring:\n- Guide students through problems rather than simply providing answers\n- Ask Socratic questions to help students discover solutions\n- Provide step-by-step explanations that highlight key concepts\n- Offer multiple approaches to problems when appropriate\n- Use clear, concise mathematical notation\n\nGuidelines:\n- Adjust your explanation level based on the apparent knowledge of the student\n- When a student is stuck, provide increasingly specific hints rather than immediate solutions\n- Connect concepts to real-world applications when possible\n- If a student's approach is incorrect, acknowledge any correct elements before redirecting\n- For complex topics, break explanations into manageable chunks\n\nCover topics including: algebra, geometry, trigonometry, calculus, statistics, and probability.\n```\n\n### Recipe Assistant\n\n```\nYou are a helpful culinary assistant that provides recipes and cooking advice.\n\nWhen providing recipes:\n- Start with a brief introduction to the dish\n- List all ingredients with precise measurements\n- Provide clear, numbered cooking steps\n- Include cooking times and temperatures\n- Add notes about potential substitutions for common allergens\n\nResponse guidelines:\n- Adapt recipes to dietary restrictions when requested (vegan, gluten-free, etc.)\n- Suggest ingredient substitutions when common items might not be available\n- Include tips for food storage and reheating when appropriate\n- For cooking technique questions, explain the why behind the method\n- Respond to cultural questions about dishes with respectful, informative content\n\nLimitations:\n- Clearly note when substitutions might significantly change a dish's outcome\n- Avoid claiming health benefits for recipes unless supported by general consensus\n- If asked about specialized dietary needs (medical conditions), remind users to consult healthcare providers\n```\n\n## Advanced Techniques\n\n### Context-Dependent Responses\n\nInstruct the model to adjust responses based on context clues:\n\n```\nAdjust your responses based on context:\n- If the user appears to be a beginner, provide more detailed explanations with basic terminology\n- If the user demonstrates expertise, use more technical language and advanced concepts\n- If the user seems frustrated, acknowledge their frustration and focus on direct solutions\n- If the question is ambiguous, ask for clarification before providing a complete answer\n```\n\n### Multi-Step Response Frameworks\n\nFor complex topics, define multi-step response frameworks:\n\n```\nFor complex troubleshooting requests, use this framework:\n1. Problem acknowledgment: Restate the issue to confirm understanding\n2. Information gathering: Ask essential diagnostic questions\n3. Initial assessment: Provide most likely cause based on available information\n4. Step-by-step solution: Offer clear, sequential troubleshooting steps\n5. Verification: Suggest how to confirm the issue is resolved\n6. Prevention: Provide tips to prevent recurrence\n```\n\n### Combining Multiple Personas\n\nFor versatile assistants, define how to switch between different expertise areas:\n\n```\nYou can provide expertise in multiple domains. Identify the primary domain of the user's question and respond as:\n\nFor programming questions: A patient coding mentor who emphasizes best practices and clear explanations\n\nFor math questions: A step-by-step math tutor who uses Socratic questioning\n\nFor writing questions: A helpful editor focusing on clarity, structure, and effective communication\n\nAlways maintain a consistent, helpful tone across all domains while adapting the specific approach to the subject matter.\n```\n\n## Best Practices\n\n1. **Be concise**: Focus on the most important guidelines rather than exhaustive instructions\n2. **Prioritize instructions**: Put the most important guidance first\n3. **Use clear structure**: Organize instructions into logical sections with headers\n4. **Test and refine**: Evaluate responses and adjust instructions based on performance\n5. **Balance constraints**: Include enough guidance for consistency without overly restricting the model\n6. **Consider user needs**: Design instructions based on the specific needs of your target users\n7. **Include examples**: For complex formats, include example responses\n\n## Implementation Tips\n\n- Start with a concise core role and iteratively add constraints and guidelines\n- For specialized applications, define technical terminology or jargon that should be used\n- Include guidance on how to handle out-of-scope or inappropriate questions\n- For multi-turn interactions, provide instructions on maintaining context across exchanges\n- Test system instructions with a variety of user inputs to ensure consistent behavior\n\n## Conclusion\n\nEffective system instructions create a foundation for consistent, helpful interactions with Gemini models. By clearly defining the role, response style, boundaries, and specialized knowledge areas, you can customize the model's behavior to meet specific use cases while maintaining appropriate guardrails.\n\u0005End File\u0006# Copyright 2023 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"A wrapper class implementing Anthropic API for testing and benchmarking.\n\nThis allows us to use the same notebook for both Anthropic and Google (Gemini) APIs.\n\"\"\"\n\nimport os\nimport time\nfrom typing import Dict, Iterator, List, Optional\nimport anthropic\n\n\ndef get_model(model=\"claude-3-opus-20240229\"):\n  \"\"\"Initialize anthropic client.\"\"\"\n  if not os.environ.get(\"ANTHROPIC_API_KEY\"):\n    raise ValueError(\"ANTHROPIC_API_KEY environment variable is not set.\")\n  anthropic_client = anthropic.Anthropic()\n  return Model(anthropic_client, model=model)\n\n\nclass Content:\n  \"\"\"Class for Anthropic API content.\"\"\"\n\n  def __init__(self, data: Dict):\n    self.parts = [Part(part) for part in data[\"parts\"]] if \"parts\" in data else []\n    self.role = data.get(\"role\")\n    self.text = data.get(\"text\")\n    self.type = data.get(\"type\")\n\n  def to_dict(self):\n    \"\"\"Convert to dict.\"\"\"\n    if self.type and self.text:\n      return {\"type\": self.type, \"text\": self.text}\n    else:\n      return {\"role\": self.role, \"content\": [p.to_dict() for p in self.parts]}\n\n\nclass Part:\n  \"\"\"Class for Anthropic API part.\"\"\"\n\n  def __init__(self, data: Dict):\n    # Extract MIME type and value for image parts (Claude API format)\n    if isinstance(data, dict) and \"source\" in data and data.get(\n        \"type\") == \"image\":\n      self.mime_type = data[\"source\"].get(\"media_type\", \"\")\n      self.file_data = data[\"source\"].get(\"data\", \"\")\n      self.type = \"image\"\n    # Handle text parts\n    elif isinstance(data, dict) and \"text\" in data:\n      self.text = data[\"text\"]\n      self.type = \"text\"\n    # Handle string inputs (convert to text part)\n    elif isinstance(data, str):\n      self.text = data\n      self.type = \"text\"\n    else:\n      # Store original data for any other format\n      self.data = data\n      self.type = \"unknown\"\n\n  def to_dict(self):\n    \"\"\"Convert to dict for Claude API format.\"\"\"\n    if self.type == \"text\":\n      return {\"type\": \"text\", \"text\": self.text}\n    elif self.type == \"image\":\n      return {\n          \"type\": \"image\",\n          \"source\": {\n              \"type\": \"base64\",\n              \"media_type\": self.mime_type,\n              \"data\": self.file_data\n          }\n      }\n    else:\n      return self.data\n\n\nclass Candidate:\n  \"\"\"Class for Anthropic API candidate.\"\"\"\n\n  def __init__(self, data: Dict):\n    self.content = Content(data[\"content\"]) if \"content\" in data else None\n    self.finish_reason = data.get(\"stop_reason\", \"\")\n    self.index = 0\n    self.safety_ratings = []  # Placeholder for safety ratings\n\n\nclass CountTokensResponse:\n  \"\"\"Class for Anthropic API token counting response.\"\"\"\n\n  def __init__(self, data: Dict):\n    self.total_tokens = data.get(\"input_tokens\", 0)\n    self.token_count_result = data\n\n\nclass ContentResponse:\n  \"\"\"Class for Anthropic API content response.\"\"\"\n\n  def __init__(self, data: Dict, stream: bool = False):\n    self.candidates = []\n    self.usage_metadata = None\n    self.stream = stream\n    self._stream_handler = None\n    self.result = data\n\n    # Process non-streaming response\n    if not stream:\n      candidate_data = {\n          \"content\": {\"text\": data.get(\"content\", \"\"), \"role\": \"assistant\"},\n          \"stop_reason\": data.get(\"stop_reason\", \"\")\n      }\n      self.candidates = [Candidate(candidate_data)]\n\n  def __iter__(self) -> Iterator[\"ContentResponse\"]:\n    \"\"\"Iterate through streaming responses.\n\n    Claude's streaming API is different from Gemini's, so we need to adapt.\n    \"\"\"\n    if not self.stream:\n      yield self\n      return\n\n    # In a real implementation, this would actually process the streaming response\n    # For this wrapper, we'll simplify and just yield the final response\n    yield self\n\n\nclass Model:\n  \"\"\"Wrapper for Anthropic Claude models to match Google Gemini API interface.\"\"\"\n\n  def __init__(self, client, model: str = \"claude-3-opus-20240229\"):\n    self.client = client\n    self.model_name = model\n    self.temperature = 0.7\n    self.max_output_tokens = 1024\n    self.top_p = 0.8\n    self.top_k = 40\n\n  def generate_content(\n      self,\n      contents,\n      generation_config: Optional[Dict] = None,\n      safety_settings: Optional[Dict] = None,\n      stream: bool = False,\n  ) -> ContentResponse:\n    \"\"\"Generate content with the model.\n\n    Args:\n      contents: Input contents (can be string, Content, or list)\n      generation_config: Generation parameters\n      safety_settings: Safety settings (not fully implemented)\n      stream: Whether to stream the response\n\n    Returns:\n      ContentResponse object\n    \"\"\"\n    # Process generation config\n    if generation_config:\n      temperature = generation_config.get(\"temperature\", self.temperature)\n      max_tokens = generation_config.get(\"max_output_tokens\",\n                                         self.max_output_tokens)\n      top_p = generation_config.get(\"top_p\", self.top_p)\n    else:\n      temperature = self.temperature\n      max_tokens = self.max_output_tokens\n      top_p = self.top_p\n\n    # Process contents into messages format for Claude\n    if isinstance(contents, str):\n      messages = [{\"role\": \"user\", \"content\": contents}]\n    elif isinstance(contents, Content):\n      messages = [contents.to_dict()]\n    elif isinstance(contents, list):\n      messages = []\n      for item in contents:\n        if isinstance(item, str):\n          messages.append({\"role\": \"user\", \"content\": item})\n        elif isinstance(item, Content):\n          messages.append(item.to_dict())\n        elif isinstance(item, dict):\n          messages.append(item)\n\n    # Extract system prompt if present\n    system = None\n    if isinstance(contents, list):\n      for i, content in enumerate(contents):\n        if isinstance(content, dict) and content.get(\"role\") == \"system\":\n          system = content.get(\"parts\", [{}])[0].get(\"text\", \"\")\n          contents.pop(i)\n          break\n\n    # Format messages for Claude API\n    formatted_messages = []\n    current_role = None\n    current_content = []\n\n    for msg in messages:\n      role = msg.get(\"role\", \"user\")\n      \n      # Handle different message formats\n      if \"content\" in msg:\n        content_data = msg[\"content\"]\n        # If content is a list of parts\n        if isinstance(content_data, list):\n          for part in content_data:\n            if isinstance(part, dict) and \"text\" in part:\n              current_content.append({\"type\": \"text\", \"text\": part[\"text\"]})\n            elif isinstance(part, dict) and part.get(\"type\") == \"image\":\n              # Convert image part format\n              if \"source\" in part:\n                current_content.append({\n                    \"type\": \"image\",\n                    \"source\": part[\"source\"]\n                })\n        # If content is a string\n        elif isinstance(content_data, str):\n          current_content.append({\"type\": \"text\", \"text\": content_data})\n      \n      # For simpler text messages\n      elif \"text\" in msg:\n        current_content.append({\"type\": \"text\", \"text\": msg[\"text\"]})\n      \n      if role != current_role:\n        if current_role and current_content:\n          formatted_messages.append({\n              \"role\": current_role,\n              \"content\": current_content\n          })\n          current_content = []\n        current_role = role\n\n    # Add the last message\n    if current_role and current_content:\n      formatted_messages.append({\n          \"role\": current_role,\n          \"content\": current_content\n      })\n\n    # Extract the actual messages for Claude format\n    messages = []\n    for i, msg in enumerate(formatted_messages):\n      if msg[\"role\"] in [\"user\", \"assistant\"]:\n        messages.append(msg)\n\n    # Make API call\n    try:\n      response = self.client.messages.create(\n          model=self.model_name,\n          max_tokens=max_tokens,\n          temperature=temperature,\n          top_p=top_p,\n          system=system,\n          messages=messages,\n          stream=stream,\n      )\n      \n      return ContentResponse(response, stream=stream)\n    \n    except Exception as e:\n      print(f\"Error calling Anthropic API: {e}\")\n      # Return a minimal error response\n      error_response = {\n          \"content\": f\"Error: {str(e)}\",\n          \"stop_reason\": \"error\"\n      }\n      return ContentResponse(error_response, stream=False)\n\n  def count_tokens(self, contents) -> CountTokensResponse:\n    \"\"\"Count tokens in the input.\n\n    Args:\n      contents: Input contents (can be string, Content, or list)\n\n    Returns:\n      CountTokensResponse object\n    \"\"\"\n    # Process contents\n    if isinstance(contents, str):\n      prompt = contents\n    elif isinstance(contents, Content):\n      # Simplified conversion\n      prompt = \" \".join([p.text for p in contents.parts if hasattr(p, \"text\")])\n    elif isinstance(contents, list):\n      prompt_parts = []\n      for item in contents:\n        if isinstance(item, str):\n          prompt_parts.append(item)\n        elif isinstance(item, Content):\n          for p in item.parts:\n            if hasattr(p, \"text\"):\n              prompt_parts.append(p.text)\n      prompt = \" \".join(prompt_parts)\n    else:\n      prompt = str(contents)\n\n    # Call Anthropic token counting endpoint\n    try:\n      count = self.client.count_tokens(prompt)\n      return CountTokensResponse({\"input_tokens\": count.tokens})\n    except:\n      # Fallback approximation\n      token_count = len(prompt.split()) // 0.75  # Rough approximation\n      return CountTokensResponse({\"input_tokens\": int(token_count)})\n\u0005End File\u0006# Copyright 2023 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"A wrapper class implementing OpenAI API for testing and benchmarking.\n\nThis allows us to use the same notebook for both OpenAI and Google (Gemini) APIs.\n\"\"\"\n\nimport base64\nimport os\nfrom typing import Dict, Iterator, List, Optional, Union\nimport time\n\nimport openai\n\n\ndef get_model(model_name: str = \"gpt-4-turbo\"):\n  \"\"\"Initialize OpenAI API client.\"\"\"\n  # Check for API key\n  if not os.environ.get(\"OPENAI_API_KEY\"):\n    raise ValueError(\"OPENAI_API_KEY environment variable is not set.\")\n\n  client = openai.OpenAI()\n  return Model(client, model_name)\n\n\nclass Content:\n  \"\"\"Content class for compatibility with Gemini API.\"\"\"\n\n  def __init__(self, data: Dict):\n    self.parts = [Part(part) for part in data.get(\"parts\", [])]\n    self.role = data.get(\"role\", \"user\")\n\n  def to_dict(self) -> Dict:\n    \"\"\"Convert to dict for OpenAI API.\"\"\"\n    # Special handling for system role\n    if self.role == \"system\" and len(self.parts) == 1 and hasattr(\n        self.parts[0], \"text\"):\n      return {\"role\": \"system\", \"content\": self.parts[0].text}\n\n    # For messages with multiple parts or non-text parts\n    contents = []\n    for part in self.parts:\n      if hasattr(part, \"text\"):\n        contents.append({\"type\": \"text\", \"text\": part.text})\n      elif hasattr(part, \"file_data\") and hasattr(part, \"mime_type\"):\n        contents.append({\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": f\"data:{part.mime_type};base64,{part.file_data}\"\n            }\n        })\n    \n    return {\"role\": self.role, \"content\": contents}\n\n\nclass Part:\n  \"\"\"Part class for compatibility with Gemini API.\"\"\"\n\n  def __init__(self, data: Union[Dict, str]):\n    if isinstance(data, str):\n      self.text = data\n      self.type = \"text\"\n    elif isinstance(data, dict):\n      if \"text\" in data:\n        self.text = data[\"text\"]\n        self.type = \"text\"\n      elif \"file_data\" in data or \"inline_data\" in data:\n        self.file_data = data.get(\"file_data\", \"\") or data.get(\"inline_data\", {}).get(\"data\", \"\")\n        self.mime_type = data.get(\"mime_type\", \"\") or data.get(\"inline_data\", {}).get(\"mime_type\", \"\")\n        self.type = \"image\"\n    else:\n      self.data = data\n      self.type = \"unknown\"\n\n\nclass Candidate:\n  \"\"\"Candidate class for compatibility with Gemini API.\"\"\"\n\n  def __init__(self, data: Dict):\n    self.content = Content({\"parts\": [{\"text\": data.get(\"text\", \"\")}], \"role\": \"assistant\"})\n    self.finish_reason = data.get(\"finish_reason\", \"\")\n    self.safety_ratings = []\n    self.index = 0\n\n\nclass CountTokensResponse:\n  \"\"\"Token count response for compatibility with Gemini API.\"\"\"\n\n  def __init__(self, data: Dict):\n    self.total_tokens = data.get(\"total_tokens\", 0)\n    self.prompt_tokens = data.get(\"prompt_tokens\", 0)\n    self.completion_tokens = data.get(\"completion_tokens\", 0)\n\n\nclass ContentResponse:\n  \"\"\"Response class for compatibility with Gemini API.\"\"\"\n\n  def __init__(self, data: Dict, stream: bool = False):\n    self.candidates = []\n    self._stream = stream\n    self.usage_metadata = None\n    self.result = data\n\n    if not stream:\n      # Non-streaming response\n      if \"choices\" in data:\n        for choice in data[\"choices\"]:\n          candidate_data = {\n              \"text\": choice[\"message\"][\"content\"],\n              \"finish_reason\": choice.get(\"finish_reason\", \"\")\n          }\n          self.candidates.append(Candidate(candidate_data))\n      \n      # Extract usage metadata if available\n      if \"usage\" in data:\n        self.usage_metadata = data[\"usage\"]\n    else:\n      # For streaming, we'll set up the initial state\n      self._stream_iter = data\n      self.accumulated_text = \"\"\n\n  def __iter__(self) -> Iterator[\"ContentResponse\"]:\n    \"\"\"Iterate through streaming responses.\"\"\"\n    if not self._stream:\n      yield self\n      return\n\n    chunk_response = ContentResponse({}, stream=False)\n    for chunk in self._stream_iter:\n      if hasattr(chunk.choices[0], \"delta\") and hasattr(chunk.choices[0].delta, \"content\"):\n        content = chunk.choices[0].delta.content\n        if content:\n          self.accumulated_text += content\n          \n          # Create a response for this chunk\n          chunk_data = {\n              \"choices\": [{\n                  \"message\": {\"content\": self.accumulated_text},\n                  \"finish_reason\": chunk.choices[0].finish_reason\n              }]\n          }\n          chunk_response = ContentResponse(chunk_data, stream=False)\n          \n          yield chunk_response\n\n\nclass Model:\n  \"\"\"Wrapper for OpenAI models to match Google Gemini API interface.\"\"\"\n\n  def __init__(self, client, model_name: str):\n    self.client = client\n    self.model_name = model_name\n    self.temperature = 0.7\n    self.max_tokens = 1024\n    self.top_p = 1.0\n\n  def generate_content(\n      self,\n      contents: Union[str, Content, List],\n      generation_config: Optional[Dict] = None,\n      safety_settings: Optional[Dict] = None,\n      stream: bool = False,\n  ) -> ContentResponse:\n    \"\"\"Generate content with OpenAI model.\n    \n    Args:\n      contents: Input contents (string, Content object, or list)\n      generation_config: Generation parameters\n      safety_settings: Safety settings (not fully implemented)\n      stream: Whether to stream the response\n      \n    Returns:\n      ContentResponse object\n    \"\"\"\n    # Process generation config\n    if generation_config:\n      temperature = generation_config.get(\"temperature\", self.temperature)\n      max_tokens = generation_config.get(\"max_output_tokens\", self.max_tokens)\n      top_p = generation_config.get(\"top_p\", self.top_p)\n    else:\n      temperature = self.temperature\n      max_tokens = self.max_tokens\n      top_p = self.top_p\n\n    # Process contents into OpenAI message format\n    if isinstance(contents, str):\n      messages = [{\"role\": \"user\", \"content\": contents}]\n    elif isinstance(contents, Content):\n      messages = [contents.to_dict()]\n    elif isinstance(contents, list):\n      messages = []\n      for item in contents:\n        if isinstance(item, str):\n          messages.append({\"role\": \"user\", \"content\": item})\n        elif isinstance(item, Content):\n          messages.append(item.to_dict())\n        elif isinstance(item, dict) and \"role\" in item:\n          # Handle direct message dict\n          if \"parts\" in item and isinstance(item[\"parts\"], list):\n            # Convert parts to OpenAI format\n            content_list = []\n            for part in item[\"parts\"]:\n              if isinstance(part, dict) and \"text\" in part:\n                content_list.append({\"type\": \"text\", \"text\": part[\"text\"]})\n              elif isinstance(part, dict) and \"inline_data\" in part:\n                mime_type = part[\"inline_data\"][\"mime_type\"]\n                data = part[\"inline_data\"][\"data\"]\n                content_list.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{data}\"\n                    }\n                })\n            messages.append({\"role\": item[\"role\"], \"content\": content_list})\n          elif \"content\" in item:\n            messages.append(item)\n          else:\n            # Default handling\n            messages.append(item)\n\n    # Make the API call\n    try:\n      response = self.client.chat.completions.create(\n          model=self.model_name,\n          messages=messages,\n          temperature=temperature,\n          max_tokens=max_tokens,\n          top_p=top_p,\n          stream=stream,\n      )\n      \n      return ContentResponse(response, stream=stream)\n    \n    except Exception as e:\n      print(f\"Error calling OpenAI API: {e}\")\n      # Return error response\n      error_response = {\n          \"choices\": [{\n              \"message\": {\"content\": f\"Error: {str(e)}\"},\n              \"finish_reason\": \"error\"\n          }]\n      }\n      return ContentResponse(error_response)\n\n  def count_tokens(self, contents) -> CountTokensResponse:\n    \"\"\"Count tokens in the input.\n    \n    Args:\n      contents: Input contents\n      \n    Returns:\n      CountTokensResponse object\n    \"\"\"\n    # Process contents\n    if isinstance(contents, str):\n      messages = [{\"role\": \"user\", \"content\": contents}]\n    elif isinstance(contents, Content):\n      messages = [contents.to_dict()]\n    elif isinstance(contents, list):\n      messages = []\n      for item in contents:\n        if isinstance(item, str):\n          messages.append({\"role\": \"user\", \"content\": item})\n        elif isinstance(item, Content):\n          messages.append(item.to_dict())\n        elif isinstance(item, dict):\n          messages.append(item)\n\n    # Use tiktoken for token counting\n    try:\n      import tiktoken\n      encoding = tiktoken.encoding_for_model(self.model_name)\n      \n      # Simple approximation - just count tokens in all text\n      token_count = 0\n      for message in messages:\n        if isinstance(message.get(\"content\"), str):\n          token_count += len(encoding.encode(message[\"content\"]))\n        elif isinstance(message.get(\"content\"), list):\n          for content_item in message[\"content\"]:\n            if content_item.get(\"type\") == \"text\":\n              token_count += len(encoding.encode(content_item.get(\"text\", \"\")))\n      \n      return CountTokensResponse({\n          \"total_tokens\": token_count,\n          \"prompt_tokens\": token_count,\n          \"completion_tokens\": 0\n      })\n    \n    except:\n      # Fallback to simple approximation\n      total_chars = 0\n      for message in messages:\n        if isinstance(message.get(\"content\"), str):\n          total_chars += len(message[\"content\"])\n        elif isinstance(message.get(\"content\"), list):\n          for content_item in message[\"content\"]:\n            if content_item.get(\"type\") == \"text\":\n              total_chars += len(content_item.get(\"text\", \"\"))\n      \n      # Very rough approximation: 1 token ≈ 4 characters\n      token_count = total_chars // 4\n      \n      return CountTokensResponse({\n          \"total_tokens\": token_count,\n          \"prompt_tokens\": token_count,\n          \"completion_tokens\": 0\n      })\n\u0005End File\u0006{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"1kSAD9RWlOA2\"\n   },\n   \"source\": [\n    \"# Gemini, OpenAI and Anthropic Benchmarking\\n\",\n    \"\\n\",\n    \"This notebook demonstrates how to use the Gemini API directly with our Python SDK and shows how to compare the results with OpenAI's or Anthropic's models.\\n\",\n    \"\\n\",\n    \"When you run the notebook cells below, you should see instructions on how to use it. If you don't have a Gemini API key or OpenAI API key already, follow the instructions in these notebook cells to obtain one.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"Wl4iW_UcWiVq\"\n   },\n   \"source\": [\n    \"## Setup\\n\",\n    \"\\n\",\n    \"First, let's set up the APIs we want to use.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"cellView\": \"form\",\n    \"id\": \"1cNZ4Oq2WmYn\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"#@title Set up the Gemini API\\n\",\n    \"\\n\",\n    \"# !pip install -q -U google-generativeai\\n\",\n    \"import google.generativeai as genai\\n\",\n    \"import os\\n\",\n    \"\\n\",\n    \"# Choose the Gemini model\\n\",\n    \"GEMINI_MODEL = \\\"gemini-1.5-pro\\\" #@param [\\\"gemini-1.0-pro\\\", \\\"gemini-1.5-flash\\\", \\\"gemini-1.5-pro\\\"]\\n\",\n    \"\\n\",\n    \"# Configure the API key\\n\",\n    \"API_KEY = \\\"\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"\\n\",\n    \"if not API_KEY and not os.environ.get(\\\"GOOGLE_API_KEY\\\"):\\n\",\n    \"  print(\\\"No Gemini API key found.\\\")\\n\",\n    \"  print(\\\"Get a Gemini API key from https://makersuite.google.com/app/apikey\\\")\\n\",\n    \"  print(\\\"You can also set the GOOGLE_API_KEY environment variable.\\\")\\n\",\n    \"else:\\n\",\n    \"  API_KEY = API_KEY or os.environ.get(\\\"GOOGLE_API_KEY\\\")\\n\",\n    \"  genai.configure(api_key=API_KEY)\\n\",\n    \"  gemini_model = genai.GenerativeModel(GEMINI_MODEL)\\n\",\n    \"  print(f\\\"Gemini API initialized with model: {GEMINI_MODEL}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"cellView\": \"form\",\n    \"id\": \"y8Lf7mzKasHZ\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"#@title Set up the OpenAI API (optional)\\n\",\n    \"\\n\",\n    \"use_openai = True  #@param {type:\\\"boolean\\\"}\\n\",\n    \"OPENAI_MODEL = \\\"gpt-4o\\\" #@param [\\\"gpt-3.5-turbo-0125\\\", \\\"gpt-4-turbo-2024-04-09\\\", \\\"gpt-4o\\\"]\\n\",\n    \"\\n\",\n    \"if use_openai:\\n\",\n    \"  # !pip install -q openai\\n\",\n    \"  \\n\",\n    \"  # Import our OpenAI wrapper for compatibility with Gemini API\\n\",\n    \"  from wrappers import openai_wrapper\\n\",\n    \"  \\n\",\n    \"  # Configure OpenAI API key\\n\",\n    \"  OPENAI_API_KEY = \\\"\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"  \\n\",\n    \"  if not OPENAI_API_KEY and not os.environ.get(\\\"OPENAI_API_KEY\\\"):\\n\",\n    \"    print(\\\"No OpenAI API key found.\\\")\\n\",\n    \"    print(\\\"Get an OpenAI API key from https://platform.openai.com/api-keys\\\")\\n\",\n    \"    print(\\\"You can also set the OPENAI_API_KEY environment variable.\\\")\\n\",\n    \"    use_openai = False\\n\",\n    \"  else:\\n\",\n    \"    if OPENAI_API_KEY:\\n\",\n    \"      os.environ[\\\"OPENAI_API_KEY\\\"] = OPENAI_API_KEY\\n\",\n    \"    openai_model = openai_wrapper.get_model(OPENAI_MODEL)\\n\",\n    \"    print(f\\\"OpenAI API initialized with model: {OPENAI_MODEL}\\\")\\n\",\n    \"else:\\n\",\n    \"  print(\\\"OpenAI API setup skipped.\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"cellView\": \"form\",\n    \"id\": \"9GZVn2YM0ZxN\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"#@title Set up the Anthropic API (optional)\\n\",\n    \"\\n\",\n    \"use_anthropic = True  #@param {type:\\\"boolean\\\"}\\n\",\n    \"ANTHROPIC_MODEL = \\\"claude-3-opus-20240229\\\" #@param [\\\"claude-3-opus-20240229\\\", \\\"claude-3-sonnet-20240229\\\", \\\"claude-3-haiku-20240307\\\"]\\n\",\n    \"\\n\",\n    \"if use_anthropic:\\n\",\n    \"  # !pip install -q anthropic\\n\",\n    \"  \\n\",\n    \"  # Import our Anthropic wrapper for compatibility with Gemini API\\n\",\n    \"  from wrappers import anthropic_wrapper\\n\",\n    \"  \\n\",\n    \"  # Configure Anthropic API key\\n\",\n    \"  ANTHROPIC_API_KEY = \\\"\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"  \\n\",\n    \"  if not ANTHROPIC_API_KEY and not os.environ.get(\\\"ANTHROPIC_API_KEY\\\"):\\n\",\n    \"    print(\\\"No Anthropic API key found.\\\")\\n\",\n    \"    print(\\\"Get an Anthropic API key from https://console.anthropic.com/settings/keys\\\")\\n\",\n    \"    print(\\\"You can also set the ANTHROPIC_API_KEY environment variable.\\\")\\n\",\n    \"    use_anthropic = False\\n\",\n    \"  else:\\n\",\n    \"    if ANTHROPIC_API_KEY:\\n\",\n    \"      os.environ[\\\"ANTHROPIC_API_KEY\\\"] = ANTHROPIC_API_KEY\\n\",\n    \"    anthropic_model = anthropic_wrapper.get_model(ANTHROPIC_MODEL)\\n\",\n    \"    print(f\\\"Anthropic API initialized with model: {ANTHROPIC_MODEL}\\\")\\n\",\n    \"else:\\n\",\n    \"  print(\\\"Anthropic API setup skipped.\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"_TDBNtwSWrLb\"\n   },\n   \"source\": [\n    \"## Helper Functions\\n\",\n    \"\\n\",\n    \"Let's define some helper functions to make it easier to compare results.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"PWWNwz3vZTjH\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"import time\\n\",\n    \"from IPython.display import display, Markdown\\n\",\n    \"\\n\",\n    \"def get_response(model, prompt, stream=False, temperature=0.7):\\n\",\n    \"  \\\"\\\"\\\"Get a response from the model and measure response time.\\\"\\\"\\\"\\n\",\n    \"  start_time = time.time()\\n\",\n    \"  try:\\n\",\n    \"    generation_config = {\\n\",\n    \"        \\\"temperature\\\": temperature,\\n\",\n    \"        \\\"top_p\\\": 1.0,\\n\",\n    \"        \\\"top_k\\\": 32,\\n\",\n    \"        \\\"max_output_tokens\\\": 2048,\\n\",\n    \"    }\\n\",\n    \"    \\n\",\n    \"    response = model.generate_content(\\n\",\n    \"        prompt,\\n\",\n    \"        generation_config=generation_config,\\n\",\n    \"        stream=stream\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    if stream:\\n\",\n    \"      # For streaming, we collect the entire response\\n\",\n    \"      text = \\\"\\\"\\n\",\n    \"      for chunk in response:\\n\",\n    \"        if hasattr(chunk, \\\"candidates\\\") and chunk.candidates:\\n\",\n    \"          for candidate in chunk.candidates:\\n\",\n    \"            if hasattr(candidate.content, \\\"parts\\\"):\\n\",\n    \"              for part in candidate.content.parts:\\n\",\n    \"                if hasattr(part, \\\"text\\\"):\\n\",\n    \"                  text += part.text\\n\",\n    \"                  display(Markdown(text))\\n\",\n    \"                  # Clear output for smoother streaming\\n\",\n    \"                  # import IPython.display\\n\",\n    \"                  # IPython.display.clear_output(wait=True)\\n\",\n    \"      response_text = text\\n\",\n    \"    else:\\n\",\n    \"      # For non-streaming, just get the text\\n\",\n    \"      if hasattr(response, \\\"candidates\\\") and response.candidates:\\n\",\n    \"        candidate = response.candidates[0]\\n\",\n    \"        if hasattr(candidate.content, \\\"parts\\\"):\\n\",\n    \"          response_text = \\\"\\\".join([part.text for part in candidate.content.parts if hasattr(part, \\\"text\\\")])\\n\",\n    \"        elif hasattr(candidate.content, \\\"text\\\"):\\n\",\n    \"          response_text = candidate.content.text\\n\",\n    \"        else:\\n\",\n    \"          response_text = str(candidate.content)\\n\",\n    \"      else:\\n\",\n    \"        response_text = str(response)\\n\",\n    \"  except Exception as e:\\n\",\n    \"    response_text = f\\\"Error: {str(e)}\\\"\\n\",\n    \"  \\n\",\n    \"  end_time = time.time()\\n\",\n    \"  return response_text, end_time - start_time\\n\",\n    \"\\n\",\n    \"def compare_models(prompt, models, model_names, stream=False, temperature=0.7):\\n\",\n    \"  \\\"\\\"\\\"Compare responses from multiple models.\\\"\\\"\\\"\\n\",\n    \"  results = []\\n\",\n    \"  \\n\",\n    \"  # Count tokens for Gemini model\\n\",\n    \"  if hasattr(models[0], \\\"count_tokens\\\"):\\n\",\n    \"    try:\\n\",\n    \"      token_count = models[0].count_tokens(prompt)\\n\",\n    \"      print(f\\\"Input tokens: {token_count.total_tokens}\\\\n\\\")\\n\",\n    \"    except Exception as e:\\n\",\n    \"      print(f\\\"Could not count tokens: {e}\\\\n\\\")\\n\",\n    \"  \\n\",\n    \"  for i, (model, name) in enumerate(zip(models, model_names)):\\n\",\n    \"    print(f\\\"\\\\n--- {name} response ---\\\")\\n\",\n    \"    response, response_time = get_response(model, prompt, stream=stream, temperature=temperature)\\n\",\n    \"    if not stream:  # For non-streaming, display response after completion\\n\",\n    \"      display(Markdown(response))\\n\",\n    \"    print(f\\\"Response time: {response_time:.2f} seconds\\\")\\n\",\n    \"    results.append((name, response, response_time))\\n\",\n    \"  \\n\",\n    \"  return results\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"cxPz-VClZz_Z\"\n   },\n   \"source\": [\n    \"## Comparison Interface\\n\",\n    \"\\n\",\n    \"Now, let's create a simple interface to compare the models.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"cellView\": \"form\",\n    \"id\": \"UB0EFLqMZ-fD\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"#@title Run a comparison\\n\",\n    \"prompt = \\\"What is the most efficient algorithm for sorting a large dataset with minimal memory usage?\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"use_streaming = False #@param {type:\\\"boolean\\\"}\\n\",\n    \"temperature = 0.7 #@param {type:\\\"slider\\\", min:0.0, max:1.0, step:0.1}\\n\",\n    \"\\n\",\n    \"# Determine which models to use\\n\",\n    \"models = [gemini_model]\\n\",\n    \"model_names = [GEMINI_MODEL]\\n\",\n    \"\\n\",\n    \"if 'use_openai' in globals() and use_openai:\\n\",\n    \"  models.append(openai_model)\\n\",\n    \"  model_names.append(OPENAI_MODEL)\\n\",\n    \"\\n\",\n    \"if 'use_anthropic' in globals() and use_anthropic:\\n\",\n    \"  models.append(anthropic_model)\\n\",\n    \"  model_names.append(ANTHROPIC_MODEL)\\n\",\n    \"\\n\",\n    \"print(f\\\"Comparing {', '.join(model_names)}...\\\\n\\\")\\n\",\n    \"print(f\\\"Prompt: {prompt}\\\\n\\\")\\n\",\n    \"\\n\",\n    \"results = compare_models(prompt, models, model_names, stream=use_streaming, temperature=temperature)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"1p-Kb-jNTbr2\"\n   },\n   \"source\": [\n    \"## Examples to Try\\n\",\n    \"\\n\",\n    \"Here are some examples you can try by copying into the prompt field above:\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"Xnl9RxQaTgmT\"\n   },\n   \"source\": [\n    \"### Creative Writing\\n\",\n    \"\\n\",\n    \"```\\n\",\n    \"Write a short story about a robot that learns to paint, set in Renaissance Italy.\\n\",\n    \"```\\n\",\n    \"\\n\",\n    \"### Complex Reasoning\\n\",\n    \"\\n\",\n    \"```\\n\",\n    \"A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost? Explain your reasoning step by step.\\n\",\n    \"```\\n\",\n    \"\\n\",\n    \"### Technical Knowledge\\n\",\n    \"\\n\",\n    \"```\\n\",\n    \"Explain the CAP theorem in distributed systems and provide practical examples of how different databases make tradeoffs between consistency, availability, and partition tolerance.\\n\",\n    \"```\\n\",\n    \"\\n\",\n    \"### Code Generation\\n\",\n    \"\\n\",\n    \"```\\n\",\n    \"Write a Python function that implements a binary search tree with methods for insertion, deletion, and in-order traversal. Include comments explaining the time complexity of each operation.\\n\",\n    \"```\\n\",\n    \"\\n\",\n    \"### Summarization\\n\",\n    \"\\n\",\n    \"```\\n\",\n    \"Summarize the key principles of machine learning in no more than 5 bullet points that would be understandable to a high school student.\\n\",\n    \"```\\n\",\n    \"\\n\",\n    \"### Multi-step Reasoning\\n\",\n    \"\\n\",\n    \"```\\n\",\n    \"If I have 8 socks in a drawer: 4 black, 3 blue, and 1 red, what is the minimum number of socks I need to take out to guarantee I have a matching pair? Explain your reasoning step by step.\\n\",\n    \"```\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"2qWWzc0_j0K2\"\n   },\n   \"source\": [\n    \"## Working with Images\\n\",\n    \"\\n\",\n    \"You can also compare how models analyze images. Let's define a helper function for this.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"Hn1VeI7dj4gv\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"import os\\n\",\n    \"import requests\\n\",\n    \"import tempfile\\n\",\n    \"from PIL import Image\\n\",\n    \"import base64\\n\",\n    \"from IPython.display import display, Image as IPImage\\n\",\n    \"\\n\",\n    \"def download_image(url, display_image=True):\\n\",\n    \"  \\\"\\\"\\\"Download an image from a URL and return path and base64 data.\\\"\\\"\\\"\\n\",\n    \"  try:\\n\",\n    \"    response = requests.get(url, stream=True)\\n\",\n    \"    response.raise_for_status()\\n\",\n    \"    \\n\",\n    \"    # Create a temporary file\\n\",\n    \"    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".jpg\\\")\\n\",\n    \"    with open(temp_file.name, 'wb') as f:\\n\",\n    \"      for chunk in response.iter_content(chunk_size=8192):\\n\",\n    \"        f.write(chunk)\\n\",\n    \"    \\n\",\n    \"    # Get image MIME type\\n\",\n    \"    img = Image.open(temp_file.name)\\n\",\n    \"    mime_type = f\\\"image/{img.format.lower()}\\\"\\n\",\n    \"    \\n\",\n    \"    # Convert to base64\\n\",\n    \"    with open(temp_file.name, \\\"rb\\\") as img_file:\\n\",\n    \"      base64_data = base64.b64encode(img_file.read()).decode('utf-8')\\n\",\n    \"    \\n\",\n    \"    if display_image:\\n\",\n    \"      display(IPImage(filename=temp_file.name, width=500))\\n\",\n    \"    \\n\",\n    \"    return temp_file.name, base64_data, mime_type\\n\",\n    \"  except Exception as e:\\n\",\n    \"    print(f\\\"Error downloading image: {e}\\\")\\n\",\n    \"    return None, None, None\\n\",\n    \"\\n\",\n    \"def compare_models_with_image(text_prompt, image_url, models, model_names, temperature=0.7):\\n\",\n    \"  \\\"\\\"\\\"Compare responses from multiple models with an image input.\\\"\\\"\\\"\\n\",\n    \"  # Download the image\\n\",\n    \"  img_path, img_base64, mime_type = download_image(image_url)\\n\",\n    \"  if not img_path:\\n\",\n    \"    return []\\n\",\n    \"  \\n\",\n    \"  results = []\\n\",\n    \"  \\n\",\n    \"  for i, (model, name) in enumerate(zip(models, model_names)):\\n\",\n    \"    print(f\\\"\\\\n--- {name} response ---\\\")\\n\",\n    \"    start_time = time.time()\\n\",\n    \"    \\n\",\n    \"    try:\\n\",\n    \"      # Different ways to format the image input for different APIs\\n\",\n    \"      if \\\"gemini\\\" in name.lower():\\n\",\n    \"        # For Gemini API\\n\",\n    \"        prompt = [\\n\",\n    \"            {\\\"role\\\": \\\"user\\\", \\\"parts\\\": [\\n\",\n    \"                {\\\"text\\\": text_prompt},\\n\",\n    \"                {\\\"inline_data\\\": {\\\"mime_type\\\": mime_type, \\\"data\\\": img_base64}}\\n\",\n    \"            ]}\\n\",\n    \"        ]\\n\",\n    \"      elif \\\"gpt\\\" in name.lower():\\n\",\n    \"        # For OpenAI API\\n\",\n    \"        prompt = [\\n\",\n    \"            {\\\"role\\\": \\\"user\\\", \\\"content\\\": [\\n\",\n    \"                {\\\"type\\\": \\\"text\\\", \\\"text\\\": text_prompt},\\n\",\n    \"                {\\\"type\\\": \\\"image_url\\\", \\\"image_url\\\": {\\\"url\\\": f\\\"data:{mime_type};base64,{img_base64}\\\"}}\\n\",\n    \"            ]}\\n\",\n    \"        ]\\n\",\n    \"      elif \\\"claude\\\" in name.lower():\\n\",\n    \"        # For Anthropic API\\n\",\n    \"        prompt = [\\n\",\n    \"            {\\\"role\\\": \\\"user\\\", \\\"content\\\": [\\n\",\n    \"                {\\\"type\\\": \\\"text\\\", \\\"text\\\": text_prompt},\\n\",\n    \"                {\\\"type\\\": \\\"image\\\", \\\"source\\\": {\\\"type\\\": \\\"base64\\\", \\\"media_type\\\": mime_type, \\\"data\\\": img_base64}}\\n\",\n    \"            ]}\\n\",\n    \"        ]\\n\",\n    \"      \\n\",\n    \"      generation_config = {\\\"temperature\\\": temperature}\\n\",\n    \"      response = model.generate_content(prompt, generation_config=generation_config)\\n\",\n    \"      \\n\",\n    \"      # Extract text from response\\n\",\n    \"      if hasattr(response, \\\"candidates\\\") and response.candidates:\\n\",\n    \"        candidate = response.candidates[0]\\n\",\n    \"        if hasattr(candidate.content, \\\"parts\\\"):\\n\",\n    \"          response_text = \\\"\\\".join([part.text for part in candidate.content.parts if hasattr(part, \\\"text\\\")])\\n\",\n    \"        elif hasattr(candidate.content, \\\"text\\\"):\\n\",\n    \"          response_text = candidate.content.text\\n\",\n    \"        else:\\n\",\n    \"          response_text = str(candidate.content)\\n\",\n    \"      else:\\n\",\n    \"        response_text = str(response)\\n\",\n    \"      \\n\",\n    \"    except Exception as e:\\n\",\n    \"      response_text = f\\\"Error: {str(e)}\\\"\\n\",\n    \"    \\n\",\n    \"    end_time = time.time()\\n\",\n    \"    response_time = end_time - start_time\\n\",\n    \"    \\n\",\n    \"    display(Markdown(response_text))\\n\",\n    \"    print(f\\\"Response time: {response_time:.2f} seconds\\\")\\n\",\n    \"    results.append((name, response_text, response_time))\\n\",\n    \"  \\n\",\n    \"  # Clean up the temporary file\\n\",\n    \"  if img_path and os.path.exists(img_path):\\n\",\n    \"    os.unlink(img_path)\\n\",\n    \"  \\n\",\n    \"  return results\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"cellView\": \"form\",\n    \"id\": \"6E2RlKd3lELn\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"#@title Run an image comparison\\n\",\n    \"image_url = \\\"https://storage.googleapis.com/github-repo/img/tokyo_street.jpg\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"text_prompt = \\\"What do you see in this image? Describe it in detail.\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"temperature = 0.7 #@param {type:\\\"slider\\\", min:0.0, max:1.0, step:0.1}\\n\",\n    \"\\n\",\n    \"# Determine which models to use\\n\",\n    \"models = [gemini_model]\\n\",\n    \"model_names = [GEMINI_MODEL]\\n\",\n    \"\\n\",\n    \"if 'use_openai' in globals() and use_openai:\\n\",\n    \"  models.append(openai_model)\\n\",\n    \"  model_names.append(OPENAI_MODEL)\\n\",\n    \"\\n\",\n    \"if 'use_anthropic' in globals() and use_anthropic:\\n\",\n    \"  models.append(anthropic_model)\\n\",\n    \"  model_names.append(ANTHROPIC_MODEL)\\n\",\n    \"\\n\",\n    \"print(f\\\"Comparing {', '.join(model_names)} with image...\\\\n\\\")\\n\",\n    \"print(f\\\"Prompt: {text_prompt}\\\\n\\\")\\n\",\n    \"\\n\",\n    \"results = compare_models_with_image(text_prompt, image_url, models, model_names, temperature=temperature)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"G9R2-z6WxP5g\"\n   },\n   \"source\": [\n    \"## Sample Image URLs\\n\",\n    \"\\n\",\n    \"Here are some sample image URLs you can try:\\n\",\n    \"\\n\",\n    \"1. Tokyo street scene: `https://storage.googleapis.com/github-repo/img/tokyo_street.jpg`\\n\",\n    \"2. Chess position: `https://storage.googleapis.com/github-repo/img/chess_position.jpg`\\n\",\n    \"3. Chart: `https://storage.googleapis.com/github-repo/img/chart_example.png`\\n\",\n    \"4. Code screenshot: `https://storage.googleapis.com/github-repo/img/code_screenshot.png`\\n\",\n    \"5. Food image: `https://storage.googleapis.com/github-repo/img/food_image.jpg`\\n\",\n    \"\\n\",\n    \"Try different prompts like:\\n\",\n    \"- \\\"What's happening in this image?\\\"\\n\",\n    \"- \\\"What's the best move for white in this chess position?\\\"\\n\",\n    \"- \\\"Explain what this chart is showing.\\\"\\n\",\n    \"- \\\"What does this code do? Are there any bugs?\\\"\\n\",\n    \"- \\\"What ingredients do you see in this dish?\\\"\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"a1PvtMwT3NXG\"\n   },\n   \"source\": [\n    \"## Conclusion\\n\",\n    \"\\n\",\n    \"This notebook demonstrates how to compare different AI models side by side using a common interface. You can extend this to include more complex prompts, additional models, or different evaluation criteria depending on your needs.\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"colab\": {\n   \"provenance\": []\n  },\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"name\": \"python\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 0\n}\n\u0005End File\u0006{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"LgbGV0cz7uy1\"\n   },\n   \"source\": [\n    \"# Extract Information from PDF with LangChain and Gemini\\n\",\n    \"\\n\",\n    \"This notebook shows how to use LangChain with Gemini to extract structured information from PDF files. Specifically, we'll:\\n\",\n    \"\\n\",\n    \"1. Load and split PDF documents with LangChain\\n\",\n    \"2. Process the content to extract specific information using Gemini\\n\",\n    \"3. Structure the extracted information for further analysis\\n\",\n    \"\\n\",\n    \"Let's get started!\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"Yks7jg5nUFJc\"\n   },\n   \"source\": [\n    \"## Install and Import Dependencies\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"OEbyRVsA78FB\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!pip install -q langchain-google-genai langchain pypdf pydantic unstructured\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"pHUoZt2uUQXx\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"import os\\n\",\n    \"import json\\n\",\n    \"import textwrap\\n\",\n    \"from typing import List, Optional\\n\",\n    \"\\n\",\n    \"from langchain_google_genai import ChatGoogleGenerativeAI\\n\",\n    \"from langchain.text_splitter import RecursiveCharacterTextSplitter\\n\",\n    \"from langchain.chains import LLMChain\\n\",\n    \"from langchain.prompts import PromptTemplate\\n\",\n    \"from langchain.document_loaders import PyPDFLoader\\n\",\n    \"from pydantic import BaseModel, Field\\n\",\n    \"from IPython.display import display, HTML\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"5b7NqWrdUUXN\"\n   },\n   \"source\": [\n    \"## Setup API Key\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"cellView\": \"form\",\n    \"id\": \"XSQPZZ82UU7x\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# @title Set up the API key\\n\",\n    \"GOOGLE_API_KEY = \\\"\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"\\n\",\n    \"# Check if API key is provided\\n\",\n    \"if not GOOGLE_API_KEY:\\n\",\n    \"    # Check if the API key is available as an environment variable\\n\",\n    \"    if \\\"GOOGLE_API_KEY\\\" in os.environ:\\n\",\n    \"        GOOGLE_API_KEY = os.environ[\\\"GOOGLE_API_KEY\\\"]\\n\",\n    \"    else:\\n\",\n    \"        print(\\\"Please provide your Google API key!\\\")\\n\",\n    \"else:\\n\",\n    \"    # Set the API key as an environment variable\\n\",\n    \"    os.environ[\\\"GOOGLE_API_KEY\\\"] = GOOGLE_API_KEY\\n\",\n    \"\\n\",\n    \"# Initialize Gemini model\\n\",\n    \"gemini_model = \\\"gemini-1.5-pro\\\"\\n\",\n    \"print(f\\\"Using model: {gemini_model}\\\")\\n\",\n    \"\\n\",\n    \"llm = ChatGoogleGenerativeAI(model=gemini_model, temperature=0.1)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"FqSxBl3_Ueqm\"\n   },\n   \"source\": [\n    \"## Download Sample PDF\\n\",\n    \"\\n\",\n    \"Let's download a sample financial report to work with. We'll use the 2023 annual report from a public company.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"WaRVtWpRUhWB\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!wget -q -O annual_report.pdf \\\"https://storage.googleapis.com/github-repo/pdf/TSLA-2023-Annual-Report.pdf\\\"\\n\",\n    \"pdf_path = \\\"annual_report.pdf\\\"\\n\",\n    \"\\n\",\n    \"# Check file size\\n\",\n    \"import os\\n\",\n    \"file_size_mb = os.path.getsize(pdf_path) / (1024 * 1024)\\n\",\n    \"print(f\\\"Downloaded PDF size: {file_size_mb:.2f} MB\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"dW2dCgCdUzQ9\"\n   },\n   \"source\": [\n    \"## Load and Process the PDF Document\\n\",\n    \"\\n\",\n    \"Now let's load the PDF and split it into manageable chunks:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"4mXP6XHMU0Vh\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Load the PDF file\\n\",\n    \"loader = PyPDFLoader(pdf_path)\\n\",\n    \"pages = loader.load()\\n\",\n    \"\\n\",\n    \"# Print basic information\\n\",\n    \"print(f\\\"Loaded {len(pages)} pages from the PDF\\\")\\n\",\n    \"print(f\\\"First page preview: {pages[0].page_content[:200]}...\\\")\\n\",\n    \"\\n\",\n    \"# Use RecursiveCharacterTextSplitter to split the document into chunks\\n\",\n    \"text_splitter = RecursiveCharacterTextSplitter(\\n\",\n    \"    chunk_size=10000,\\n\",\n    \"    chunk_overlap=1000,\\n\",\n    \"    separators=[\\\"\\\\n\\\\n\\\", \\\"\\\\n\\\", \\\".\\\", \\\" \\\", \\\"\\\"]\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"chunks = text_splitter.split_documents(pages)\\n\",\n    \"print(f\\\"Split into {len(chunks)} chunks\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"CUDsDKaiVCbt\"\n   },\n   \"source\": [\n    \"## Define Information Extraction Structure\\n\",\n    \"\\n\",\n    \"Let's define what information we want to extract from the financial report using Pydantic models. For this example, we'll focus on key financial metrics, risk factors, and future outlook.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"6eCsLQl-VK3t\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"class FinancialMetric(BaseModel):\\n\",\n    \"    \\\"\\\"\\\"A financial metric with name, value, and year\\\"\\\"\\\"\\n\",\n    \"    name: str = Field(description=\\\"The name of the financial metric (e.g., Revenue, Net Income)\\\")\\n\",\n    \"    value: str = Field(description=\\\"The value of the metric with units (e.g., $10.2 billion)\\\")\\n\",\n    \"    year: str = Field(description=\\\"The fiscal year or period this metric refers to\\\")\\n\",\n    \"    page_number: Optional[int] = Field(description=\\\"The page number where this information was found\\\")\\n\",\n    \"\\n\",\n    \"class RiskFactor(BaseModel):\\n\",\n    \"    \\\"\\\"\\\"A risk factor identified in the report\\\"\\\"\\\"\\n\",\n    \"    title: str = Field(description=\\\"Short title or category of the risk\\\")\\n\",\n    \"    description: str = Field(description=\\\"Brief description of the risk factor\\\")\\n\",\n    \"    potential_impact: str = Field(description=\\\"How this might impact the company\\\")\\n\",\n    \"    page_number: Optional[int] = Field(description=\\\"The page number where this information was found\\\")\\n\",\n    \"\\n\",\n    \"class BusinessOutlook(BaseModel):\\n\",\n    \"    \\\"\\\"\\\"Future outlook and strategic direction\\\"\\\"\\\"\\n\",\n    \"    summary: str = Field(description=\\\"Summary of the company's future outlook\\\")\\n\",\n    \"    key_initiatives: List[str] = Field(description=\\\"List of key strategic initiatives or focus areas\\\")\\n\",\n    \"    market_trends: List[str] = Field(description=\\\"Relevant market or industry trends mentioned\\\")\\n\",\n    \"    page_number: Optional[int] = Field(description=\\\"The page number where this information was found\\\")\\n\",\n    \"\\n\",\n    \"class FinancialReportExtraction(BaseModel):\\n\",\n    \"    \\\"\\\"\\\"Complete extraction from a financial report\\\"\\\"\\\"\\n\",\n    \"    company_name: str = Field(description=\\\"Name of the company\\\")\\n\",\n    \"    report_year: str = Field(description=\\\"Year of the report\\\")\\n\",\n    \"    financial_metrics: List[FinancialMetric] = Field(description=\\\"Key financial metrics extracted from the report\\\")\\n\",\n    \"    risk_factors: List[RiskFactor] = Field(description=\\\"Major risk factors mentioned in the report\\\")\\n\",\n    \"    business_outlook: BusinessOutlook = Field(description=\\\"Future business outlook and strategic direction\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"1Bp14T_LVcn-\"\n   },\n   \"source\": [\n    \"## Create Extraction Chains\\n\",\n    \"\\n\",\n    \"Now, let's create LangChain chains to extract each component of information:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"x7BLAjx8VeE9\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Prompt template for extracting financial metrics\\n\",\n    \"financial_metrics_template = \\\"\\\"\\\"\\n\",\n    \"You are a financial analyst extracting key financial metrics from annual reports.\\n\",\n    \"\\n\",\n    \"Extract all financial metrics from the following text that comes from page {page_number} of a financial report.\\n\",\n    \"Focus on important metrics like revenue, net income, gross margin, operating expenses, cash flow, etc.\\n\",\n    \"\\n\",\n    \"TEXT:\\n\",\n    \"{text}\\n\",\n    \"\\n\",\n    \"Provide the results as a JSON list of objects, each with these fields:\\n\",\n    \"- name: The name of the financial metric\\n\",\n    \"- value: The value with units (e.g., $10.2 billion)\\n\",\n    \"- year: The fiscal year or time period\\n\",\n    \"- page_number: {page_number}\\n\",\n    \"\\n\",\n    \"Only include metrics with clear values and time periods. If no metrics are found, return an empty list.\\n\",\n    \"Format your response as valid JSON that can be parsed, with nothing else before or after.\\n\",\n    \"\\\"\\\"\\\"\\n\",\n    \"\\n\",\n    \"financial_metrics_prompt = PromptTemplate(\\n\",\n    \"    template=financial_metrics_template,\\n\",\n    \"    input_variables=[\\\"text\\\", \\\"page_number\\\"]\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"financial_metrics_chain = LLMChain(\\n\",\n    \"    llm=llm,\\n\",\n    \"    prompt=financial_metrics_prompt,\\n\",\n    \"    output_key=\\\"financial_metrics\\\"\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"# Prompt template for extracting risk factors\\n\",\n    \"risk_factors_template = \\\"\\\"\\\"\\n\",\n    \"You are a risk analyst identifying key risk factors in financial reports.\\n\",\n    \"\\n\",\n    \"Extract all risk factors mentioned in the following text that comes from page {page_number} of a financial report.\\n\",\n    \"Focus on business risks, market risks, operational risks, regulatory risks, etc.\\n\",\n    \"\\n\",\n    \"TEXT:\\n\",\n    \"{text}\\n\",\n    \"\\n\",\n    \"Provide the results as a JSON list of objects, each with these fields:\\n\",\n    \"- title: Short title or category of the risk\\n\",\n    \"- description: Brief description of the risk factor (1-2 sentences)\\n\",\n    \"- potential_impact: How this risk might impact the company\\n\",\n    \"- page_number: {page_number}\\n\",\n    \"\\n\",\n    \"Only include clearly stated risks. If no risk factors are found, return an empty list.\\n\",\n    \"Format your response as valid JSON that can be parsed, with nothing else before or after.\\n\",\n    \"\\\"\\\"\\\"\\n\",\n    \"\\n\",\n    \"risk_factors_prompt = PromptTemplate(\\n\",\n    \"    template=risk_factors_template,\\n\",\n    \"    input_variables=[\\\"text\\\", \\\"page_number\\\"]\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"risk_factors_chain = LLMChain(\\n\",\n    \"    llm=llm,\\n\",\n    \"    prompt=risk_factors_prompt,\\n\",\n    \"    output_key=\\\"risk_factors\\\"\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"# Prompt template for extracting business outlook\\n\",\n    \"business_outlook_template = \\\"\\\"\\\"\\n\",\n    \"You are a business analyst extracting information about future outlook and strategy from financial reports.\\n\",\n    \"\\n\",\n    \"Extract information about the company's future plans, outlook, and strategic direction from the following text that comes from page {page_number} of a financial report.\\n\",\n    \"Focus on statements about future goals, strategic initiatives, expected market trends, and forward-looking statements.\\n\",\n    \"\\n\",\n    \"TEXT:\\n\",\n    \"{text}\\n\",\n    \"\\n\",\n    \"Provide the results as a JSON object with these fields:\\n\",\n    \"- summary: Overall summary of the company's future outlook (2-3 sentences)\\n\",\n    \"- key_initiatives: List of key strategic initiatives or focus areas (strings)\\n\",\n    \"- market_trends: List of relevant market or industry trends mentioned (strings)\\n\",\n    \"- page_number: {page_number}\\n\",\n    \"\\n\",\n    \"Only include clearly stated outlook information. If no relevant information is found, return null for summary and empty lists for initiatives and trends.\\n\",\n    \"Format your response as valid JSON that can be parsed, with nothing else before or after.\\n\",\n    \"\\\"\\\"\\\"\\n\",\n    \"\\n\",\n    \"business_outlook_prompt = PromptTemplate(\\n\",\n    \"    template=business_outlook_template,\\n\",\n    \"    input_variables=[\\\"text\\\", \\\"page_number\\\"]\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"business_outlook_chain = LLMChain(\\n\",\n    \"    llm=llm,\\n\",\n    \"    prompt=business_outlook_prompt,\\n\",\n    \"    output_key=\\\"business_outlook\\\"\\n\",\n    \")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"4Qi-vxjpWHZ2\"\n   },\n   \"source\": [\n    \"## Company and Report Identification Chain\\n\",\n    \"\\n\",\n    \"Let's create a chain to identify the company name and report year:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"ZV-KA15wWKoN\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Prompt template for identifying company and report year\\n\",\n    \"company_info_template = \\\"\\\"\\\"\\n\",\n    \"Extract the company name and report year from the following text that comes from the first few pages of a financial report.\\n\",\n    \"\\n\",\n    \"TEXT:\\n\",\n    \"{text}\\n\",\n    \"\\n\",\n    \"Provide the results as a JSON object with these fields:\\n\",\n    \"- company_name: The full name of the company\\n\",\n    \"- report_year: The fiscal year of the report\\n\",\n    \"\\n\",\n    \"Format your response as valid JSON that can be parsed, with nothing else before or after.\\n\",\n    \"\\\"\\\"\\\"\\n\",\n    \"\\n\",\n    \"company_info_prompt = PromptTemplate(\\n\",\n    \"    template=company_info_template,\\n\",\n    \"    input_variables=[\\\"text\\\"]\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"company_info_chain = LLMChain(\\n\",\n    \"    llm=llm,\\n\",\n    \"    prompt=company_info_prompt,\\n\",\n    \"    output_key=\\\"company_info\\\"\\n\",\n    \")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"BucmV-zXWSvl\"\n   },\n   \"source\": [\n    \"## Process the Document and Extract Information\\n\",\n    \"\\n\",\n    \"Now let's process our document chunks to extract all the information:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"HJL-zw7NWT7t\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# First identify company name and report year from the first chunk\\n\",\n    \"try:\\n\",\n    \"    company_info_response = company_info_chain.run(text=chunks[0].page_content)\\n\",\n    \"    company_info = json.loads(company_info_response)\\n\",\n    \"    print(f\\\"Identified: {company_info['company_name']}, Report Year: {company_info['report_year']}\\\")\\n\",\n    \"except Exception as e:\\n\",\n    \"    print(f\\\"Error extracting company info: {e}\\\")\\n\",\n    \"    company_info = {\\\"company_name\\\": \\\"Unknown Company\\\", \\\"report_year\\\": \\\"Unknown Year\\\"}\\n\",\n    \"\\n\",\n    \"# Extract financial metrics, risk factors, and business outlook from all chunks\\n\",\n    \"all_financial_metrics = []\\n\",\n    \"all_risk_factors = []\\n\",\n    \"all_business_outlook = []\\n\",\n    \"\\n\",\n    \"# Process a subset of chunks to save time and API calls\\n\",\n    \"sample_chunks = chunks[:10]  # Process first 10 chunks\\n\",\n    \"\\n\",\n    \"for i, chunk in enumerate(sample_chunks):\\n\",\n    \"    print(f\\\"Processing chunk {i+1}/{len(sample_chunks)} (page {chunk.metadata.get('page', 'unknown')})...\\\")\\n\",\n    \"    \\n\",\n    \"    page_number = chunk.metadata.get('page', 0) + 1  # Add 1 to convert from 0-indexed to 1-indexed\\n\",\n    \"    \\n\",\n    \"    # Extract financial metrics\\n\",\n    \"    try:\\n\",\n    \"        metrics_response = financial_metrics_chain.run(text=chunk.page_content, page_number=page_number)\\n\",\n    \"        metrics = json.loads(metrics_response)\\n\",\n    \"        if metrics:\\n\",\n    \"            all_financial_metrics.extend(metrics)\\n\",\n    \"            print(f\\\"  Found {len(metrics)} financial metrics\\\")\\n\",\n    \"    except Exception as e:\\n\",\n    \"        print(f\\\"  Error extracting financial metrics: {e}\\\")\\n\",\n    \"    \\n\",\n    \"    # Extract risk factors\\n\",\n    \"    try:\\n\",\n    \"        risks_response = risk_factors_chain.run(text=chunk.page_content, page_number=page_number)\\n\",\n    \"        risks = json.loads(risks_response)\\n\",\n    \"        if risks:\\n\",\n    \"            all_risk_factors.extend(risks)\\n\",\n    \"            print(f\\\"  Found {len(risks)} risk factors\\\")\\n\",\n    \"    except Exception as e:\\n\",\n    \"        print(f\\\"  Error extracting risk factors: {e}\\\")\\n\",\n    \"    \\n\",\n    \"    # Extract business outlook\\n\",\n    \"    try:\\n\",\n    \"        outlook_response = business_outlook_chain.run(text=chunk.page_content, page_number=page_number)\\n\",\n    \"        outlook = json.loads(outlook_response)\\n\",\n    \"        if outlook and outlook.get(\\\"summary\\\") and outlook[\\\"summary\\\"] != \\\"null\\\":\\n\",\n    \"            all_business_outlook.append(outlook)\\n\",\n    \"            print(f\\\"  Found business outlook information\\\")\\n\",\n    \"    except Exception as e:\\n\",\n    \"        print(f\\\"  Error extracting business outlook: {e}\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nExtraction completed!\\\")\\n\",\n    \"print(f\\\"Total financial metrics found: {len(all_financial_metrics)}\\\")\\n\",\n    \"print(f\\\"Total risk factors found: {len(all_risk_factors)}\\\")\\n\",\n    \"print(f\\\"Total business outlook sections found: {len(all_business_outlook)}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"79PW4dFdWqc9\"\n   },\n   \"source\": [\n    \"## Synthesize Business Outlook\\n\",\n    \"\\n\",\n    \"If we found multiple business outlook sections, let's synthesize them into a single cohesive outlook:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"6Cr9TqkTWvGO\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"if all_business_outlook:\\n\",\n    \"    # Prepare the content for synthesis\\n\",\n    \"    outlook_content = json.dumps(all_business_outlook, indent=2)\\n\",\n    \"    \\n\",\n    \"    # Create a synthesis prompt\\n\",\n    \"    synthesis_template = \\\"\\\"\\\"\\n\",\n    \"    You are a business analyst synthesizing multiple outlook sections from a financial report.\\n\",\n    \"    \\n\",\n    \"    Below are several business outlook sections extracted from different parts of a financial report:\\n\",\n    \"    {outlook_content}\\n\",\n    \"    \\n\",\n    \"    Create a single, comprehensive business outlook that combines all this information.\\n\",\n    \"    Remove any duplicates and organize the information coherently.\\n\",\n    \"    \\n\",\n    \"    Provide the results as a JSON object with these fields:\\n\",\n    \"    - summary: Comprehensive summary of the company's future outlook (3-5 sentences)\\n\",\n    \"    - key_initiatives: List of key strategic initiatives or focus areas (strings), with duplicates removed\\n\",\n    \"    - market_trends: List of relevant market or industry trends mentioned (strings), with duplicates removed\\n\",\n    \"    \\n\",\n    \"    Format your response as valid JSON that can be parsed, with nothing else before or after.\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    synthesis_prompt = PromptTemplate(\\n\",\n    \"        template=synthesis_template,\\n\",\n    \"        input_variables=[\\\"outlook_content\\\"]\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    synthesis_chain = LLMChain(llm=llm, prompt=synthesis_prompt)\\n\",\n    \"    \\n\",\n    \"    # Run the synthesis\\n\",\n    \"    try:\\n\",\n    \"        synthesis_response = synthesis_chain.run(outlook_content=outlook_content)\\n\",\n    \"        synthesized_outlook = json.loads(synthesis_response)\\n\",\n    \"        print(\\\"Successfully synthesized business outlook information\\\")\\n\",\n    \"    except Exception as e:\\n\",\n    \"        print(f\\\"Error synthesizing business outlook: {e}\\\")\\n\",\n    \"        synthesized_outlook = all_business_outlook[0]  # Use the first one as fallback\\n\",\n    \"else:\\n\",\n    \"    # Create a default empty outlook if none found\\n\",\n    \"    synthesized_outlook = {\\n\",\n    \"        \\\"summary\\\": \\\"No clear business outlook information found in the analyzed sections.\\\",\\n\",\n    \"        \\\"key_initiatives\\\": [],\\n\",\n    \"        \\\"market_trends\\\": [],\\n\",\n    \"        \\\"page_number\\\": None\\n\",\n    \"    }\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"M5hYQzS3XCx-\"\n   },\n   \"source\": [\n    \"## Compile Final Results\\n\",\n    \"\\n\",\n    \"Now let's put everything together into our final structured output:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"ATKDE6N-XE7F\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Create our final result object\\n\",\n    \"final_extraction = FinancialReportExtraction(\\n\",\n    \"    company_name=company_info[\\\"company_name\\\"],\\n\",\n    \"    report_year=company_info[\\\"report_year\\\"],\\n\",\n    \"    financial_metrics=[FinancialMetric(**metric) for metric in all_financial_metrics],\\n\",\n    \"    risk_factors=[RiskFactor(**risk) for risk in all_risk_factors],\\n\",\n    \"    business_outlook=BusinessOutlook(**synthesized_outlook)\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"# Convert to dictionary for easier viewing\\n\",\n    \"extraction_dict = final_extraction.model_dump()\\n\",\n    \"\\n\",\n    \"# Print a preview of the results\\n\",\n    \"print(f\\\"Extraction Results for {extraction_dict['company_name']} ({extraction_dict['report_year']}):\\\\n\\\")\\n\",\n    \"print(f\\\"Found {len(extraction_dict['financial_metrics'])} financial metrics\\\")\\n\",\n    \"print(f\\\"Found {len(extraction_dict['risk_factors'])} risk factors\\\")\\n\",\n    \"print(\\\"Business Outlook Summary:\\\")\\n\",\n    \"print(textwrap.fill(extraction_dict['business_outlook']['summary'], width=100))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"4V_CeolYXQxu\"\n   },\n   \"source\": [\n    \"## Display Results in a Structured Format\\n\",\n    \"\\n\",\n    \"Let's create a nice HTML display of our results:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"U70PXAAPXT0t\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def create_html_report(extraction):\\n\",\n    \"    \\\"\\\"\\\"Create a nicely formatted HTML report from the extraction results\\\"\\\"\\\"\\n\",\n    \"    html = f\\\"\\\"\\\"\\n\",\n    \"    <h1>{extraction['company_name']} - {extraction['report_year']} Financial Report Analysis</h1>\\n\",\n    \"    \\n\",\n    \"    <h2>Business Outlook</h2>\\n\",\n    \"    <div style=\\\"background-color: #f5f5f5; padding: 15px; border-radius: 5px; margin-bottom: 20px;\\\">\\n\",\n    \"        <p><strong>Summary:</strong> {extraction['business_outlook']['summary']}</p>\\n\",\n    \"        \\n\",\n    \"        <p><strong>Key Strategic Initiatives:</strong></p>\\n\",\n    \"        <ul>\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    for initiative in extraction['business_outlook']['key_initiatives']:\\n\",\n    \"        html += f\\\"<li>{initiative}</li>\\\\n\\\"\\n\",\n    \"    \\n\",\n    \"    html += \\\"\\\"\\\"</ul>\\n\",\n    \"        <p><strong>Market Trends:</strong></p>\\n\",\n    \"        <ul>\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    for trend in extraction['business_outlook']['market_trends']:\\n\",\n    \"        html += f\\\"<li>{trend}</li>\\\\n\\\"\\n\",\n    \"    \\n\",\n    \"    html += \\\"\\\"\\\"</ul>\\n\",\n    \"    </div>\\n\",\n    \"    \\n\",\n    \"    <h2>Key Financial Metrics</h2>\\n\",\n    \"    <table style=\\\"width:100%; border-collapse: collapse; margin-bottom: 20px;\\\">\\n\",\n    \"        <tr style=\\\"background-color: #4CAF50; color: white;\\\">\\n\",\n    \"            <th style=\\\"padding: 8px; text-align: left;\\\">Metric</th>\\n\",\n    \"            <th style=\\\"padding: 8px; text-align: left;\\\">Value</th>\\n\",\n    \"            <th style=\\\"padding: 8px; text-align: left;\\\">Year/Period</th>\\n\",\n    \"            <th style=\\\"padding: 8px; text-align: left;\\\">Page</th>\\n\",\n    \"        </tr>\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    for i, metric in enumerate(extraction['financial_metrics']):\\n\",\n    \"        bg_color = \\\"#f2f2f2\\\" if i % 2 == 0 else \\\"white\\\"\\n\",\n    \"        html += f\\\"\\\"\\\"\\n\",\n    \"        <tr style=\\\"background-color: {bg_color};\\\">\\n\",\n    \"            <td style=\\\"padding: 8px; border: 1px solid #ddd;\\\">{metric['name']}</td>\\n\",\n    \"            <td style=\\\"padding: 8px; border: 1px solid #ddd;\\\">{metric['value']}</td>\\n\",\n    \"            <td style=\\\"padding: 8px; border: 1px solid #ddd;\\\">{metric['year']}</td>\\n\",\n    \"            <td style=\\\"padding: 8px; border: 1px solid #ddd;\\\">{metric['page_number']}</td>\\n\",\n    \"        </tr>\\n\",\n    \"        \\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    html += \\\"\\\"\\\"</table>\\n\",\n    \"    \\n\",\n    \"    <h2>Risk Factors</h2>\\n\",\n    \"    <div style=\\\"margin-bottom: 20px;\\\">\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    for i, risk in enumerate(extraction['risk_factors']):\\n\",\n    \"        html += f\\\"\\\"\\\"\\n\",\n    \"        <div style=\\\"margin-bottom: 15px; padding: 10px; border-left: 4px solid #ff9800; background-color: #fff3e0;\\\">\\n\",\n    \"            <h3>{risk['title']} (Page {risk['page_number']})</h3>\\n\",\n    \"            <p><strong>Description:</strong> {risk['description']}</p>\\n\",\n    \"            <p><strong>Potential Impact:</strong> {risk['potential_impact']}</p>\\n\",\n    \"        </div>\\n\",\n    \"        \\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    html += \\\"\\\"\\\"</div>\\n\",\n    \"    \\n\",\n    \"    <p style=\\\"font-style: italic; color: #666;\\\">Note: This analysis was performed on a subset of the document and may not reflect all information in the complete report.</p>\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    return html\\n\",\n    \"\\n\",\n    \"# Create and display the HTML report\\n\",\n    \"html_report = create_html_report(extraction_dict)\\n\",\n    \"display(HTML(html_report))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"a89Uw8DnXvlu\"\n   },\n   \"source\": [\n    \"## Save the Extracted Information\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"EWcnpCTiXxNt\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Save the extracted information as JSON\\n\",\n    \"with open(f\\\"{company_info['company_name'].replace(' ', '_')}_{company_info['report_year']}_extraction.json\\\", \\\"w\\\") as f:\\n\",\n    \"    json.dump(extraction_dict, f, indent=2)\\n\",\n    \"\\n\",\n    \"# Save the HTML report\\n\",\n    \"with open(f\\\"{company_info['company_name'].replace(' ', '_')}_{company_info['report_year']}_report.html\\\", \\\"w\\\") as f:\\n\",\n    \"    f.write(html_report)\\n\",\n    \"\\n\",\n    \"print(\\\"Files saved successfully!\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"xQOXJqYRYB5G\"\n   },\n   \"source\": [\n    \"## Conclusion\\n\",\n    \"\\n\",\n    \"In this notebook, we've demonstrated how to:\\n\",\n    \"\\n\",\n    \"1. Load and split a PDF document using LangChain's tools\\n\",\n    \"2. Create structured extraction chains with Gemini\\n\",\n    \"3. Extract specific types of information (financial metrics, risk factors, business outlook)\\n\",\n    \"4. Synthesize and structure the extracted information\\n\",\n    \"5. Present the results in a readable format\\n\",\n    \"\\n\",\n    \"This approach can be extended to extract different types of information from various documents by modifying the Pydantic models and extraction prompts.\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"colab\": {\n   \"provenance\": []\n  },\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"name\": \"python\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 0\n}\n\u0005End File\u0006# google/gemini-api-cookbook\n{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"Tce3stUlHN0L\"\n   },\n   \"source\": [\n    \"# Extracting Information from Text with Structured Output\\n\",\n    \"\\n\",\n    \"This notebook demonstrates how to extract structured information from text using Google's Gemini API. We'll explore different approaches to get information in formats like JSON that can be easily used in applications.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"qWMBIlbCnN6p\"\n   },\n   \"source\": [\n    \"## Setup\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"1K0JGrB1nPp2\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!pip install -q -U google-generativeai\\n\",\n    \"\\n\",\n    \"import os\\n\",\n    \"import json\\n\",\n    \"import google.generativeai as genai\\n\",\n    \"from IPython.display import display, Markdown, HTML\\n\",\n    \"import time\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"cellView\": \"form\",\n    \"id\": \"gZRxxgp5nTWy\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"#@title Setup the API\\n\",\n    \"API_KEY = \\\"\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"\\n\",\n    \"# Configure the API key\\n\",\n    \"if API_KEY:\\n\",\n    \"    genai.configure(api_key=API_KEY)\\n\",\n    \"elif \\\"GOOGLE_API_KEY\\\" in os.environ:\\n\",\n    \"    genai.configure(api_key=os.environ[\\\"GOOGLE_API_KEY\\\"])\\n\",\n    \"else:\\n\",\n    \"    raise ValueError(\\\"Please provide an API key\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"VBrqmOLGnWXC\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Initialize the model\\n\",\n    \"model = genai.GenerativeModel('gemini-1.5-pro')\\n\",\n    \"print(f\\\"Using model: {model.model_name}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"EFCCszE-naLB\"\n   },\n   \"source\": [\n    \"## Example Text Data\\n\",\n    \"\\n\",\n    \"Let's use some example text to extract information from.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"jy-XYHJQncrC\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Example 1: Product Reviews\\n\",\n    \"product_reviews = \\\"\\\"\\\"\\n\",\n    \"Review #1: The XPS 13 laptop has exceeded my expectations. The display is stunning with vibrant colors and the battery lasts about 9 hours of continuous use. The keyboard feels comfortable for long typing sessions, though the trackpad can be a bit finicky at times. Overall, I'd rate it 4.5/5 stars.\\n\",\n    \"\\n\",\n    \"Review #2: I've had this laptop for 3 months now and I'm disappointed. The fan is constantly running loud even with basic tasks, and I've experienced frequent software glitches. The screen quality is excellent, I'll give it that. Customer service was unhelpful when I reached out. Would not recommend - 2/5 stars.\\n\",\n    \"\\n\",\n    \"Review #3: Decent machine for the price point. The processor handles my development work smoothly, and 16GB RAM is sufficient for my needs. The build quality feels premium with the aluminum chassis. Battery life is average - about 6 hours of real-world use. I'd give it 3.8/5 stars.\\n\",\n    \"\\\"\\\"\\\"\\n\",\n    \"\\n\",\n    \"# Example 2: Event Announcement\\n\",\n    \"event_announcement = \\\"\\\"\\\"\\n\",\n    \"ANNUAL TECH CONFERENCE 2023\\n\",\n    \"\\n\",\n    \"Join us for the biggest tech event of the year! The Annual Tech Conference will be held from September 15-17, 2023, at the Grand Convention Center in San Francisco, CA.\\n\",\n    \"\\n\",\n    \"Featured Speakers:\\n\",\n    \"- Dr. Sarah Johnson, AI Research Director at Google\\n\",\n    \"- Mark Williams, CEO of TechFuture Inc.\\n\",\n    \"- Prof. Lisa Chen, Stanford University, Quantum Computing Department\\n\",\n    \"\\n\",\n    \"Schedule Highlights:\\n\",\n    \"- Day 1 (Sept 15): Opening Keynote (9 AM - 10:30 AM), Workshop Sessions (11 AM - 5 PM)\\n\",\n    \"- Day 2 (Sept 16): Panel Discussions (10 AM - 12 PM), Networking Lunch (12:30 PM - 2 PM), Tech Demos (2:30 PM - 6 PM)\\n\",\n    \"- Day 3 (Sept 17): Startup Showcase (9 AM - 12 PM), Closing Address (3 PM - 4:30 PM)\\n\",\n    \"\\n\",\n    \"Early bird tickets available until August 1st: $299 (Regular price: $499)\\n\",\n    \"Student discount: 50% off with valid ID\\n\",\n    \"\\n\",\n    \"For registration and more information, visit www.annualtechconf.com or contact events@techconf.org\\n\",\n    \"\\\"\\\"\\\"\\n\",\n    \"\\n\",\n    \"# Example 3: Job Posting\\n\",\n    \"job_posting = \\\"\\\"\\\"\\n\",\n    \"SENIOR SOFTWARE ENGINEER - MACHINE LEARNING\\n\",\n    \"Location: Remote (US-based) or Hybrid (New York office)\\n\",\n    \"Company: InnovateAI Solutions Inc.\\n\",\n    \"Posted: June 12, 2023\\n\",\n    \"\\n\",\n    \"About the Role:\\n\",\n    \"We are seeking an experienced Software Engineer with machine learning expertise to join our growing team. The ideal candidate will develop and implement ML models and algorithms to enhance our flagship product suite.\\n\",\n    \"\\n\",\n    \"Key Responsibilities:\\n\",\n    \"- Design and develop scalable ML solutions for production environments\\n\",\n    \"- Collaborate with data scientists to implement and optimize algorithms\\n\",\n    \"- Maintain and improve existing ML infrastructure\\n\",\n    \"- Participate in code reviews and mentoring junior engineers\\n\",\n    \"\\n\",\n    \"Requirements:\\n\",\n    \"- Bachelor's degree in Computer Science or related field (Master's preferred)\\n\",\n    \"- 5+ years of software engineering experience\\n\",\n    \"- 3+ years working with machine learning technologies\\n\",\n    \"- Proficiency in Python and PyTorch or TensorFlow\\n\",\n    \"- Experience with cloud platforms (AWS or GCP)\\n\",\n    \"\\n\",\n    \"Compensation:\\n\",\n    \"- Salary range: $140,000 - $180,000 depending on experience\\n\",\n    \"- Comprehensive benefits package including health, dental, and 401(k) matching\\n\",\n    \"- Stock options and annual performance bonus\\n\",\n    \"- 20 days PTO + 10 paid holidays\\n\",\n    \"\\n\",\n    \"To apply, send your resume to careers@innovateai.com with the subject line \\\"Senior ML Engineer Application\\\"\\n\",\n    \"InnovateAI is an equal opportunity employer.\\n\",\n    \"\\\"\\\"\\\"\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"ZQ2RRIYmn5TB\"\n   },\n   \"source\": [\n    \"## Approach 1: Basic Structured Output Request\\n\",\n    \"\\n\",\n    \"Let's start with a basic approach to request structured information.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"GXvwU13_n9UB\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def extract_info_basic(text, instructions):\\n\",\n    \"    \\\"\\\"\\\"Basic approach to extract structured information.\\\"\\\"\\\"\\n\",\n    \"    prompt = f\\\"\\\"\\\"\\n\",\n    \"{instructions}\\n\",\n    \"\\n\",\n    \"TEXT TO ANALYZE:\\n\",\n    \"{text}\\n\",\n    \"\\n\",\n    \"Provide the output in valid JSON format only. No additional text or explanations.\\n\",\n    \"\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    response = model.generate_content(prompt)\\n\",\n    \"    return response.text\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"1MK0KrpMoAJZ\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Extract product review information\\n\",\n    \"review_instructions = \\\"\\\"\\\"Extract the following information from each product review:\\n\",\n    \"- Review number\\n\",\n    \"- Overall sentiment (positive, negative, or neutral)\\n\",\n    \"- Rating (numerical value)\\n\",\n    \"- Pros mentioned\\n\",\n    \"- Cons mentioned\\n\",\n    \"- Usage duration (if mentioned)\\n\",\n    \"\\n\",\n    \"Structure the data as a JSON array of review objects.\\\"\\\"\\\"\\n\",\n    \"\\n\",\n    \"review_result = extract_info_basic(product_reviews, review_instructions)\\n\",\n    \"display(Markdown(f\\\"```json\\\\n{review_result}\\\\n```\\\"))\\n\",\n    \"\\n\",\n    \"# Try to parse the JSON\\n\",\n    \"try:\\n\",\n    \"    parsed_reviews = json.loads(review_result)\\n\",\n    \"    print(\\\"Successfully parsed JSON!\\\")\\n\",\n    \"except json.JSONDecodeError as e:\\n\",\n    \"    print(f\\\"Error parsing JSON: {e}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"N1NJGfpRoEHQ\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Extract event information\\n\",\n    \"event_instructions = \\\"\\\"\\\"Extract the following information from this event announcement:\\n\",\n    \"- Event name\\n\",\n    \"- Dates\\n\",\n    \"- Location\\n\",\n    \"- Speakers (with their titles/organizations)\\n\",\n    \"- Schedule for each day\\n\",\n    \"- Ticket prices\\n\",\n    \"- Contact information\\n\",\n    \"\\n\",\n    \"Structure the data as a single JSON object with appropriate fields.\\\"\\\"\\\"\\n\",\n    \"\\n\",\n    \"event_result = extract_info_basic(event_announcement, event_instructions)\\n\",\n    \"display(Markdown(f\\\"```json\\\\n{event_result}\\\\n```\\\"))\\n\",\n    \"\\n\",\n    \"# Try to parse the JSON\\n\",\n    \"try:\\n\",\n    \"    parsed_event = json.loads(event_result)\\n\",\n    \"    print(\\\"Successfully parsed JSON!\\\")\\n\",\n    \"except json.JSONDecodeError as e:\\n\",\n    \"    print(f\\\"Error parsing JSON: {e}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"bMp6xeXHoqiS\"\n   },\n   \"source\": [\n    \"## Approach 2: Schema-Based Extraction\\n\",\n    \"\\n\",\n    \"Let's try a more explicit approach by providing a specific schema for the output.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"FKzB53BlovBR\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def extract_info_with_schema(text, instructions, schema):\\n\",\n    \"    \\\"\\\"\\\"Extract information with a specific output schema.\\\"\\\"\\\"\\n\",\n    \"    prompt = f\\\"\\\"\\\"\\n\",\n    \"{instructions}\\n\",\n    \"\\n\",\n    \"TEXT TO ANALYZE:\\n\",\n    \"{text}\\n\",\n    \"\\n\",\n    \"Use the following JSON schema for your response:\\n\",\n    \"{json.dumps(schema, indent=2)}\\n\",\n    \"\\n\",\n    \"Your response must be valid JSON that conforms to this schema. No additional text or explanations.\\n\",\n    \"\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    response = model.generate_content(prompt)\\n\",\n    \"    return response.text\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"G9eVNEf3pzrC\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Extract job posting information with a schema\\n\",\n    \"job_instructions = \\\"Extract detailed information from this job posting.\\\"\\n\",\n    \"\\n\",\n    \"job_schema = {\\n\",\n    \"    \\\"title\\\": \\\"string\\\",\\n\",\n    \"    \\\"company\\\": \\\"string\\\",\\n\",\n    \"    \\\"location\\\": {\\n\",\n    \"        \\\"type\\\": \\\"string\\\",\\n\",\n    \"        \\\"options\\\": [\\\"string\\\"]\\n\",\n    \"    },\\n\",\n    \"    \\\"posted_date\\\": \\\"string\\\",\\n\",\n    \"    \\\"responsibilities\\\": [\\\"string\\\"],\\n\",\n    \"    \\\"requirements\\\": {\\n\",\n    \"        \\\"education\\\": \\\"string\\\",\\n\",\n    \"        \\\"experience\\\": [\\\"string\\\"],\\n\",\n    \"        \\\"skills\\\": [\\\"string\\\"]\\n\",\n    \"    },\\n\",\n    \"    \\\"compensation\\\": {\\n\",\n    \"        \\\"salary_range\\\": {\\n\",\n    \"            \\\"min\\\": \\\"number\\\",\\n\",\n    \"            \\\"max\\\": \\\"number\\\",\\n\",\n    \"            \\\"currency\\\": \\\"string\\\"\\n\",\n    \"        },\\n\",\n    \"        \\\"benefits\\\": [\\\"string\\\"],\\n\",\n    \"        \\\"time_off\\\": \\\"string\\\"\\n\",\n    \"    },\\n\",\n    \"    \\\"application_process\\\": \\\"string\\\",\\n\",\n    \"    \\\"equal_opportunity_statement\\\": \\\"boolean\\\"\\n\",\n    \"}\\n\",\n    \"\\n\",\n    \"job_result = extract_info_with_schema(job_posting, job_instructions, job_schema)\\n\",\n    \"display(Markdown(f\\\"```json\\\\n{job_result}\\\\n```\\\"))\\n\",\n    \"\\n\",\n    \"# Try to parse the JSON\\n\",\n    \"try:\\n\",\n    \"    parsed_job = json.loads(job_result)\\n\",\n    \"    print(\\\"Successfully parsed JSON!\\\")\\n\",\n    \"except json.JSONDecodeError as e:\\n\",\n    \"    print(f\\\"Error parsing JSON: {e}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"Z9JjM0BRp5yy\"\n   },\n   \"source\": [\n    \"## Approach 3: Using Function Calling\\n\",\n    \"\\n\",\n    \"Let's use the function calling capabilities to extract structured information.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"9vfJWm8ap9IC\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def extract_with_function_calling(text, system_instruction, function_schema):\\n\",\n    \"    \\\"\\\"\\\"Extract information using function calling.\\\"\\\"\\\"\\n\",\n    \"    chat = model.start_chat()\\n\",\n    \"    \\n\",\n    \"    response = chat.send_message(\\n\",\n    \"        system_instruction + \\\"\\\\n\\\\n\\\" + text,\\n\",\n    \"        tools=[{\\n\",\n    \"            \\\"function_declarations\\\": [function_schema]\\n\",\n    \"        }]\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    function_response = None\\n\",\n    \"    if hasattr(response, 'candidates') and response.candidates:\\n\",\n    \"        for candidate in response.candidates:\\n\",\n    \"            if hasattr(candidate, 'content') and candidate.content:\\n\",\n    \"                for part in candidate.content.parts:\\n\",\n    \"                    if hasattr(part, 'function_call'):\\n\",\n    \"                        function_response = part.function_call\\n\",\n    \"                        break\\n\",\n    \"    \\n\",\n    \"    if function_response:\\n\",\n    \"        return {\\n\",\n    \"            \\\"name\\\": function_response.name,\\n\",\n    \"            \\\"args\\\": json.loads(function_response.args)\\n\",\n    \"        }\\n\",\n    \"    else:\\n\",\n    \"        return {\\\"error\\\": \\\"No function call found in response\\\"}\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"wg5KiqRKqBNx\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Define a function schema for the product reviews\\n\",\n    \"review_function_schema = {\\n\",\n    \"    \\\"name\\\": \\\"extract_product_reviews\\\",\\n\",\n    \"    \\\"description\\\": \\\"Extract structured information from product reviews\\\",\\n\",\n    \"    \\\"parameters\\\": {\\n\",\n    \"        \\\"type\\\": \\\"object\\\",\\n\",\n    \"        \\\"properties\\\": {\\n\",\n    \"            \\\"reviews\\\": {\\n\",\n    \"                \\\"type\\\": \\\"array\\\",\\n\",\n    \"                \\\"description\\\": \\\"List of extracted reviews\\\",\\n\",\n    \"                \\\"items\\\": {\\n\",\n    \"                    \\\"type\\\": \\\"object\\\",\\n\",\n    \"                    \\\"properties\\\": {\\n\",\n    \"                        \\\"review_number\\\": {\\\"type\\\": \\\"integer\\\"},\\n\",\n    \"                        \\\"sentiment\\\": {\\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"positive\\\", \\\"negative\\\", \\\"neutral\\\"]},\\n\",\n    \"                        \\\"rating\\\": {\\\"type\\\": \\\"number\\\"},\\n\",\n    \"                        \\\"pros\\\": {\\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}},\\n\",\n    \"                        \\\"cons\\\": {\\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}},\\n\",\n    \"                        \\\"usage_duration\\\": {\\\"type\\\": \\\"string\\\"},\\n\",\n    \"                    },\\n\",\n    \"                    \\\"required\\\": [\\\"review_number\\\", \\\"sentiment\\\", \\\"rating\\\", \\\"pros\\\", \\\"cons\\\"]\\n\",\n    \"                }\\n\",\n    \"            },\\n\",\n    \"            \\\"product_type\\\": {\\\"type\\\": \\\"string\\\"},\\n\",\n    \"            \\\"average_rating\\\": {\\\"type\\\": \\\"number\\\"}\\n\",\n    \"        },\\n\",\n    \"        \\\"required\\\": [\\\"reviews\\\"]\\n\",\n    \"    }\\n\",\n    \"}\\n\",\n    \"\\n\",\n    \"# Extract product review information using function calling\\n\",\n    \"review_system_instruction = \\\"You are a data extraction assistant. Extract detailed information from these product reviews.\\\"\\n\",\n    \"\\n\",\n    \"review_function_result = extract_with_function_calling(\\n\",\n    \"    product_reviews, \\n\",\n    \"    review_system_instruction, \\n\",\n    \"    review_function_schema\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"display(Markdown(f\\\"```json\\\\n{json.dumps(review_function_result, indent=2)}\\\\n```\\\"))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"WgVYVw3-qGHR\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Define a function schema for the event announcement\\n\",\n    \"event_function_schema = {\\n\",\n    \"    \\\"name\\\": \\\"extract_event_details\\\",\\n\",\n    \"    \\\"description\\\": \\\"Extract structured information from an event announcement\\\",\\n\",\n    \"    \\\"parameters\\\": {\\n\",\n    \"        \\\"type\\\": \\\"object\\\",\\n\",\n    \"        \\\"properties\\\": {\\n\",\n    \"            \\\"event_name\\\": {\\\"type\\\": \\\"string\\\"},\\n\",\n    \"            \\\"dates\\\": {\\n\",\n    \"                \\\"type\\\": \\\"object\\\",\\n\",\n    \"                \\\"properties\\\": {\\n\",\n    \"                    \\\"start_date\\\": {\\\"type\\\": \\\"string\\\"},\\n\",\n    \"                    \\\"end_date\\\": {\\\"type\\\": \\\"string\\\"}\\n\",\n    \"                }\\n\",\n    \"            },\\n\",\n    \"            \\\"location\\\": {\\\"type\\\": \\\"string\\\"},\\n\",\n    \"            \\\"speakers\\\": {\\n\",\n    \"                \\\"type\\\": \\\"array\\\",\\n\",\n    \"                \\\"items\\\": {\\n\",\n    \"                    \\\"type\\\": \\\"object\\\",\\n\",\n    \"                    \\\"properties\\\": {\\n\",\n    \"                        \\\"name\\\": {\\\"type\\\": \\\"string\\\"},\\n\",\n    \"                        \\\"title\\\": {\\\"type\\\": \\\"string\\\"},\\n\",\n    \"                        \\\"organization\\\": {\\\"type\\\": \\\"string\\\"}\\n\",\n    \"                    }\\n\",\n    \"                }\\n\",\n    \"            },\\n\",\n    \"            \\\"schedule\\\": {\\n\",\n    \"                \\\"type\\\": \\\"array\\\",\\n\",\n    \"                \\\"items\\\": {\\n\",\n    \"                    \\\"type\\\": \\\"object\\\",\\n\",\n    \"                    \\\"properties\\\": {\\n\",\n    \"                        \\\"day\\\": {\\\"type\\\": \\\"string\\\"},\\n\",\n    \"                        \\\"date\\\": {\\\"type\\\": \\\"string\\\"},\\n\",\n    \"                        \\\"activities\\\": {\\n\",\n    \"                            \\\"type\\\": \\\"array\\\",\\n\",\n    \"                            \\\"items\\\": {\\n\",\n    \"                                \\\"type\\\": \\\"object\\\",\\n\",\n    \"                                \\\"properties\\\": {\\n\",\n    \"                                    \\\"name\\\": {\\\"type\\\": \\\"string\\\"},\\n\",\n    \"                                    \\\"time\\\": {\\\"type\\\": \\\"string\\\"}\\n\",\n    \"                                }\\n\",\n    \"                            }\\n\",\n    \"                        }\\n\",\n    \"                    }\\n\",\n    \"                }\\n\",\n    \"            },\\n\",\n    \"            \\\"pricing\\\": {\\n\",\n    \"                \\\"type\\\": \\\"object\\\",\\n\",\n    \"                \\\"properties\\\": {\\n\",\n    \"                    \\\"early_bird\\\": {\\\"type\\\": \\\"number\\\"},\\n\",\n    \"                    \\\"regular\\\": {\\\"type\\\": \\\"number\\\"},\\n\",\n    \"                    \\\"special_discounts\\\": {\\n\",\n    \"                        \\\"type\\\": \\\"array\\\",\\n\",\n    \"                        \\\"items\\\": {\\n\",\n    \"                            \\\"type\\\": \\\"object\\\",\\n\",\n    \"                            \\\"properties\\\": {\\n\",\n    \"                                \\\"type\\\": {\\\"type\\\": \\\"string\\\"},\\n\",\n    \"                                \\\"discount\\\": {\\\"type\\\": \\\"string\\\"}\\n\",\n    \"                            }\\n\",\n    \"                        }\\n\",\n    \"                    }\\n\",\n    \"                }\\n\",\n    \"            },\\n\",\n    \"            \\\"contact\\\": {\\n\",\n    \"                \\\"type\\\": \\\"object\\\",\\n\",\n    \"                \\\"properties\\\": {\\n\",\n    \"                    \\\"website\\\": {\\\"type\\\": \\\"string\\\"},\\n\",\n    \"                    \\\"email\\\": {\\\"type\\\": \\\"string\\\"}\\n\",\n    \"                }\\n\",\n    \"            }\\n\",\n    \"        },\\n\",\n    \"        \\\"required\\\": [\\\"event_name\\\", \\\"dates\\\", \\\"location\\\"]\\n\",\n    \"    }\\n\",\n    \"}\\n\",\n    \"\\n\",\n    \"# Extract event information using function calling\\n\",\n    \"event_system_instruction = \\\"You are a data extraction assistant. Extract detailed information from this event announcement.\\\"\\n\",\n    \"\\n\",\n    \"event_function_result = extract_with_function_calling(\\n\",\n    \"    event_announcement, \\n\",\n    \"    event_system_instruction, \\n\",\n    \"    event_function_schema\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"display(Markdown(f\\\"```json\\\\n{json.dumps(event_function_result, indent=2)}\\\\n```\\\"))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"j5wRJSMUqL_5\"\n   },\n   \"source\": [\n    \"## Approach 4: Creating a Multi-Stage Pipeline\\n\",\n    \"\\n\",\n    \"Let's create a more robust multi-stage pipeline for complex extraction tasks.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"4_pEHZexqQ85\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def multi_stage_extraction(text, initial_instruction, schemas, stage_instructions=None):\\n\",\n    \"    \\\"\\\"\\\"Multi-stage extraction pipeline.\\n\",\n    \"    \\n\",\n    \"    Args:\\n\",\n    \"        text: Text to analyze\\n\",\n    \"        initial_instruction: Initial instruction for text analysis\\n\",\n    \"        schemas: Dictionary of schemas for each extraction stage\\n\",\n    \"        stage_instructions: Optional dictionary of specific instructions for each stage\\n\",\n    \"    \\n\",\n    \"    Returns:\\n\",\n    \"        Dictionary with extraction results for each stage\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    if stage_instructions is None:\\n\",\n    \"        stage_instructions = {}\\n\",\n    \"    \\n\",\n    \"    # First stage: Initial analysis\\n\",\n    \"    prompt = f\\\"\\\"\\\"\\n\",\n    \"{initial_instruction}\\n\",\n    \"\\n\",\n    \"TEXT TO ANALYZE:\\n\",\n    \"{text}\\n\",\n    \"\\n\",\n    \"Provide a high-level summary as JSON with these fields:\\n\",\n    \"- document_type: The type of document this appears to be\\n\",\n    \"- main_entities: List of primary entities (people, organizations, products) mentioned\\n\",\n    \"- key_topics: List of main topics or themes\\n\",\n    \"- suggested_extraction_fields: List of fields that would be valuable to extract\\n\",\n    \"\\n\",\n    \"Respond with JSON only.\\n\",\n    \"\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    start_time = time.time()\\n\",\n    \"    initial_response = model.generate_content(prompt)\\n\",\n    \"    initial_time = time.time() - start_time\\n\",\n    \"    \\n\",\n    \"    try:\\n\",\n    \"        initial_analysis = json.loads(initial_response.text)\\n\",\n    \"    except json.JSONDecodeError:\\n\",\n    \"        # Fallback if response isn't valid JSON\\n\",\n    \"        initial_analysis = {\\n\",\n    \"            \\\"document_type\\\": \\\"unknown\\\",\\n\",\n    \"            \\\"main_entities\\\": [],\\n\",\n    \"            \\\"key_topics\\\": [],\\n\",\n    \"            \\\"suggested_extraction_fields\\\": []\\n\",\n    \"        }\\n\",\n    \"    \\n\",\n    \"    # Second stage: Detailed extraction for each schema\\n\",\n    \"    results = {\\\"initial_analysis\\\": initial_analysis}\\n\",\n    \"    timings = {\\\"initial_analysis\\\": initial_time}\\n\",\n    \"    \\n\",\n    \"    for stage_name, schema in schemas.items():\\n\",\n    \"        instruction = stage_instructions.get(stage_name, f\\\"Extract {stage_name} information from the text.\\\")\\n\",\n    \"        \\n\",\n    \"        prompt = f\\\"\\\"\\\"\\n\",\n    \"{instruction}\\n\",\n    \"\\n\",\n    \"TEXT TO ANALYZE:\\n\",\n    \"{text}\\n\",\n    \"\\n\",\n    \"Use the following JSON schema for your response:\\n\",\n    \"{json.dumps(schema, indent=2)}\\n\",\n    \"\\n\",\n    \"Your response must be valid JSON that conforms to this schema. No additional text.\\n\",\n    \"\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        start_time = time.time()\\n\",\n    \"        response = model.generate_content(prompt)\\n\",\n    \"        stage_time = time.time() - start_time\\n\",\n    \"        \\n\",\n    \"        try:\\n\",\n    \"            results[stage_name] = json.loads(response.text)\\n\",\n    \"        except json.JSONDecodeError as e:\\n\",\n    \"            results[stage_name] = {\\\"error\\\": f\\\"Failed to parse JSON: {str(e)}\\\", \\\"raw_response\\\": response.text}\\n\",\n    \"        \\n\",\n    \"        timings[stage_name] = stage_time\\n\",\n    \"    \\n\",\n    \"    results[\\\"_timings\\\"] = timings\\n\",\n    \"    return results\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"b2nYPkzFqTwC\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Define schemas for job posting analysis\\n\",\n    \"job_schemas = {\\n\",\n    \"    \\\"company_info\\\": {\\n\",\n    \"        \\\"company_name\\\": \\\"string\\\",\\n\",\n    \"        \\\"industry\\\": \\\"string\\\",\\n\",\n    \"        \\\"company_description\\\": \\\"string\\\",\\n\",\n    \"        \\\"equal_opportunity_statement\\\": \\\"boolean\\\"\\n\",\n    \"    },\\n\",\n    \"    \\\"job_details\\\": {\\n\",\n    \"        \\\"title\\\": \\\"string\\\",\\n\",\n    \"        \\\"location_type\\\": \\\"string\\\",\\n\",\n    \"        \\\"locations\\\": [\\\"string\\\"],\\n\",\n    \"        \\\"posting_date\\\": \\\"string\\\",\\n\",\n    \"        \\\"department\\\": \\\"string\\\"\\n\",\n    \"    },\\n\",\n    \"    \\\"requirements\\\": {\\n\",\n    \"        \\\"education\\\": {\\n\",\n    \"            \\\"minimum\\\": \\\"string\\\",\\n\",\n    \"            \\\"preferred\\\": \\\"string\\\"\\n\",\n    \"        },\\n\",\n    \"        \\\"experience\\\": {\\n\",\n    \"            \\\"years\\\": \\\"string\\\",\\n\",\n    \"            \\\"specific_requirements\\\": [\\\"string\\\"]\\n\",\n    \"        },\\n\",\n    \"        \\\"skills\\\": {\\n\",\n    \"            \\\"technical\\\": [\\\"string\\\"],\\n\",\n    \"            \\\"soft\\\": [\\\"string\\\"]\\n\",\n    \"        },\\n\",\n    \"        \\\"certifications\\\": [\\\"string\\\"]\\n\",\n    \"    },\\n\",\n    \"    \\\"compensation\\\": {\\n\",\n    \"        \\\"salary\\\": {\\n\",\n    \"            \\\"min\\\": \\\"number\\\",\\n\",\n    \"            \\\"max\\\": \\\"number\\\",\\n\",\n    \"            \\\"currency\\\": \\\"string\\\",\\n\",\n    \"            \\\"period\\\": \\\"string\\\"\\n\",\n    \"        },\\n\",\n    \"        \\\"benefits\\\": [\\\"string\\\"],\\n\",\n    \"        \\\"bonus\\\": \\\"string\\\",\\n\",\n    \"        \\\"stock_options\\\": \\\"boolean\\\",\\n\",\n    \"        \\\"time_off\\\": {\\n\",\n    \"            \\\"vacation\\\": \\\"string\\\",\\n\",\n    \"            \\\"holidays\\\": \\\"string\\\"\\n\",\n    \"        }\\n\",\n    \"    },\\n\",\n    \"    \\\"application_process\\\": {\\n\",\n    \"        \\\"how_to_apply\\\": \\\"string\\\",\\n\",\n    \"        \\\"contact_email\\\": \\\"string\\\",\\n\",\n    \"        \\\"required_documents\\\": [\\\"string\\\"],\\n\",\n    \"        \\\"application_deadline\\\": \\\"string\\\"\\n\",\n    \"    }\\n\",\n    \"}\\n\",\n    \"\\n\",\n    \"# Define specific instructions for each stage\\n\",\n    \"job_stage_instructions = {\\n\",\n    \"    \\\"company_info\\\": \\\"Extract information about the company posting this job.\\\",\\n\",\n    \"    \\\"job_details\\\": \\\"Extract basic details about the job position itself.\\\",\\n\",\n    \"    \\\"requirements\\\": \\\"Extract all requirements and qualifications needed for this position.\\\",\\n\",\n    \"    \\\"compensation\\\": \\\"Extract all information related to compensation, benefits, and perks.\\\",\\n\",\n    \"    \\\"application_process\\\": \\\"Extract information about how to apply for this position.\\\"\\n\",\n    \"}\\n\",\n    \"\\n\",\n    \"# Run the multi-stage extraction\\n\",\n    \"job_extraction_results = multi_stage_extraction(\\n\",\n    \"    job_posting,\\n\",\n    \"    \\\"Analyze this job posting to extract structured information.\\\",\\n\",\n    \"    job_schemas,\\n\",\n    \"    job_stage_instructions\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"# Display the timing information\\n\",\n    \"timings = job_extraction_results.pop(\\\"_timings\\\")\\n\",\n    \"print(\\\"Extraction timings:\\\")\\n\",\n    \"for stage, time_taken in timings.items():\\n\",\n    \"    print(f\\\"  {stage}: {time_taken:.2f} seconds\\\")\\n\",\n    \"\\n\",\n    \"# Display the results\\n\",\n    \"display(Markdown(f\\\"```json\\\\n{json.dumps(job_extraction_results, indent=2)}\\\\n```\\\"))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"Dj4eUWBDqzWR\"\n   },\n   \"source\": [\n    \"## Visualizing Extraction Results\\n\",\n    \"\\n\",\n    \"Let's create a simple function to visualize the extracted structured data.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"9QzrYwzEq3aR\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def create_html_visualization(extraction_results):\\n\",\n    \"    \\\"\\\"\\\"Create an HTML visualization of extraction results.\\\"\\\"\\\"\\n\",\n    \"    html = \\\"\\\"\\\"\\n\",\n    \"    <style>\\n\",\n    \"    .extraction-container {\\n\",\n    \"        font-family: Arial, sans-serif;\\n\",\n    \"        max-width: 1200px;\\n\",\n    \"        margin: 0 auto;\\n\",\n    \"    }\\n\",\n    \"    .section {\\n\",\n    \"        margin-bottom: 20px;\\n\",\n    \"        border: 1px solid #ddd;\\n\",\n    \"        border-radius: 5px;\\n\",\n    \"        padding: 15px;\\n\",\n    \"        background-color: #f9f9f9;\\n\",\n    \"    }\\n\",\n    \"    .section h2 {\\n\",\n    \"        margin-top: 0;\\n\",\n    \"        color: #333;\\n\",\n    \"        border-bottom: 1px solid #ddd;\\n\",\n    \"        padding-bottom: 8px;\\n\",\n    \"    }\\n\",\n    \"    .field {\\n\",\n    \"        margin-bottom: 10px;\\n\",\n    \"    }\\n\",\n    \"    .field-name {\\n\",\n    \"        font-weight: bold;\\n\",\n    \"        color: #555;\\n\",\n    \"    }\\n\",\n    \"    .field-value {\\n\",\n    \"        margin-left: 10px;\\n\",\n    \"    }\\n\",\n    \"    .list-value {\\n\",\n    \"        margin: 0;\\n\",\n    \"        padding-left: 30px;\\n\",\n    \"    }\\n\",\n    \"    .nested-object {\\n\",\n    \"        margin-left: 20px;\\n\",\n    \"        border-left: 2px solid #ddd;\\n\",\n    \"        padding-left: 10px;\\n\",\n    \"    }\\n\",\n    \"    </style>\\n\",\n    \"    \\n\",\n    \"    <div class=\\\"extraction-container\\\">\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def render_value(value, indent=0):\\n\",\n    \"        if isinstance(value, dict):\\n\",\n    \"            result = f'<div class=\\\"nested-object\\\">'\\n\",\n    \"            for k, v in value.items():\\n\",\n    \"                field_name = k.replace('_', ' ').title()\\n\",\n    \"                result += f'<div class=\\\"field\\\"><span class=\\\"field-name\\\">{field_name}:</span> {render_value(v, indent+1)}</div>'\\n\",\n    \"            result += '</div>'\\n\",\n    \"            return result\\n\",\n    \"        elif isinstance(value, list):\\n\",\n    \"            if not value:\\n\",\n    \"                return \\\"<span class='field-value'>None</span>\\\"\\n\",\n    \"            result = '<ul class=\\\"list-value\\\">'\\n\",\n    \"            for item in value:\\n\",\n    \"                result += f'<li>{render_value(item, indent+1)}</li>'\\n\",\n    \"            result += '</ul>'\\n\",\n    \"            return result\\n\",\n    \"        elif isinstance(value, bool):\\n\",\n    \"            return f\\\"<span class='field-value'>{'Yes' if value else 'No'}</span>\\\"\\n\",\n    \"        elif value is None:\\n\",\n    \"            return \\\"<span class='field-value'>Not specified</span>\\\"\\n\",\n    \"        else:\\n\",\n    \"            return f\\\"<span class='field-value'>{value}</span>\\\"\\n\",\n    \"    \\n\",\n    \"    # Add each section\\n\",\n    \"    for section_name, section_data in extraction_results.items():\\n\",\n    \"        if section_name.startswith('_'):\\n\",\n    \"            continue  # Skip metadata fields\\n\",\n    \"            \\n\",\n    \"        # Format section name\\n\",\n    \"        display_name = section_name.replace('_', ' ').title()\\n\",\n    \"        \\n\",\n    \"        html += f'<div class=\\\"section\\\"><h2>{display_name}</h2>'\\n\",\n    \"        \\n\",\n    \"        # Handle section content\\n\",\n    \"        if isinstance(section_data, dict):\\n\",\n    \"            for key, value in section_data.items():\\n\",\n    \"                field_name = key.replace('_', ' ').title()\\n\",\n    \"                html += f'<div class=\\\"field\\\"><span class=\\\"field-name\\\">{field_name}:</span> {render_value(value)}</div>'\\n\",\n    \"        elif isinstance(section_data, list):\\n\",\n    \"            html += '<ul class=\\\"list-value\\\">'\\n\",\n    \"            for item in section_data:\\n\",\n    \"                html += f'<li>{render_value(item)}</li>'\\n\",\n    \"            html += '</ul>'\\n\",\n    \"        else:\\n\",\n    \"            html += f'<div class=\\\"field\\\">{section_data}</div>'\\n\",\n    \"            \\n\",\n    \"        html += '</div>'\\n\",\n    \"    \\n\",\n    \"    html += '</div>'\\n\",\n    \"    return html\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"jwJPV4xGq6Wy\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Create and display visualization for job extraction\\n\",\n    \"job_visualization = create_html_visualization(job_extraction_results)\\n\",\n    \"display(HTML(job_visualization))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"5rD_zGHI1_RM\"\n   },\n   \"source\": [\n    \"## Approach 5: Advanced Extraction with Context and Reasoning\\n\",\n    \"\\n\",\n    \"Let's implement a more advanced approach that includes reasoning steps and context awareness.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"5wG9VBCz2CkE\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def extract_with_reasoning(text, instruction, schema, examples=None):\\n\",\n    \"    \\\"\\\"\\\"Extract information with explicit reasoning steps.\\n\",\n    \"    \\n\",\n    \"    Args:\\n\",\n    \"        text: Text to analyze\\n\",\n    \"        instruction: Extraction instruction\\n\",\n    \"        schema: JSON schema for the output\\n\",\n    \"        examples: Optional examples of extraction (few-shot learning)\\n\",\n    \"    \\n\",\n    \"    Returns:\\n\",\n    \"        Dictionary with extraction results and reasoning\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    examples_text = \\\"\\\"\\n\",\n    \"    if examples:\\n\",\n    \"        examples_text = \\\"Here are some examples of the expected extraction:\\\\n\\\\n\\\"\\n\",\n    \"        for i, example in enumerate(examples):\\n\",\n    \"            examples_text += f\\\"Example {i+1}:\\\\n\\\"\\n\",\n    \"            examples_text += f\\\"Text: {example['text']}\\\\n\\\"\\n\",\n    \"            examples_text += f\\\"Reasoning: {example['reasoning']}\\\\n\\\"\\n\",\n    \"            examples_text += f\\\"Result: {json.dumps(example['result'], indent=2)}\\\\n\\\\n\\\"\\n\",\n    \"    \\n\",\n    \"    prompt = f\\\"\\\"\\\"\\n\",\n    \"{instruction}\\n\",\n    \"\\n\",\n    \"TEXT TO ANALYZE:\\n\",\n    \"{text}\\n\",\n    \"\\n\",\n    \"{examples_text}\\n\",\n    \"\\n\",\n    \"Follow these steps:\\n\",\n    \"1. First, identify and list the key information in the text relevant to the task\\n\",\n    \"2. For any ambiguous or implicit information, explain your reasoning\\n\",\n    \"3. For any missing information in the schema, provide reasonable defaults or indicate as null/empty\\n\",\n    \"4. Structure your final answer according to this JSON schema:\\n\",\n    \"{json.dumps(schema, indent=2)}\\n\",\n    \"\\n\",\n    \"Your response should have two parts:\\n\",\n    \"1. REASONING: Explain your extraction process and any decisions you made\\n\",\n    \"2. RESULT: The final JSON output that strictly follows the schema\\n\",\n    \"\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    response = model.generate_content(prompt)\\n\",\n    \"    \\n\",\n    \"    # Parse the response to separate reasoning and result\\n\",\n    \"    text = response.text\\n\",\n    \"    \\n\",\n    \"    reasoning = \\\"\\\"\\n\",\n    \"    result = {}\\n\",\n    \"    \\n\",\n    \"    # Try to extract reasoning and result sections\\n\",\n    \"    reasoning_match = None\\n\",\n    \"    result_match = None\\n\",\n    \"    \\n\",\n    \"    if \\\"REASONING:\\\" in text and \\\"RESULT:\\\" in text:\\n\",\n    \"        parts = text.split(\\\"RESULT:\\\")\\n\",\n    \"        reasoning = parts[0].replace(\\\"REASONING:\\\", \\\"\\\").strip()\\n\",\n    \"        result_text = parts[1].strip()\\n\",\n    \"        \\n\",\n    \"        # Try to parse the JSON result\\n\",\n    \"        try:\\n\",\n    \"            # Find where the JSON starts and ends\\n\",\n    \"            json_start = result_text.find('{')\\n\",\n    \"            json_end = result_text.rfind('}')\\n\",\n    \"            if json_start >= 0 and json_end >= 0:\\n\",\n    \"                json_str = result_text[json_start:json_end+1]\\n\",\n    \"                result = json.loads(json_str)\\n\",\n    \"            else:\\n\",\n    \"                result = {\\\"error\\\": \\\"Could not locate JSON in result\\\"}\\n\",\n    \"        except json.JSONDecodeError as e:\\n\",\n    \"            result = {\\\"error\\\": f\\\"Failed to parse JSON: {str(e)}\\\", \\\"raw_text\\\": result_text}\\n\",\n    \"    else:\\n\",\n    \"        # If the response doesn't follow the expected format, try to extract JSON directly\\n\",\n    \"        try:\\n\",\n    \"            # Look for JSON-like structure\\n\",\n    \"            json_start = text.find('{')\\n\",\n    \"            json_end = text.rfind('}')\\n\",\n    \"            if json_start >= 0 and json_end >= 0:\\n\",\n    \"                json_str = text[json_start:json_end+1]\\n\",\n    \"                result = json.loads(json_str)\\n\",\n    \"                reasoning = text[:json_start].strip()\\n\",\n    \"            else:\\n\",\n    \"                reasoning = \\\"Could not identify explicit reasoning\\\"\\n\",\n    \"                result = {\\\"error\\\": \\\"Could not locate JSON in response\\\"}\\n\",\n    \"        except json.JSONDecodeError as e:\\n\",\n    \"            reasoning = \\\"Could not identify explicit reasoning\\\"\\n\",\n    \"            result = {\\\"error\\\": f\\\"Failed to parse JSON: {str(e)}\\\", \\\"raw_text\\\": text}\\n\",\n    \"    \\n\",\n    \"    return {\\n\",\n    \"        \\\"reasoning\\\": reasoning,\\n\",\n    \"        \\\"result\\\": result\\n\",\n    \"    }\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"E3dQJ1lT2GYs\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Define an example for few-shot learning\\n\",\n    \"event_examples = [\\n\",\n    \"    {\\n\",\n    \"        \\\"text\\\": \\\"\\\"\\\"DEVELOPER CONFERENCE 2023\\n\",\n    \"Join us May 10-12, 2023 at the Tech Center in Boston for the biggest developer event of the year.\\n\",\n    \"Speakers include Jane Smith (CTO, TechCorp) and Dr. Michael Johnson (AI Research Lead).\\n\",\n    \"Regular tickets: $350, Student discount available (25% off).\\n\",\n    \"Contact: info@devconf.com\\\"\\\"\\\",\\n\",\n    \"        \\\"reasoning\\\": \\\"\\\"\\\"I identified the event name as 'DEVELOPER CONFERENCE 2023' from the title. The dates are explicitly mentioned as May 10-12, 2023. The location is stated as 'Tech Center in Boston'. For speakers, there are two people mentioned with their roles: Jane Smith (CTO at TechCorp) and Dr. Michael Johnson (AI Research Lead). For pricing, I found the regular ticket price ($350) and a student discount (25% off). The contact email is clearly provided. The schedule details aren't explicitly mentioned, so I'll leave that field empty.\\\"\\\"\\\",\\n\",\n    \"        \\\"result\\\": {\\n\",\n    \"            \\\"event_name\\\": \\\"DEVELOPER CONFERENCE 2023\\\",\\n\",\n    \"            \\\"dates\\\": {\\\"start_date\\\": \\\"May 10, 2023\\\", \\\"end_date\\\": \\\"May 12, 2023\\\"},\\n\",\n    \"            \\\"location\\\": \\\"Tech Center in Boston\\\",\\n\",\n    \"            \\\"speakers\\\": [\\n\",\n    \"                {\\\"name\\\": \\\"Jane Smith\\\", \\\"title\\\": \\\"CTO\\\", \\\"organization\\\": \\\"TechCorp\\\"},\\n\",\n    \"                {\\\"name\\\": \\\"Dr. Michael Johnson\\\", \\\"title\\\": \\\"AI Research Lead\\\", \\\"organization\\\": \\\"\\\"}\\n\",\n    \"            ],\\n\",\n    \"            \\\"pricing\\\": {\\n\",\n    \"                \\\"regular\\\": 350,\\n\",\n    \"                \\\"special_discounts\\\": [{\\\"type\\\": \\\"Student\\\", \\\"discount\\\": \\\"25% off\\\"}]\\n\",\n    \"            },\\n\",\n    \"            \\\"contact\\\": {\\\"email\\\": \\\"info@devconf.com\\\"}\\n\",\n    \"        }\\n\",\n    \"    }\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"# Define the schema for event extraction\\n\",\n    \"event_reasoning_schema = {\\n\",\n    \"    \\\"event_name\\\": \\\"string\\\",\\n\",\n    \"    \\\"dates\\\": {\\n\",\n    \"        \\\"start_date\\\": \\\"string\\\",\\n\",\n    \"        \\\"end_date\\\": \\\"string\\\"\\n\",\n    \"    },\\n\",\n    \"    \\\"location\\\": \\\"string\\\",\\n\",\n    \"    \\\"speakers\\\": [\\n\",\n    \"        {\\n\",\n    \"            \\\"name\\\": \\\"string\\\",\\n\",\n    \"            \\\"title\\\": \\\"string\\\",\\n\",\n    \"            \\\"organization\\\": \\\"string\\\"\\n\",\n    \"        }\\n\",\n    \"    ],\\n\",\n    \"    \\\"schedule\\\": [\\n\",\n    \"        {\\n\",\n    \"            \\\"day\\\": \\\"string\\\",\\n\",\n    \"            \\\"date\\\": \\\"string\\\",\\n\",\n    \"            \\\"activities\\\": [\\n\",\n    \"                {\\n\",\n    \"                    \\\"name\\\": \\\"string\\\",\\n\",\n    \"                    \\\"time\\\": \\\"string\\\"\\n\",\n    \"                }\\n\",\n    \"            ]\\n\",\n    \"        }\\n\",\n    \"    ],\\n\",\n    \"    \\\"pricing\\\": {\\n\",\n    \"        \\\"early_bird\\\": \\\"number\\\",\\n\",\n    \"        \\\"regular\\\": \\\"number\\\",\\n\",\n    \"        \\\"special_discounts\\\": [\\n\",\n    \"            {\\n\",\n    \"                \\\"type\\\": \\\"string\\\",\\n\",\n    \"                \\\"discount\\\": \\\"string\\\"\\n\",\n    \"            }\\n\",\n    \"        ]\\n\",\n    \"    },\\n\",\n    \"    \\\"contact\\\": {\\n\",\n    \"        \\\"website\\\": \\\"string\\\",\\n\",\n    \"        \\\"email\\\": \\\"string\\\"\\n\",\n    \"    }\\n\",\n    \"}\\n\",\n    \"\\n\",\n    \"# Extract event information with reasoning\\n\",\n    \"event_instruction = \\\"Extract detailed structured information from this event announcement. Include all relevant details about the event name, dates, location, speakers, schedule, pricing, and contact information.\\\"\\n\",\n    \"\\n\",\n    \"event_reasoning_result = extract_with_reasoning(\\n\",\n    \"    event_announcement,\\n\",\n    \"    event_instruction,\\n\",\n    \"    event_reasoning_schema,\\n\",\n    \"    event_examples\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"# Display the reasoning\\n\",\n    \"print(\\\"Extraction Reasoning:\\\")\\n\",\n    \"display(Markdown(event_reasoning_result[\\\"reasoning\\\"]))\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nExtraction Result:\\\")\\n\",\n    \"display(Markdown(f\\\"```json\\\\n{json.dumps(event_reasoning_result['result'], indent=2)}\\\\n```\\\"))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"W_bT4Pf82NPc\"\n   },\n   \"source\": [\n    \"## Comparison of Approaches\\n\",\n    \"\\n\",\n    \"Let's compare the different approaches we've used for information extraction:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"7qLx-aCW2QtM\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"approaches = [\\n\",\n    \"    {\\n\",\n    \"        \\\"name\\\": \\\"Basic Structured Output Request\\\",\\n\",\n    \"        \\\"pros\\\": [\\n\",\n    \"            \\\"Simple to implement\\\",\\n\",\n    \"            \\\"Works well for straightforward extraction tasks\\\",\\n\",\n    \"            \\\"Minimal prompt engineering required\\\"\\n\",\n    \"        ],\\n\",\n    \"        \\\"cons\\\": [\\n\",\n    \"            \\\"Less control over output format\\\",\\n\",\n    \"            \\\"May produce inconsistent structure\\\",\\n\",\n    \"            \\\"No explicit reasoning or validation\\\"\\n\",\n    \"        ],\\n\",\n    \"        \\\"best_for\\\": \\\"Simple extraction tasks with flexible output requirements\\\"\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"name\\\": \\\"Schema-Based Extraction\\\",\\n\",\n    \"        \\\"pros\\\": [\\n\",\n    \"            \\\"More consistent output structure\\\",\\n\",\n    \"            \\\"Better enforcement of expected fields\\\",\\n\",\n    \"            \\\"Helps model understand the exact format needed\\\"\\n\",\n    \"        ],\\n\",\n    \"        \\\"cons\\\": [\\n\",\n    \"            \\\"Requires defining schema in advance\\\",\\n\",\n    \"            \\\"Still may produce invalid JSON occasionally\\\",\\n\",\n    \"            \\\"Less flexibility for unexpected information\\\"\\n\",\n    \"        ],\\n\",\n    \"        \\\"best_for\\\": \\\"Structured extraction where output format is well-defined\\\"\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"name\\\": \\\"Function Calling\\\",\\n\",\n    \"        \\\"pros\\\": [\\n\",\n    \"            \\\"Most reliable JSON output format\\\",\\n\",\n    \"            \\\"Strong enforcement of schema constraints\\\",\\n\",\n    \"            \\\"Clean integration with application code\\\"\\n\",\n    \"        ],\\n\",\n    \"        \\\"cons\\\": [\\n\",\n    \"            \\\"Requires more setup with function declarations\\\",\\n\",\n    \"            \\\"Less flexibility for free-form extraction\\\",\\n\",\n    \"            \\\"May not work with all model versions\\\"\\n\",\n    \"        ],\\n\",\n    \"        \\\"best_for\\\": \\\"Production applications requiring reliable structured output\\\"\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"name\\\": \\\"Multi-Stage Pipeline\\\",\\n\",\n    \"        \\\"pros\\\": [\\n\",\n    \"            \\\"Can handle complex documents with varied information\\\",\\n\",\n    \"            \\\"More focused extraction for each component\\\",\\n\",\n    \"            \\\"Initial analysis helps tailor subsequent extraction\\\"\\n\",\n    \"        ],\\n\",\n    \"        \\\"cons\\\": [\\n\",\n    \"            \\\"Multiple API calls increase latency and cost\\\",\\n\",\n    \"            \\\"More complex implementation\\\",\\n\",\n    \"            \\\"Potential for error propagation between stages\\\"\\n\",\n    \"        ],\\n\",\n    \"        \\\"best_for\\\": \\\"Complex documents with multiple distinct information categories\\\"\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"name\\\": \\\"Extraction with Reasoning\\\",\\n\",\n    \"        \\\"pros\\\": [\\n\",\n    \"            \\\"Provides explicit reasoning for extraction decisions\\\",\\n\",\n    \"            \\\"Better handling of ambiguous or implicit information\\\",\\n\",\n    \"            \\\"Examples improve extraction quality (few-shot learning)\\\"\\n\",\n    \"        ],\\n\",\n    \"        \\\"cons\\\": [\\n\",\n    \"            \\\"Longer prompts increase token usage\\\",\\n\",\n    \"            \\\"More complex response parsing\\\",\\n\",\n    \"            \\\"May still require validation of output\\\"\\n\",\n    \"        ],\\n\",\n    \"        \\\"best_for\\\": \\\"Critical applications where reasoning and transparency are important\\\"\\n\",\n    \"    }\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"# Create a comparison table\\n\",\n    \"comparison_html = \\\"\\\"\\\"\\n\",\n    \"<style>\\n\",\n    \"  table {\\n\",\n    \"    border-collapse: collapse;\\n\",\n    \"    width: 100%;\\n\",\n    \"    font-family: Arial, sans-serif;\\n\",\n    \"  }\\n\",\n    \"  th, td {\\n\",\n    \"    border: 1px solid #ddd;\\n\",\n    \"    padding: 8px;\\n\",\n    \"    text-align: left;\\n\",\n    \"  }\\n\",\n    \"  th {\\n\",\n    \"    background-color: #f2f2f2;\\n\",\n    \"    font-weight: bold;\\n\",\n    \"  }\\n\",\n    \"  tr:nth-child(even) {\\n\",\n    \"    background-color: #f9f9f9;\\n\",\n    \"  }\\n\",\n    \"  ul {\\n\",\n    \"    margin: 0;\\n\",\n    \"    padding-left: 20px;\\n\",\n    \"  }\\n\",\n    \"</style>\\n\",\n    \"<table>\\n\",\n    \"  <tr>\\n\",\n    \"    <th>Approach</th>\\n\",\n    \"    <th>Pros</th>\\n\",\n    \"    <th>Cons</th>\\n\",\n    \"    <th>Best For</th>\\n\",\n    \"  </tr>\\n\",\n    \"\\\"\\\"\\\"\\n\",\n    \"\\n\",\n    \"for approach in approaches:\\n\",\n    \"    pros_html = \\\"<ul>\\\" + \\\"\\\".join([f\\\"<li>{pro}</li>\\\" for pro in approach[\\\"pros\\\"]]) + \\\"</ul>\\\"\\n\",\n    \"    cons_html = \\\"<ul>\\\" + \\\"\\\".join([f\\\"<li>{con}</li>\\\" for con in approach[\\\"cons\\\"]]) + \\\"</ul>\\\"\\n\",\n    \"    \\n\",\n    \"    comparison_html += f\\\"\\\"\\\"\\n\",\n    \"  <tr>\\n\",\n    \"    <td><strong>{approach[\\\"name\\\"]}</strong></td>\\n\",\n    \"    <td>{pros_html}</td>\\n\",\n    \"    <td>{cons_html}</td>\\n\",\n    \"    <td>{approach[\\\"best_for\\\"]}</td>\\n\",\n    \"  </tr>\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"\\n\",\n    \"comparison_html += \\\"</table>\\\"\\n\",\n    \"\\n\",\n    \"display(HTML(comparison_html))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"7lMBYbNb4Wc8\"\n   },\n   \"source\": [\n    \"## Best Practices for Information Extraction\\n\",\n    \"\\n\",\n    \"Based on our experiments, here are some best practices for information extraction:\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"pFgMxWgY4Y9E\"\n   },\n   \"source\": [\n    \"1. **Be explicit about the output format**: Clearly define the exact structure you need, including field names and data types.\\n\",\n    \"\\n\",\n    \"2. **Use schemas whenever possible**: Providing a schema helps the model understand the expected output structure and improves consistency.\\n\",\n    \"\\n\",\n    \"3. **Choose the right approach for your use case**:\\n\",\n    \"   - For simple extraction: Basic structured output requests\\n\",\n    \"   - For consistent structured data: Schema-based extraction or function calling\\n\",\n    \"   - For complex documents: Multi-stage pipelines\\n\",\n    \"   - For critical applications: Extraction with reasoning\\n\",\n    \"\\n\",\n    \"4. **Always validate outputs**: Even with structured approaches, validate the generated JSON to ensure it matches your requirements.\\n\",\n    \"\\n\",\n    \"5. **Include examples for complex tasks**: For challenging extraction tasks, providing examples (few-shot learning) can significantly improve results.\\n\",\n    \"\\n\",\n    \"6. **Break down complex tasks**: Divide complex extraction into multiple targeted steps rather than trying to extract everything at once.\\n\",\n    \"\\n\",\n    \"7. **Consider error handling**: Implement robust error handling for cases where the model produces invalid or unexpected outputs.\\n\",\n    \"\\n\",\n    \"8. **Balance precision and recall**: Decide whether it's more important to get all possible information (high recall) or only the most accurate information (high precision).\\n\",\n    \"\\n\",\n    \"9. **Use appropriate temperature settings**: Lower temperature (e.g., 0.1-0.3) generally produces more consistent structured outputs.\\n\",\n    \"\\n\",\n    \"10. **Implement post-processing**: For critical applications, consider implementing additional validation and normalization of extracted data.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"W5k-7cFx55B0\"\n   },\n   \"source\": [\n    \"## Conclusion\\n\",\n    \"\\n\",\n    \"In this notebook, we've explored various approaches for extracting structured information from text using Google's Gemini API. Each approach has its strengths and is suitable for different use cases:\\n\",\n    \"\\n\",\n    \"- **Basic Structured Output Request**: Simple and straightforward for basic extraction needs\\n\",\n    \"- **Schema-Based Extraction**: Better consistency with explicit schema definition\\n\",\n    \"- **Function Calling**: Most reliable for production applications requiring structured output\\n\",\n    \"- **Multi-Stage Pipeline**: Powerful for complex documents with varied information types\\n\",\n    \"- **Extraction with Reasoning**: Provides transparency and handles ambiguity well\\n\",\n    \"\\n\",\n    \"By combining these approaches and following the best practices, you can build robust information extraction systems for a wide range of applications.\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"colab\": {\n   \"provenance\": []\n  },\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"name\": \"python\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 0\n}\n\u0005End File\u0006# google/gemini-api-cookbook\n{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"ZktIAr-k5rny\"\n   },\n   \"source\": [\n    \"# Vector Search with MongoDB Atlas\\n\",\n    \"\\n\",\n    \"In this notebook, we'll demonstrate how to integrate MongoDB Atlas Vector Search with Google's Generative AI models to create a question-answering system over your documents.\\n\",\n    \"\\n\",\n    \"We'll cover:\\n\",\n    \"1. Setting up MongoDB Atlas with Vector Search\\n\",\n    \"2. Creating and uploading document embeddings\\n\",\n    \"3. Implementing a RAG (Retrieval-Augmented Generation) pattern for answering questions using Gemini\\n\",\n    \"\\n\",\n    \"Let's get started!\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"RXgHgPqT5-Q6\"\n   },\n   \"source\": [\n    \"## Setup and Install Dependencies\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"Q6kAjeMi57Zy\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!pip install -q pymongo google-generativeai sentence-transformers\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"DLYhpRHn6ByR\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"import os\\n\",\n    \"import json\\n\",\n    \"import uuid\\n\",\n    \"import time\\n\",\n    \"from typing import List, Dict, Any\\n\",\n    \"\\n\",\n    \"import google.generativeai as genai\\n\",\n    \"from pymongo import MongoClient\\n\",\n    \"from sentence_transformers import SentenceTransformer\\n\",\n    \"from IPython.display import display, Markdown\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"ik8BQQZb6ER5\"\n   },\n   \"source\": [\n    \"## Configure API Keys and Connection Details\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"cellView\": \"form\",\n    \"id\": \"mCWYz0jz6IsR\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# @title Set up API Key and MongoDB Connection\\n\",\n    \"GOOGLE_API_KEY = \\\"\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"MONGODB_CONNECTION_STRING = \\\"\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"\\n\",\n    \"# Check if API key is provided or available as environment variable\\n\",\n    \"if not GOOGLE_API_KEY:\\n\",\n    \"    if \\\"GOOGLE_API_KEY\\\" in os.environ:\\n\",\n    \"        GOOGLE_API_KEY = os.environ[\\\"GOOGLE_API_KEY\\\"]\\n\",\n    \"    else:\\n\",\n    \"        print(\\\"Please provide your Google API key!\\\")\\n\",\n    \"else:\\n\",\n    \"    os.environ[\\\"GOOGLE_API_KEY\\\"] = GOOGLE_API_KEY\\n\",\n    \"\\n\",\n    \"# Check if MongoDB connection string is provided or available as environment variable\\n\",\n    \"if not MONGODB_CONNECTION_STRING:\\n\",\n    \"    if \\\"MONGODB_CONNECTION_STRING\\\" in os.environ:\\n\",\n    \"        MONGODB_CONNECTION_STRING = os.environ[\\\"MONGODB_CONNECTION_STRING\\\"]\\n\",\n    \"    else:\\n\",\n    \"        print(\\\"Please provide your MongoDB connection string!\\\")\\n\",\n    \"else:\\n\",\n    \"    os.environ[\\\"MONGODB_CONNECTION_STRING\\\"] = MONGODB_CONNECTION_STRING\\n\",\n    \"\\n\",\n    \"# Configure Gemini API\\n\",\n    \"genai.configure(api_key=GOOGLE_API_KEY)\\n\",\n    \"\\n\",\n    \"# Configure MongoDB (validation will be done later)\\n\",\n    \"DB_NAME = \\\"gemini_vector_search\\\"\\n\",\n    \"COLLECTION_NAME = \\\"documents\\\"\\n\",\n    \"EMBEDDING_MODEL = \\\"all-MiniLM-L6-v2\\\"  # Small, fast model good for semantic search\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"Eo4Gd_Dx6MgR\"\n   },\n   \"source\": [\n    \"## Configure MongoDB Connection and Test\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"fSdnqBE76QWp\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def get_mongodb_client():\\n\",\n    \"    \\\"\\\"\\\"Create and return a MongoDB client.\\\"\\\"\\\"\\n\",\n    \"    try:\\n\",\n    \"        client = MongoClient(MONGODB_CONNECTION_STRING)\\n\",\n    \"        # Test the connection\\n\",\n    \"        client.admin.command('ping')\\n\",\n    \"        print(\\\"Successfully connected to MongoDB Atlas!\\\")\\n\",\n    \"        return client\\n\",\n    \"    except Exception as e:\\n\",\n    \"        print(f\\\"Error connecting to MongoDB Atlas: {e}\\\")\\n\",\n    \"        return None\\n\",\n    \"\\n\",\n    \"# Initialize MongoDB client\\n\",\n    \"mongo_client = get_mongodb_client()\\n\",\n    \"\\n\",\n    \"if mongo_client:\\n\",\n    \"    # Initialize database and collection\\n\",\n    \"    db = mongo_client[DB_NAME]\\n\",\n    \"    collection = db[COLLECTION_NAME]\\n\",\n    \"    print(f\\\"Using database: {DB_NAME}, collection: {COLLECTION_NAME}\\\")\\n\",\n    \"else:\\n\",\n    \"    print(\\\"Failed to connect to MongoDB. Please check your connection string.\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"tT_4n-f76VMp\"\n   },\n   \"source\": [\n    \"## Initialize Embedding Model\\n\",\n    \"\\n\",\n    \"We'll use a Sentence Transformer model from Hugging Face to generate embeddings for our documents.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"nsvHEkq46ZdJ\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Initialize the embedding model\\n\",\n    \"embedding_model = SentenceTransformer(EMBEDDING_MODEL)\\n\",\n    \"print(f\\\"Initialized embedding model: {EMBEDDING_MODEL}\\\")\\n\",\n    \"print(f\\\"Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"2XnYPVOz6eA5\"\n   },\n   \"source\": [\n    \"## Create Sample Documents\\n\",\n    \"\\n\",\n    \"Let's create some sample documents about machine learning concepts to demonstrate our vector search capabilities.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"IkXLnMU36h1p\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Create sample documents\\n\",\n    \"documents = [\\n\",\n    \"    {\\n\",\n    \"        \\\"title\\\": \\\"Introduction to Neural Networks\\\",\\n\",\n    \"        \\\"content\\\": \\\"\\\"\\\"Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. \\n\",\n    \"        They interpret sensory data through a kind of machine perception, labeling or clustering raw input. \\n\",\n    \"        The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text or time series, must be translated.\\n\",\n    \"        \\n\",\n    \"        Neural networks help us cluster and classify data. You can think of them as a clustering and classification layer on top of the data you store and manage. \\n\",\n    \"        They help to group unlabeled data according to similarities among the example inputs, and they classify data when they have a labeled dataset to train on.\\n\",\n    \"        \\n\",\n    \"        The building block of neural networks is the perceptron, which is a mathematical model of a biological neuron. Like biological neurons, perceptrons have inputs, \\n\",\n    \"        which are dendrites, and produce outputs, which would be the axon in a biological neuron. Each perceptron forms a decision boundary based on its input weights.\\\"\\\"\\\",\\n\",\n    \"        \\\"category\\\": \\\"Deep Learning\\\",\\n\",\n    \"        \\\"tags\\\": [\\\"neural networks\\\", \\\"deep learning\\\", \\\"perceptron\\\"]\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"title\\\": \\\"Support Vector Machines Explained\\\",\\n\",\n    \"        \\\"content\\\": \\\"\\\"\\\"Support Vector Machines (SVMs) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. \\n\",\n    \"        Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples \\n\",\n    \"        to one category or the other, making it a non-probabilistic binary linear classifier.\\n\",\n    \"        \\n\",\n    \"        An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. \\n\",\n    \"        New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\\n\",\n    \"        \\n\",\n    \"        In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs \\n\",\n    \"        into high-dimensional feature spaces. When data is not labeled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find \\n\",\n    \"        natural clustering of the data to groups, and then map new data to these formed groups.\\\"\\\"\\\",\\n\",\n    \"        \\\"category\\\": \\\"Machine Learning\\\",\\n\",\n    \"        \\\"tags\\\": [\\\"svm\\\", \\\"classification\\\", \\\"supervised learning\\\"]\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"title\\\": \\\"Introduction to Reinforcement Learning\\\",\\n\",\n    \"        \\\"content\\\": \\\"\\\"\\\"Reinforcement learning is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of \\n\",\n    \"        cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\\n\",\n    \"        \\n\",\n    \"        Reinforcement learning differs from supervised learning in that labeled input/output pairs need not be presented, and sub-optimal actions need not be explicitly corrected. \\n\",\n    \"        Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).\\n\",\n    \"        \\n\",\n    \"        The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context utilize dynamic programming techniques. \\n\",\n    \"        The main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP \\n\",\n    \"        and they target large MDPs where exact methods become infeasible.\\\"\\\"\\\",\\n\",\n    \"        \\\"category\\\": \\\"Machine Learning\\\",\\n\",\n    \"        \\\"tags\\\": [\\\"reinforcement learning\\\", \\\"machine learning\\\", \\\"MDP\\\"]\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"title\\\": \\\"K-means Clustering Algorithm\\\",\\n\",\n    \"        \\\"content\\\": \\\"\\\"\\\"K-means clustering is one of the simplest and popular unsupervised machine learning algorithms. The objective of K-means is simple: group similar data points together \\n\",\n    \"        and discover underlying patterns. To achieve this objective, K-means looks for a fixed number (k) of clusters in a dataset.\\n\",\n    \"        \\n\",\n    \"        A cluster refers to a collection of data points aggregated together because of certain similarities. You'll define a target number k, which refers to the number of centroids \\n\",\n    \"        you need in the dataset. A centroid is the imaginary or real location representing the center of the cluster.\\n\",\n    \"        \\n\",\n    \"        The K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible. \\n\",\n    \"        The 'means' in the K-means refers to averaging of the data; that is, finding the centroid.\\n\",\n    \"        \\n\",\n    \"        To process the learning data, the K-means algorithm in data mining starts with a first group of randomly selected centroids, which are used as the beginning points for every cluster, \\n\",\n    \"        and then performs iterative (repetitive) calculations to optimize the positions of the centroids. It halts when the centroids have stabilized or the defined number of iterations has been achieved.\\\"\\\"\\\",\\n\",\n    \"        \\\"category\\\": \\\"Machine Learning\\\",\\n\",\n    \"        \\\"tags\\\": [\\\"clustering\\\", \\\"unsupervised learning\\\", \\\"k-means\\\"]\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"title\\\": \\\"Random Forests for Classification and Regression\\\",\\n\",\n    \"        \\\"content\\\": \\\"\\\"\\\"Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees \\n\",\n    \"        at training time and outputting the class that is the mode of the classes (classification) or mean/average prediction (regression) of the individual trees.\\n\",\n    \"        \\n\",\n    \"        Random decision forests correct for decision trees' habit of overfitting to their training set. Random forests generally outperform decision trees, but their accuracy is lower than \\n\",\n    \"        gradient boosted trees. However, data characteristics can affect their performance.\\n\",\n    \"        \\n\",\n    \"        The random forest algorithm combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification \\n\",\n    \"        and regression problems. Here's how a random forest works: A random forest consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest \\n\",\n    \"        spits out a class prediction and the class with the most votes becomes our model's prediction.\\n\",\n    \"        \\n\",\n    \"        One big advantage of random forest is that it can be used for both classification and regression problems, which form the majority of current machine learning systems.\\\"\\\"\\\",\\n\",\n    \"        \\\"category\\\": \\\"Machine Learning\\\",\\n\",\n    \"        \\\"tags\\\": [\\\"random forest\\\", \\\"ensemble learning\\\", \\\"decision trees\\\"]\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"title\\\": \\\"Introduction to Natural Language Processing\\\",\\n\",\n    \"        \\\"content\\\": \\\"\\\"\\\"Natural Language Processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, \\n\",\n    \"        in particular how to program computers to process and analyze large amounts of natural language data.\\n\",\n    \"        \\n\",\n    \"        The goal is a computer capable of \\\"understanding\\\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information \\n\",\n    \"        and insights contained in the documents as well as categorize and organize the documents themselves.\\n\",\n    \"        \\n\",\n    \"        Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. Natural language processing has its roots in the 1950s. \\n\",\n    \"        Already in 1950, Alan Turing published an article titled \\\"Computing Machinery and Intelligence\\\" which proposed what is now called the Turing test as a criterion of intelligence, a task that involves \\n\",\n    \"        the automated interpretation and generation of natural language, but at the time not articulated as a problem separate from artificial intelligence.\\n\",\n    \"        \\n\",\n    \"        Modern NLP algorithms are based on machine learning, especially statistical machine learning. The paradigm of machine learning is different from that of most prior attempts at language processing \\n\",\n    \"        in that the learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed.\\\"\\\"\\\",\\n\",\n    \"        \\\"category\\\": \\\"Natural Language Processing\\\",\\n\",\n    \"        \\\"tags\\\": [\\\"NLP\\\", \\\"language processing\\\", \\\"text analysis\\\"]\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"title\\\": \\\"Understanding Convolutional Neural Networks\\\",\\n\",\n    \"        \\\"content\\\": \\\"\\\"\\\"Convolutional Neural Networks (CNNs) are a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant \\n\",\n    \"        artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics.\\n\",\n    \"        \\n\",\n    \"        CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing. They are also known as shift invariant or space invariant artificial neural networks (SIANN), \\n\",\n    \"        based on their shared-weights architecture and translation invariance characteristics.\\n\",\n    \"        \\n\",\n    \"        Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons \\n\",\n    \"        respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\\n\",\n    \"        \\n\",\n    \"        CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns the filters that in traditional algorithms were hand-engineered. \\n\",\n    \"        This independence from prior knowledge and human effort in feature design is a major advantage.\\\"\\\"\\\",\\n\",\n    \"        \\\"category\\\": \\\"Deep Learning\\\",\\n\",\n    \"        \\\"tags\\\": [\\\"CNN\\\", \\\"deep learning\\\", \\\"computer vision\\\"]\\n\",\n    \"    }\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"print(f\\\"Created {len(documents)} sample documents\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"n1JzKkGO6qKy\"\n   },\n   \"source\": [\n    \"## Generate Embeddings and Store Documents in MongoDB\\n\",\n    \"\\n\",\n    \"Now we'll generate embeddings for each document and store them in MongoDB.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"mYBBimrJ6uW5\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def generate_embedding(text: str) -> List[float]:\\n\",\n    \"    \\\"\\\"\\\"Generate embedding for a text using the sentence transformer model.\\\"\\\"\\\"\\n\",\n    \"    embedding = embedding_model.encode(text)\\n\",\n    \"    return embedding.tolist()\\n\",\n    \"\\n\",\n    \"def prepare_documents_with_embeddings(documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\\n\",\n    \"    \\\"\\\"\\\"Prepare documents with embeddings for MongoDB.\\\"\\\"\\\"\\n\",\n    \"    processed_documents = []\\n\",\n    \"    \\n\",\n    \"    for doc in documents:\\n\",\n    \"        # Create a unique ID for each document\\n\",\n    \"        doc_id = str(uuid.uuid4())\\n\",\n    \"        \\n\",\n    \"        # Get embedding for title + content (this could be improved by chunking for long documents)\\n\",\n    \"        embedding_text = doc[\\\"title\\\"] + \\\" \\\" + doc[\\\"content\\\"]\\n\",\n    \"        embedding = generate_embedding(embedding_text)\\n\",\n    \"        \\n\",\n    \"        # Create the document with embedding\\n\",\n    \"        processed_doc = {\\n\",\n    \"            \\\"_id\\\": doc_id,\\n\",\n    \"            \\\"title\\\": doc[\\\"title\\\"],\\n\",\n    \"            \\\"content\\\": doc[\\\"content\\\"],\\n\",\n    \"            \\\"category\\\": doc[\\\"category\\\"],\\n\",\n    \"            \\\"tags\\\": doc[\\\"tags\\\"],\\n\",\n    \"            \\\"embedding\\\": embedding,\\n\",\n    \"            \\\"created_at\\\": time.time()\\n\",\n    \"        }\\n\",\n    \"        \\n\",\n    \"        processed_documents.append(processed_doc)\\n\",\n    \"    \\n\",\n    \"    return processed_documents\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"AYDQL5Yb60ny\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Process documents and add embeddings\\n\",\n    \"documents_with_embeddings = prepare_documents_with_embeddings(documents)\\n\",\n    \"\\n\",\n    \"# Insert documents into MongoDB if connected\\n\",\n    \"if mongo_client:\\n\",\n    \"    # First, drop the collection to start fresh\\n\",\n    \"    db.drop_collection(COLLECTION_NAME)\\n\",\n    \"    collection = db[COLLECTION_NAME]\\n\",\n    \"    \\n\",\n    \"    # Insert the documents\\n\",\n    \"    result = collection.insert_many(documents_with_embeddings)\\n\",\n    \"    print(f\\\"Inserted {len(result.inserted_ids)} documents into MongoDB\\\")\\n\",\n    \"else:\\n\",\n    \"    print(\\\"Cannot insert documents: MongoDB connection not available\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"iCJ4pR6d64zR\"\n   },\n   \"source\": [\n    \"## Create a Vector Search Index in MongoDB Atlas\\n\",\n    \"\\n\",\n    \"Now we need to create a vector search index in MongoDB Atlas. This can be done either through the Atlas UI or using the MongoDB API.\\n\",\n    \"\\n\",\n    \"Here's how to do it via code:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"lL7sxlvV69hy\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def create_vector_search_index():\\n\",\n    \"    \\\"\\\"\\\"Create a vector search index in MongoDB Atlas.\\\"\\\"\\\"\\n\",\n    \"    if not mongo_client:\\n\",\n    \"        print(\\\"Cannot create index: MongoDB connection not available\\\")\\n\",\n    \"        return False\\n\",\n    \"    \\n\",\n    \"    # Define the index configuration\\n\",\n    \"    index_config = {\\n\",\n    \"        \\\"name\\\": \\\"vector_index\\\",\\n\",\n    \"        \\\"definition\\\": {\\n\",\n    \"            \\\"mappings\\\": {\\n\",\n    \"                \\\"dynamic\\\": True,\\n\",\n    \"                \\\"fields\\\": {\\n\",\n    \"                    \\\"embedding\\\": {\\n\",\n    \"                        \\\"dimensions\\\": embedding_model.get_sentence_embedding_dimension(),\\n\",\n    \"                        \\\"similarity\\\": \\\"cosine\\\",\\n\",\n    \"                        \\\"type\\\": \\\"knnVector\\\"\\n\",\n    \"                    }\\n\",\n    \"                }\\n\",\n    \"            }\\n\",\n    \"        }\\n\",\n    \"    }\\n\",\n    \"    \\n\",\n    \"    try:\\n\",\n    \"        # First, check if the index already exists\\n\",\n    \"        existing_indexes = db.command(\\\"listSearchIndexes\\\", collection.name)\\n\",\n    \"        \\n\",\n    \"        for idx in existing_indexes.get(\\\"cursor\\\", {}).get(\\\"firstBatch\\\", []):\\n\",\n    \"            if idx.get(\\\"name\\\") == \\\"vector_index\\\":\\n\",\n    \"                print(\\\"Vector search index already exists\\\")\\n\",\n    \"                return True\\n\",\n    \"        \\n\",\n    \"        # If the index doesn't exist, create it\\n\",\n    \"        result = db.command(\\\"createSearchIndex\\\", collection.name, index=index_config)\\n\",\n    \"        print(f\\\"Created vector search index: {result}\\\")\\n\",\n    \"        return True\\n\",\n    \"    except Exception as e:\\n\",\n    \"        print(f\\\"Error creating vector search index: {e}\\\")\\n\",\n    \"        return False\\n\",\n    \"\\n\",\n    \"# Create the vector search index\\n\",\n    \"create_vector_search_index()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"k4x-EuWD7C2R\"\n   },\n   \"source\": [\n    \"## Implement Vector Search\\n\",\n    \"\\n\",\n    \"Now let's implement the vector search functionality to find similar documents.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"zcGBHk9J7HCB\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def vector_search(query: str, limit: int = 3) -> List[Dict[str, Any]]:\\n\",\n    \"    \\\"\\\"\\\"Perform vector search on the documents.\\\"\\\"\\\"\\n\",\n    \"    if not mongo_client:\\n\",\n    \"        print(\\\"Cannot perform search: MongoDB connection not available\\\")\\n\",\n    \"        return []\\n\",\n    \"    \\n\",\n    \"    # Generate embedding for the query\\n\",\n    \"    query_embedding = generate_embedding(query)\\n\",\n    \"    \\n\",\n    \"    # Define the search pipeline\\n\",\n    \"    pipeline = [\\n\",\n    \"        {\\n\",\n    \"            \\\"$search\\\": {\\n\",\n    \"                \\\"index\\\": \\\"vector_index\\\",\\n\",\n    \"                \\\"knnBeta\\\": {\\n\",\n    \"                    \\\"vector\\\": query_embedding,\\n\",\n    \"                    \\\"path\\\": \\\"embedding\\\",\\n\",\n    \"                    \\\"k\\\": limit\\n\",\n    \"                }\\n\",\n    \"            }\\n\",\n    \"        },\\n\",\n    \"        {\\n\",\n    \"            \\\"$project\\\": {\\n\",\n    \"                \\\"_id\\\": 1,\\n\",\n    \"                \\\"title\\\": 1,\\n\",\n    \"                \\\"content\\\": 1,\\n\",\n    \"                \\\"category\\\": 1,\\n\",\n    \"                \\\"tags\\\": 1,\\n\",\n    \"                \\\"score\\\": { \\\"$meta\\\": \\\"searchScore\\\" }\\n\",\n    \"            }\\n\",\n    \"        }\\n\",\n    \"    ]\\n\",\n    \"    \\n\",\n    \"    try:\\n\",\n    \"        # Execute the search\\n\",\n    \"        results = list(collection.aggregate(pipeline))\\n\",\n    \"        return results\\n\",\n    \"    except Exception as e:\\n\",\n    \"        print(f\\\"Error performing vector search: {e}\\\")\\n\",\n    \"        return []\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"xbswxoXD7Llp\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Test the vector search\\n\",\n    \"test_query = \\\"How do convolutional neural networks work?\\\"\\n\",\n    \"search_results = vector_search(test_query, limit=2)\\n\",\n    \"\\n\",\n    \"print(f\\\"Search results for query: '{test_query}'\\\\n\\\")\\n\",\n    \"for i, result in enumerate(search_results):\\n\",\n    \"    print(f\\\"Result {i+1}: {result['title']} (Score: {result['score']:.4f})\\\")\\n\",\n    \"    print(f\\\"Category: {result['category']}\\\")\\n\",\n    \"    print(f\\\"Tags: {', '.join(result['tags'])}\\\")\\n\",\n    \"    content_preview = result['content'][:150] + \\\"...\\\" if len(result['content']) > 150 else result['content']\\n\",\n    \"    print(f\\\"Content Preview: {content_preview}\\\\n\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"vRGkbPjl7QVp\"\n   },\n   \"source\": [\n    \"## Implement RAG (Retrieval-Augmented Generation) with Gemini\\n\",\n    \"\\n\",\n    \"Now let's implement the RAG pattern using Gemini to answer questions based on the retrieved documents.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"JVMDfLNc7T-R\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def answer_question(question: str, num_docs: int = 3) -> str:\\n\",\n    \"    \\\"\\\"\\\"Answer a question using RAG with Gemini and MongoDB Atlas Vector Search.\\\"\\\"\\\"\\n\",\n    \"    # Retrieve relevant documents\\n\",\n    \"    relevant_docs = vector_search(question, limit=num_docs)\\n\",\n    \"    \\n\",\n    \"    if not relevant_docs:\\n\",\n    \"        return \\\"I couldn't find any relevant information to answer your question.\\\"\\n\",\n    \"    \\n\",\n    \"    # Prepare context from relevant documents\\n\",\n    \"    context = \\\"\\\"\\n\",\n    \"    for i, doc in enumerate(relevant_docs):\\n\",\n    \"        context += f\\\"Document {i+1}: {doc['title']}\\\\n{doc['content']}\\\\n\\\\n\\\"\\n\",\n    \"    \\n\",\n    \"    # Initialize Gemini model\\n\",\n    \"    model = genai.GenerativeModel('gemini-1.5-pro')\\n\",\n    \"    \\n\",\n    \"    # Prepare the prompt\\n\",\n    \"    prompt = f\\\"\\\"\\\"\\n\",\n    \"    You are a knowledgeable AI assistant that answers questions based on the provided information.\\n\",\n    \"    \\n\",\n    \"    Here is some context information to help you answer the question:\\n\",\n    \"    \\n\",\n    \"    {context}\\n\",\n    \"    \\n\",\n    \"    Based only on the information provided above, please answer the following question:\\n\",\n    \"    {question}\\n\",\n    \"    \\n\",\n    \"    If the information provided doesn't contain the answer, please say \\\"I don't have enough information to answer this question\\\" rather than making up an answer.\\n\",\n    \"    Provide a comprehensive and accurate answer based on the context provided.\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    # Generate the response\\n\",\n    \"    response = model.generate_content(prompt)\\n\",\n    \"    \\n\",\n    \"    return response.text\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"3vAY1X1N7XAR\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Test the question answering\\n\",\n    \"test_questions = [\\n\",\n    \"    \\\"What are the main components of a neural network?\\\",\\n\",\n    \"    \\\"How does K-means clustering work?\\\",\\n\",\n    \"    \\\"What's the difference between supervised and unsupervised learning?\\\",\\n\",\n    \"    \\\"What are convolutional neural networks used for?\\\",\\n\",\n    \"    \\\"How do random forests prevent overfitting?\\\"\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"for question in test_questions:\\n\",\n    \"    print(f\\\"Question: {question}\\\")\\n\",\n    \"    answer = answer_question(question)\\n\",\n    \"    display(Markdown(f\\\"**Answer:**\\\\n{answer}\\\"))\\n\",\n    \"    print(\\\"\\\\n\\\" + \\\"-\\\"*80 + \\\"\\\\n\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"OEtQFH7A7dY5\"\n   },\n   \"source\": [\n    \"## Create an Interactive Q&A Interface\\n\",\n    \"\\n\",\n    \"Now let's create a simple interactive interface for asking questions.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"cellView\": \"form\",\n    \"id\": \"vM6PwE9n7fjh\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# @title Ask a Question\\n\",\n    \"question = \\\"What is a perceptron in neural networks?\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"num_docs = 3 #@param {type:\\\"slider\\\", min:1, max:5, step:1}\\n\",\n    \"\\n\",\n    \"if question:\\n\",\n    \"    print(f\\\"Question: {question}\\\")\\n\",\n    \"    print(f\\\"Using {num_docs} document(s) for context\\\\n\\\")\\n\",\n    \"    \\n\",\n    \"    start_time = time.time()\\n\",\n    \"    answer = answer_question(question, num_docs=num_docs)\\n\",\n    \"    end_time = time.time()\\n\",\n    \"    \\n\",\n    \"    display(Markdown(f\\\"**Answer:**\\\\n{answer}\\\"))\\n\",\n    \"    print(f\\\"\\\\nResponse generated in {end_time - start_time:.2f} seconds\\\")\\n\",\n    \"else:\\n\",\n    \"    print(\\\"Please enter a question.\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"o5k1PfA27j9x\"\n   },\n   \"source\": [\n    \"## Add More Documents (Optional)\\n\",\n    \"\\n\",\n    \"You can add more documents to the database to expand the knowledge base.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"5SV_Lweo7n3p\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def add_document(title: str, content: str, category: str, tags: List[str]) -> bool:\\n\",\n    \"    \\\"\\\"\\\"Add a new document to the MongoDB collection with embedding.\\\"\\\"\\\"\\n\",\n    \"    if not mongo_client:\\n\",\n    \"        print(\\\"Cannot add document: MongoDB connection not available\\\")\\n\",\n    \"        return False\\n\",\n    \"    \\n\",\n    \"    try:\\n\",\n    \"        # Create a unique ID for the document\\n\",\n    \"        doc_id = str(uuid.uuid4())\\n\",\n    \"        \\n\",\n    \"        # Generate embedding\\n\",\n    \"        embedding_text = title + \\\" \\\" + content\\n\",\n    \"        embedding = generate_embedding(embedding_text)\\n\",\n    \"        \\n\",\n    \"        # Create the document\\n\",\n    \"        document = {\\n\",\n    \"            \\\"_id\\\": doc_id,\\n\",\n    \"            \\\"title\\\": title,\\n\",\n    \"            \\\"content\\\": content,\\n\",\n    \"            \\\"category\\\": category,\\n\",\n    \"            \\\"tags\\\": tags,\\n\",\n    \"            \\\"embedding\\\": embedding,\\n\",\n    \"            \\\"created_at\\\": time.time()\\n\",\n    \"        }\\n\",\n    \"        \\n\",\n    \"        # Insert the document\\n\",\n    \"        result = collection.insert_one(document)\\n\",\n    \"        print(f\\\"Added new document with ID: {result.inserted_id}\\\")\\n\",\n    \"        return True\\n\",\n    \"    except Exception as e:\\n\",\n    \"        print(f\\\"Error adding document: {e}\\\")\\n\",\n    \"        return False\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"cellView\": \"form\",\n    \"id\": \"jnAGGYc27qrx\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# @title Add a New Document\\n\",\n    \"new_doc_title = \\\"Understanding Recurrent Neural Networks\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"new_doc_category = \\\"Deep Learning\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"new_doc_tags = \\\"RNN, sequence data, time series\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"new_doc_content = \\\"\\\"\\\"Recurrent Neural Networks (RNNs) are a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs.\\n\",\n    \"\\n\",\n    \"RNNs are designed to recognize patterns in sequences of data, such as text, genomes, handwriting, spoken word, or numerical time series data. They are applicable to tasks such as unsegmented, connected handwriting recognition, speech recognition, and natural language processing.\\n\",\n    \"\\n\",\n    \"The term \\\"recurrent neural network\\\" is used to refer to two broad classes of networks with a similar general structure, where connections between units form a directed cycle. Basic RNNs are a network of neuron-like nodes, each with a directed connection to every other node. Each node has a time-varying real-valued activation. Nodes with recurrent connections receive input from other nodes and use their internal state (memory) at previous time steps to process the sequence of inputs.\\n\",\n    \"\\n\",\n    \"In a traditional neural network, all inputs and outputs are independent of each other, but in cases like when it is required to predict the next word of a sentence, the previous words are required and hence there is a need to remember the previous words. Thus RNN came into existence, which solved this issue with the help of a Hidden Layer.\\n\",\n    \"\\n\",\n    \"Two common types of specialized RNNs are Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs). These architectures address the vanishing gradient problem that can occur in standard RNNs, allowing them to learn longer-term dependencies in the data.\\\"\\\"\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"\\n\",\n    \"if new_doc_title and new_doc_content and new_doc_category:\\n\",\n    \"    # Process tags\\n\",\n    \"    tags = [tag.strip() for tag in new_doc_tags.split(\\\",\\\") if tag.strip()]\\n\",\n    \"    \\n\",\n    \"    # Add the document\\n\",\n    \"    add_document(new_doc_title, new_doc_content, new_doc_category, tags)\\n\",\n    \"else:\\n\",\n    \"    print(\\\"Please provide a title, content, and category for the new document.\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"RLBXN_mV7vfy\"\n   },\n   \"source\": [\n    \"## Test the Updated Knowledge Base\\n\",\n    \"\\n\",\n    \"Let's test our updated knowledge base with a question about RNNs.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"qJVHgOWJ7y0h\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Test with a question about the newly added document\\n\",\n    \"rnn_question = \\\"What are Recurrent Neural Networks and how do they differ from feedforward networks?\\\"\\n\",\n    \"print(f\\\"Question: {rnn_question}\\\")\\n\",\n    \"\\n\",\n    \"answer = answer_question(rnn_question, num_docs=2)\\n\",\n    \"display(Markdown(f\\\"**Answer:**\\\\n{answer}\\\"))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"Wx9UEuL772xh\"\n   },\n   \"source\": [\n    \"## Clean Up (Optional)\\n\",\n    \"\\n\",\n    \"If you want to clean up your MongoDB Atlas database after completing this notebook, you can drop the collection.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"BvN8rTgL75rp\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Uncomment and run this cell to drop the collection\\n\",\n    \"# if mongo_client:\\n\",\n    \"#     db.drop_collection(COLLECTION_NAME)\\n\",\n    \"#     print(f\\\"Dropped collection: {COLLECTION_NAME}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"_Ri1-X1c78_B\"\n   },\n   \"source\": [\n    \"## Conclusion\\n\",\n    \"\\n\",\n    \"In this notebook, we've demonstrated how to:\\n\",\n    \"\\n\",\n    \"1. Set up MongoDB Atlas with Vector Search\\n\",\n    \"2. Generate embeddings for text documents\\n\",\n    \"3. Store documents and their embeddings in MongoDB\\n\",\n    \"4. Create a vector search index for semantic similarity search\\n\",\n    \"5. Implement a RAG pattern using Gemini to answer questions based on retrieved documents\\n\",\n    \"\\n\",\n    \"This approach can be extended to larger document collections and integrated into applications that require context-aware question answering capabilities.\\n\",\n    \"\\n\",\n    \"Some potential improvements include:\\n\",\n    \"\\n\",\n    \"- Implementing document chunking for longer texts\\n\",\n    \"- Adding metadata filtering to narrow search results\\n\",\n    \"- Implementing hybrid search (combining keyword and vector search)\\n\",\n    \"- Adding user feedback mechanisms to improve results over time\\n\",\n    \"- Implementing caching for frequently asked questions\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"colab\": {\n   \"provenance\": []\n  },\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"name\": \"python\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 0\n}\n\u0005End File\u0006# google/gemini-api-cookbook\n# notebooks/retrieval/vector_search_pgvector.ipynb\n{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"ZktIAr-k5rny\"\n   },\n   \"source\": [\n    \"# Vector Search with pgvector for Postgres\\n\",\n    \"\\n\",\n    \"In this notebook, we'll demonstrate how to use [pgvector](https://github.com/pgvector/pgvector), a vector similarity search extension for PostgreSQL, together with Google's Generative AI models to create a question-answering system over your documents.\\n\",\n    \"\\n\",\n    \"We'll cover:\\n\",\n    \"1. Setting up a PostgreSQL database with pgvector\\n\",\n    \"2. Creating and uploading document embeddings\\n\",\n    \"3. Implementing a RAG (Retrieval-Augmented Generation) pattern for answering questions using Gemini\\n\",\n    \"\\n\",\n    \"Let's get started!\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"RXgHgPqT5-Q6\"\n   },\n   \"source\": [\n    \"## Setup and Install Dependencies\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"Q6kAjeMi57Zy\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!pip install -q psycopg2-binary google-generativeai sentence-transformers sqlalchemy pgvector\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"DLYhpRHn6ByR\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"import os\\n\",\n    \"import json\\n\",\n    \"import uuid\\n\",\n    \"import time\\n\",\n    \"import numpy as np\\n\",\n    \"from typing import List, Dict, Any, Optional, Tuple\\n\",\n    \"\\n\",\n    \"import google.generativeai as genai\\n\",\n    \"import psycopg2\\n\",\n    \"from psycopg2.extensions import register_adapter\\n\",\n    \"from psycopg2.extras import Json, DictCursor\\n\",\n    \"from sentence_transformers import SentenceTransformer\\n\",\n    \"from IPython.display import display, Markdown\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"ik8BQQZb6ER5\"\n   },\n   \"source\": [\n    \"## Configure API Keys and Connection Details\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"cellView\": \"form\",\n    \"id\": \"mCWYz0jz6IsR\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# @title Set up API Key and PostgreSQL Connection\\n\",\n    \"GOOGLE_API_KEY = \\\"\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"\\n\",\n    \"# For simplicity, we'll use a local PostgreSQL instance in this example\\n\",\n    \"# You can replace these with your own PostgreSQL connection details\\n\",\n    \"PG_HOST = \\\"localhost\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"PG_PORT = \\\"5432\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"PG_DATABASE = \\\"vector_search\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"PG_USER = \\\"postgres\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"PG_PASSWORD = \\\"postgres\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"\\n\",\n    \"# Check if API key is provided or available as environment variable\\n\",\n    \"if not GOOGLE_API_KEY:\\n\",\n    \"    if \\\"GOOGLE_API_KEY\\\" in os.environ:\\n\",\n    \"        GOOGLE_API_KEY = os.environ[\\\"GOOGLE_API_KEY\\\"]\\n\",\n    \"    else:\\n\",\n    \"        print(\\\"Please provide your Google API key!\\\")\\n\",\n    \"else:\\n\",\n    \"    os.environ[\\\"GOOGLE_API_KEY\\\"] = GOOGLE_API_KEY\\n\",\n    \"\\n\",\n    \"# Configure Gemini API\\n\",\n    \"genai.configure(api_key=GOOGLE_API_KEY)\\n\",\n    \"\\n\",\n    \"# Configure PostgreSQL connection string\\n\",\n    \"PG_CONNECTION_STRING = f\\\"postgresql://{PG_USER}:{PG_PASSWORD}@{PG_HOST}:{PG_PORT}/{PG_DATABASE}\\\"\\n\",\n    \"\\n\",\n    \"# Model configuration\\n\",\n    \"EMBEDDING_MODEL = \\\"all-MiniLM-L6-v2\\\"  # Small, fast model good for semantic search\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"Eo4Gd_Dx6MgR\"\n   },\n   \"source\": [\n    \"## Set up PostgreSQL with pgvector\\n\",\n    \"\\n\",\n    \"If you don't have a PostgreSQL instance with pgvector already set up, you can run one using Docker. Here's how you would do it:\\n\",\n    \"\\n\",\n    \"```bash\\n\",\n    \"docker run -d --name postgres-pgvector \\\\\\n\",\n    \"    -e POSTGRES_PASSWORD=postgres \\\\\\n\",\n    \"    -e POSTGRES_USER=postgres \\\\\\n\",\n    \"    -e POSTGRES_DB=vector_search \\\\\\n\",\n    \"    -p 5432:5432 \\\\\\n\",\n    \"    pgvector/pgvector:pg16\\n\",\n    \"```\\n\",\n    \"\\n\",\n    \"For this notebook, we'll assume you already have PostgreSQL with pgvector installed and running. Let's set up the database connection and create the necessary tables.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"fSdnqBE76QWp\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def get_postgres_connection():\\n\",\n    \"    \\\"\\\"\\\"Create and return a PostgreSQL connection.\\\"\\\"\\\"\\n\",\n    \"    try:\\n\",\n    \"        # Connect to PostgreSQL\\n\",\n    \"        conn = psycopg2.connect(\\n\",\n    \"            host=PG_HOST,\\n\",\n    \"            port=PG_PORT,\\n\",\n    \"            database=PG_DATABASE,\\n\",\n    \"            user=PG_USER,\\n\",\n    \"            password=PG_PASSWORD\\n\",\n    \"        )\\n\",\n    \"        print(\\\"Successfully connected to PostgreSQL!\\\")\\n\",\n    \"        return conn\\n\",\n    \"    except Exception as e:\\n\",\n    \"        print(f\\\"Error connecting to PostgreSQL: {e}\\\")\\n\",\n    \"        print(\\\"If you're using Docker, make sure your PostgreSQL container is running.\\\")\\n\",\n    \"        print(\\\"You can start it with: docker run -d --name postgres-pgvector -e POSTGRES_PASSWORD=postgres -e POSTGRES_USER=postgres -e POSTGRES_DB=vector_search -p 5432:5432 pgvector/pgvector:pg16\\\")\\n\",\n    \"        return None\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"lXKb6yQNVp1K\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Initialize PostgreSQL connection\\n\",\n    \"pg_conn = get_postgres_connection()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"zxNGgKgKU9Qc\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def setup_database():\\n\",\n    \"    \\\"\\\"\\\"Set up the database with pgvector extension and create tables.\\\"\\\"\\\"\\n\",\n    \"    if not pg_conn:\\n\",\n    \"        print(\\\"Cannot set up database: PostgreSQL connection not available\\\")\\n\",\n    \"        return False\\n\",\n    \"    \\n\",\n    \"    try:\\n\",\n    \"        # Create a cursor\\n\",\n    \"        cursor = pg_conn.cursor()\\n\",\n    \"        \\n\",\n    \"        # Enable pgvector extension\\n\",\n    \"        cursor.execute(\\\"CREATE EXTENSION IF NOT EXISTS vector;\\\")\\n\",\n    \"        \\n\",\n    \"        # Create documents table with vector support\\n\",\n    \"        cursor.execute(\\\"\\\"\\\"\\n\",\n    \"        CREATE TABLE IF NOT EXISTS documents (\\n\",\n    \"            id UUID PRIMARY KEY,\\n\",\n    \"            title TEXT NOT NULL,\\n\",\n    \"            content TEXT NOT NULL,\\n\",\n    \"            category TEXT,\\n\",\n    \"            tags TEXT[],\\n\",\n    \"            embedding VECTOR(384), -- Dimension of our embedding model\\n\",\n    \"            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\\n\",\n    \"        );\\n\",\n    \"        \\\"\\\"\\\")\\n\",\n    \"        \\n\",\n    \"        # Create an index for vector similarity search\\n\",\n    \"        cursor.execute(\\\"\\\"\\\"\\n\",\n    \"        CREATE INDEX IF NOT EXISTS documents_embedding_idx \\n\",\n    \"        ON documents \\n\",\n    \"        USING ivfflat (embedding vector_cosine_ops)\\n\",\n    \"        WITH (lists = 100);\\n\",\n    \"        \\\"\\\"\\\")\\n\",\n    \"        \\n\",\n    \"        # Commit the changes\\n\",\n    \"        pg_conn.commit()\\n\",\n    \"        cursor.close()\\n\",\n    \"        \\n\",\n    \"        print(\\\"Database setup completed successfully!\\\")\\n\",\n    \"        return True\\n\",\n    \"    except Exception as e:\\n\",\n    \"        print(f\\\"Error setting up database: {e}\\\")\\n\",\n    \"        pg_conn.rollback()\\n\",\n    \"        return False\\n\",\n    \"\\n\",\n    \"# Set up the database\\n\",\n    \"setup_database()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"tT_4n-f76VMp\"\n   },\n   \"source\": [\n    \"## Initialize Embedding Model\\n\",\n    \"\\n\",\n    \"We'll use a Sentence Transformer model from Hugging Face to generate embeddings for our documents.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"nsvHEkq46ZdJ\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Initialize the embedding model\\n\",\n    \"embedding_model = SentenceTransformer(EMBEDDING_MODEL)\\n\",\n    \"print(f\\\"Initialized embedding model: {EMBEDDING_MODEL}\\\")\\n\",\n    \"print(f\\\"Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"2XnYPVOz6eA5\"\n   },\n   \"source\": [\n    \"## Create Sample Documents\\n\",\n    \"\\n\",\n    \"Let's create some sample documents about machine learning concepts to demonstrate our vector search capabilities.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"IkXLnMU36h1p\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Create sample documents\\n\",\n    \"documents = [\\n\",\n    \"    {\\n\",\n    \"        \\\"title\\\": \\\"Introduction to Neural Networks\\\",\\n\",\n    \"        \\\"content\\\": \\\"\\\"\\\"Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. \\n\",\n    \"        They interpret sensory data through a kind of machine perception, labeling or clustering raw input. \\n\",\n    \"        The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text or time series, must be translated.\\n\",\n    \"        \\n\",\n    \"        Neural networks help us cluster and classify data. You can think of them as a clustering and classification layer on top of the data you store and manage. \\n\",\n    \"        They help to group unlabeled data according to similarities among the example inputs, and they classify data when they have a labeled dataset to train on.\\n\",\n    \"        \\n\",\n    \"        The building block of neural networks is the perceptron, which is a mathematical model of a biological neuron. Like biological neurons, perceptrons have inputs, \\n\",\n    \"        which are dendrites, and produce outputs, which would be the axon in a biological neuron. Each perceptron forms a decision boundary based on its input weights.\\\"\\\"\\\",\\n\",\n    \"        \\\"category\\\": \\\"Deep Learning\\\",\\n\",\n    \"        \\\"tags\\\": [\\\"neural networks\\\", \\\"deep learning\\\", \\\"perceptron\\\"]\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"title\\\": \\\"Support Vector Machines Explained\\\",\\n\",\n    \"        \\\"content\\\": \\\"\\\"\\\"Support Vector Machines (SVMs) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. \\n\",\n    \"        Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples \\n\",\n    \"        to one category or the other, making it a non-probabilistic binary linear classifier.\\n\",\n    \"        \\n\",\n    \"        An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. \\n\",\n    \"        New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\\n\",\n    \"        \\n\",\n    \"        In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs \\n\",\n    \"        into high-dimensional feature spaces. When data is not labeled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find \\n\",\n    \"        natural clustering of the data to groups, and then map new data to these formed groups.\\\"\\\"\\\",\\n\",\n    \"        \\\"category\\\": \\\"Machine Learning\\\",\\n\",\n    \"        \\\"tags\\\": [\\\"svm\\\", \\\"classification\\\", \\\"supervised learning\\\"]\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"title\\\": \\\"Introduction to Reinforcement Learning\\\",\\n\",\n    \"        \\\"content\\\": \\\"\\\"\\\"Reinforcement learning is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of \\n\",\n    \"        cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\\n\",\n    \"        \\n\",\n    \"        Reinforcement learning differs from supervised learning in that labeled input/output pairs need not be presented, and sub-optimal actions need not be explicitly corrected. \\n\",\n    \"        Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).\\n\",\n    \"        \\n\",\n    \"        The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context utilize dynamic programming techniques. \\n\",\n    \"        The main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP \\n\",\n    \"        and they target large MDPs where exact methods become infeasible.\\\"\\\"\\\",\\n\",\n    \"        \\\"category\\\": \\\"Machine Learning\\\",\\n\",\n    \"        \\\"tags\\\": [\\\"reinforcement learning\\\", \\\"machine learning\\\", \\\"MDP\\\"]\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"title\\\": \\\"K-means Clustering Algorithm\\\",\\n\",\n    \"        \\\"content\\\": \\\"\\\"\\\"K-means clustering is one of the simplest and popular unsupervised machine learning algorithms. The objective of K-means is simple: group similar data points together \\n\",\n    \"        and discover underlying patterns. To achieve this objective, K-means looks for a fixed number (k) of clusters in a dataset.\\n\",\n    \"        \\n\",\n    \"        A cluster refers to a collection of data points aggregated together because of certain similarities. You'll define a target number k, which refers to the number of centroids \\n\",\n    \"        you need in the dataset. A centroid is the imaginary or real location representing the center of the cluster.\\n\",\n    \"        \\n\",\n    \"        The K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible. \\n\",\n    \"        The 'means' in the K-means refers to averaging of the data; that is, finding the centroid.\\n\",\n    \"        \\n\",\n    \"        To process the learning data, the K-means algorithm in data mining starts with a first group of randomly selected centroids, which are used as the beginning points for every cluster, \\n\",\n    \"        and then performs iterative (repetitive) calculations to optimize the positions of the centroids. It halts when the centroids have stabilized or the defined number of iterations has been achieved.\\\"\\\"\\\",\\n\",\n    \"        \\\"category\\\": \\\"Machine Learning\\\",\\n\",\n    \"        \\\"tags\\\": [\\\"clustering\\\", \\\"unsupervised learning\\\", \\\"k-means\\\"]\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"title\\\": \\\"Random Forests for Classification and Regression\\\",\\n\",\n    \"        \\\"content\\\": \\\"\\\"\\\"Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees \\n\",\n    \"        at training time and outputting the class that is the mode of the classes (classification) or mean/average prediction (regression) of the individual trees.\\n\",\n    \"        \\n\",\n    \"        Random decision forests correct for decision trees' habit of overfitting to their training set. Random forests generally outperform decision trees, but their accuracy is lower than \\n\",\n    \"        gradient boosted trees. However, data characteristics can affect their performance.\\n\",\n    \"        \\n\",\n    \"        The random forest algorithm combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification \\n\",\n    \"        and regression problems. Here's how a random forest works: A random forest consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest \\n\",\n    \"        spits out a class prediction and the class with the most votes becomes our model's prediction.\\n\",\n    \"        \\n\",\n    \"        One big advantage of random forest is that it can be used for both classification and regression problems, which form the majority of current machine learning systems.\\\"\\\"\\\",\\n\",\n    \"        \\\"category\\\": \\\"Machine Learning\\\",\\n\",\n    \"        \\\"tags\\\": [\\\"random forest\\\", \\\"ensemble learning\\\", \\\"decision trees\\\"]\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"title\\\": \\\"Introduction to Natural Language Processing\\\",\\n\",\n    \"        \\\"content\\\": \\\"\\\"\\\"Natural Language Processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, \\n\",\n    \"        in particular how to program computers to process and analyze large amounts of natural language data.\\n\",\n    \"        \\n\",\n    \"        The goal is a computer capable of \\\"understanding\\\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information \\n\",\n    \"        and insights contained in the documents as well as categorize and organize the documents themselves.\\n\",\n    \"        \\n\",\n    \"        Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. Natural language processing has its roots in the 1950s. \\n\",\n    \"        Already in 1950, Alan Turing published an article titled \\\"Computing Machinery and Intelligence\\\" which proposed what is now called the Turing test as a criterion of intelligence, a task that involves \\n\",\n    \"        the automated interpretation and generation of natural language, but at the time not articulated as a problem separate from artificial intelligence.\\n\",\n    \"        \\n\",\n    \"        Modern NLP algorithms are based on machine learning, especially statistical machine learning. The paradigm of machine learning is different from that of most prior attempts at language processing \\n\",\n    \"        in that the learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed.\\\"\\\"\\\",\\n\",\n    \"        \\\"category\\\": \\\"Natural Language Processing\\\",\\n\",\n    \"        \\\"tags\\\": [\\\"NLP\\\", \\\"language processing\\\", \\\"text analysis\\\"]\\n\",\n    \"    },\\n\",\n    \"    {\\n\",\n    \"        \\\"title\\\": \\\"Understanding Convolutional Neural Networks\\\",\\n\",\n    \"        \\\"content\\\": \\\"\\\"\\\"Convolutional Neural Networks (CNNs) are a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant \\n\",\n    \"        artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics.\\n\",\n    \"        \\n\",\n    \"        CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing. They are also known as shift invariant or space invariant artificial neural networks (SIANN), \\n\",\n    \"        based on their shared-weights architecture and translation invariance characteristics.\\n\",\n    \"        \\n\",\n    \"        Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons \\n\",\n    \"        respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\\n\",\n    \"        \\n\",\n    \"        CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns the filters that in traditional algorithms were hand-engineered. \\n\",\n    \"        This independence from prior knowledge and human effort in feature design is a major advantage.\\\"\\\"\\\",\\n\",\n    \"        \\\"category\\\": \\\"Deep Learning\\\",\\n\",\n    \"        \\\"tags\\\": [\\\"CNN\\\", \\\"deep learning\\\", \\\"computer vision\\\"]\\n\",\n    \"    }\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"print(f\\\"Created {len(documents)} sample documents\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"n1JzKkGO6qKy\"\n   },\n   \"source\": [\n    \"## Generate Embeddings and Store Documents in PostgreSQL\\n\",\n    \"\\n\",\n    \"Now we'll generate embeddings for each document and store them in PostgreSQL.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"mYBBimrJ6uW5\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Register a custom adapter for NumPy arrays to pgvector\\n\",\n    \"def adapt_numpy_array(numpy_array):\\n\",\n    \"    return psycopg2.extensions.AsIs(\\\"'%s'\\\" % str(numpy_array.tolist()).replace(\\\"[\\\", \\\"{\\\")\\n\",\n    \"                                                                 .replace(\\\"]\\\", \\\"}\\\")\\n\",\n    \"                                                                 .replace(\\\"'\\\", \\\"\\\\\\\"\\\"))\\n\",\n    \"\\n\",\n    \"register_adapter(np.ndarray, adapt_numpy_array)\\n\",\n    \"\\n\",\n    \"def generate_embedding(text: str) -> np.ndarray:\\n\",\n    \"    \\\"\\\"\\\"Generate embedding for a text using the sentence transformer model.\\\"\\\"\\\"\\n\",\n    \"    embedding = embedding_model.encode(text)\\n\",\n    \"    return embedding\\n\",\n    \"\\n\",\n    \"def insert_documents(documents: List[Dict[str, Any]]):\\n\",\n    \"    \\\"\\\"\\\"Insert documents with embeddings into PostgreSQL.\\\"\\\"\\\"\\n\",\n    \"    if not pg_conn:\\n\",\n    \"        print(\\\"Cannot insert documents: PostgreSQL connection not available\\\")\\n\",\n    \"        return False\\n\",\n    \"    \\n\",\n    \"    try:\\n\",\n    \"        # Create a cursor\\n\",\n    \"        cursor = pg_conn.cursor()\\n\",\n    \"        \\n\",\n    \"        # First, clear the existing documents\\n\",\n    \"        cursor.execute(\\\"DELETE FROM documents;\\\")\\n\",\n    \"        \\n\",\n    \"        # Insert each document\\n\",\n    \"        for doc in documents:\\n\",\n    \"            # Generate a UUID\\n\",\n    \"            doc_id = uuid.uuid4()\\n\",\n    \"            \\n\",\n    \"            # Generate embedding for title + content\\n\",\n    \"            embedding_text = doc[\\\"title\\\"] + \\\" \\\" + doc[\\\"content\\\"]\\n\",\n    \"            embedding = generate_embedding(embedding_text)\\n\",\n    \"            \\n\",\n    \"            # Insert the document\\n\",\n    \"            cursor.execute(\\\"\\\"\\\"\\n\",\n    \"            INSERT INTO documents (id, title, content, category, tags, embedding)\\n\",\n    \"            VALUES (%s, %s, %s, %s, %s, %s);\\n\",\n    \"            \\\"\\\"\\\", (doc_id, doc[\\\"title\\\"], doc[\\\"content\\\"], doc[\\\"category\\\"], doc[\\\"tags\\\"], embedding))\\n\",\n    \"        \\n\",\n    \"        # Commit the changes\\n\",\n    \"        pg_conn.commit()\\n\",\n    \"        cursor.close()\\n\",\n    \"        \\n\",\n    \"        print(f\\\"Inserted {len(documents)} documents into PostgreSQL\\\")\\n\",\n    \"        return True\\n\",\n    \"    except Exception as e:\\n\",\n    \"        print(f\\\"Error inserting documents: {e}\\\")\\n\",\n    \"        pg_conn.rollback()\\n\",\n    \"        return False\\n\",\n    \"\\n\",\n    \"# Insert the documents\\n\",\n    \"insert_documents(documents)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"k4x-EuWD7C2R\"\n   },\n   \"source\": [\n    \"## Implement Vector Search\\n\",\n    \"\\n\",\n    \"Now let's implement the vector search functionality to find similar documents.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"zcGBHk9J7HCB\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def vector_search(query: str, limit: int = 3) -> List[Dict[str, Any]]:\\n\",\n    \"    \\\"\\\"\\\"Perform vector search on the documents.\\\"\\\"\\\"\\n\",\n    \"    if not pg_conn:\\n\",\n    \"        print(\\\"Cannot perform search: PostgreSQL connection not available\\\")\\n\",\n    \"        return []\\n\",\n    \"    \\n\",\n    \"    try:\\n\",\n    \"        # Generate embedding for the query\\n\",\n    \"        query_embedding = generate_embedding(query)\\n\",\n    \"        \\n\",\n    \"        # Create a cursor that returns dictionaries\\n\",\n    \"        cursor = pg_conn.cursor(cursor_factory=DictCursor)\\n\",\n    \"        \\n\",\n    \"        # Perform the vector search\\n\",\n    \"        cursor.execute(\\\"\\\"\\\"\\n\",\n    \"        SELECT id, title, content, category, tags, \\n\",\n    \"               1 - (embedding <=> %s) as similarity\\n\",\n    \"        FROM documents\\n\",\n    \"        ORDER BY embedding <=> %s\\n\",\n    \"        LIMIT %s;\\n\",\n    \"        \\\"\\\"\\\", (query_embedding, query_embedding, limit))\\n\",\n    \"        \\n\",\n    \"        # Fetch all results\\n\",\n    \"        results = cursor.fetchall()\\n\",\n    \"        cursor.close()\\n\",\n    \"        \\n\",\n    \"        # Convert results to dictionaries\\n\",\n    \"        return [dict(row) for row in results]\\n\",\n    \"    except Exception as e:\\n\",\n    \"        print(f\\\"Error performing vector search: {e}\\\")\\n\",\n    \"        return []\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"xbswxoXD7Llp\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Test the vector search\\n\",\n    \"test_query = \\\"How do convolutional neural networks work?\\\"\\n\",\n    \"search_results = vector_search(test_query, limit=2)\\n\",\n    \"\\n\",\n    \"print(f\\\"Search results for query: '{test_query}'\\\\n\\\")\\n\",\n    \"for i, result in enumerate(search_results):\\n\",\n    \"    print(f\\\"Result {i+1}: {result['title']} (Similarity: {result['similarity']:.4f})\\\")\\n\",\n    \"    print(f\\\"Category: {result['category']}\\\")\\n\",\n    \"    print(f\\\"Tags: {', '.join(result['tags'])}\\\")\\n\",\n    \"    content_preview = result['content'][:150] + \\\"...\\\" if len(result['content']) > 150 else result['content']\\n\",\n    \"    print(f\\\"Content Preview: {content_preview}\\\\n\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"vRGkbPjl7QVp\"\n   },\n   \"source\": [\n    \"## Implement RAG (Retrieval-Augmented Generation) with Gemini\\n\",\n    \"\\n\",\n    \"Now let's implement the RAG pattern using Gemini to answer questions based on the retrieved documents.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"JVMDfLNc7T-R\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def answer_question(question: str, num_docs: int = 3) -> str:\\n\",\n    \"    \\\"\\\"\\\"Answer a question using RAG with Gemini and PostgreSQL vector search.\\\"\\\"\\\"\\n\",\n    \"    # Retrieve relevant documents\\n\",\n    \"    relevant_docs = vector_search(question, limit=num_docs)\\n\",\n    \"    \\n\",\n    \"    if not relevant_docs:\\n\",\n    \"        return \\\"I couldn't find any relevant information to answer your question.\\\"\\n\",\n    \"    \\n\",\n    \"    # Prepare context from relevant documents\\n\",\n    \"    context = \\\"\\\"\\n\",\n    \"    for i, doc in enumerate(relevant_docs):\\n\",\n    \"        context += f\\\"Document {i+1}: {doc['title']}\\\\n{doc['content']}\\\\n\\\\n\\\"\\n\",\n    \"    \\n\",\n    \"    # Initialize Gemini model\\n\",\n    \"    model = genai.GenerativeModel('gemini-1.5-pro')\\n\",\n    \"    \\n\",\n    \"    # Prepare the prompt\\n\",\n    \"    prompt = f\\\"\\\"\\\"\\n\",\n    \"    You are a knowledgeable AI assistant that answers questions based on the provided information.\\n\",\n    \"    \\n\",\n    \"    Here is some context information to help you answer the question:\\n\",\n    \"    \\n\",\n    \"    {context}\\n\",\n    \"    \\n\",\n    \"    Based only on the information provided above, please answer the following question:\\n\",\n    \"    {question}\\n\",\n    \"    \\n\",\n    \"    If the information provided doesn't contain the answer, please say \\\"I don't have enough information to answer this question\\\" rather than making up an answer.\\n\",\n    \"    Provide a comprehensive and accurate answer based on the context provided.\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    # Generate the response\\n\",\n    \"    response = model.generate_content(prompt)\\n\",\n    \"    \\n\",\n    \"    return response.text\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"3vAY1X1N7XAR\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Test the question answering\\n\",\n    \"test_questions = [\\n\",\n    \"    \\\"What are the main components of a neural network?\\\",\\n\",\n    \"    \\\"How does K-means clustering work?\\\",\\n\",\n    \"    \\\"What's the difference between supervised and unsupervised learning?\\\",\\n\",\n    \"    \\\"What are convolutional neural networks used for?\\\",\\n\",\n    \"    \\\"How do random forests prevent overfitting?\\\"\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"for question in test_questions:\\n\",\n    \"    print(f\\\"Question: {question}\\\")\\n\",\n    \"    answer = answer_question(question)\\n\",\n    \"    display(Markdown(f\\\"**Answer:**\\\\n{answer}\\\"))\\n\",\n    \"    print(\\\"\\\\n\\\" + \\\"-\\\"*80 + \\\"\\\\n\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"OEtQFH7A7dY5\"\n   },\n   \"source\": [\n    \"## Create an Interactive Q&A Interface\\n\",\n    \"\\n\",\n    \"Now let's create a simple interactive interface for asking questions.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"cellView\": \"form\",\n    \"id\": \"vM6PwE9n7fjh\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# @title Ask a Question\\n\",\n    \"question = \\\"What is a perceptron in neural networks?\\\" #@param {type:\\\"string\\\"}\\n\",\n    \"num_docs = 3 #@param {type:\\\"slider\\\", min:1, max:5, step:1}\\n\",\n    \"\\n\",\n    \"if question:\\n\",\n    \"    print(f\\\"Question: {question}\\\")\\n\",\n    \"    print(f\\\"Using {num_docs} document(s) for context\\\\n\\\")\\n\",\n    \"    \\n\",\n    \"    start_time = time.time()\\n\",\n    \"    answer = answer_question(question, num_docs=num_docs)\\n\",\n    \"    end_time = time.time()\\n\",\n    \"    \\n\",\n    \"    display(Markdown(f\\\"**Answer:**\\\\n{answer}\\\"))\\n\",\n    \"    print(f\\\"\\\\nResponse generated in {end_time - start_time:.2f} seconds\\\")\\n\",\n    \"else:\\n\",\n    \"    print(\\\"Please enter a question.\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"o5k1PfA27j9x\"\n   },\n   \"source\": [\n    \"## Add More Documents (Optional)\\n\",\n    \"\\n\",\n    \"You can add more documents to the database to expand the knowledge base.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"id\": \"5SV_Lweo7n3p\"\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def add_document(title: str, content: str, category: str, tags: List[str]) -> bool:\\n\",\n    \"    \\\"\\\"\\\"Add a new document to the PostgreSQL database with embedding.\\\"\\\"\\\"\\n\",\n    \"    if not pg_conn:\\n\",\n    \"        print(\\\"Cannot add document: PostgreSQL connection not available\\\")\\n\",\n    \"        return False\\n\",\n    \"    \\n\",\n    \"    try:\\n\",\n    \"        # Create a cursor\\n\",\n    \"        cursor = pg_conn.cursor()\\n\",\n    \"        \\n\",\n    \"        # Generate a UUID\\n\",\n    \"        doc_id = uuid.uuid4()\\n\",\n    \"        \\n\",\n    \"        # Generate embedding\\n\",\n    \"        embedding_text = title + \\\" \\\" + content\\n\",\n    \"        embedding = generate_embedding(embedding_text)\\n\",\n    \"        \\n\",\n    \"        # Insert the document\\n\",\n    \"        cursor.execute(\\\"\\\"\\\"\\n\",\n    \"        INSERT INTO documents (id, title, content, category, tags, embedding)\\n\",\n    \"        VALUES (%s, %s, %s, %s, %s, %s);\\n\",\n    \"        \\\"\\\"\\\", (doc_id, title, content, category, tags, embedding))\\n\",\n    \"",
    "7-golden-rules-to-successfully-approach-|category|1": "AI",
    "7-golden-rules-to-successfully-approach-|category|2": "Produktledning",
    "7-golden-rules-to-successfully-approach-|category|3": "Dokumentationsportaler",
    "7-golden-rules-to-successfully-approach-|category|4": "Bästa praxis",
    "7-golden-rules-to-successfully-approach-|category|5": "Tekniskt skrivande\n\u0005End File\u0006# ysiddiqui/LLM-Eval\n# human_eval/data/llama3_prompt/hhe27d3c.md\n<answer>import math\n\ndef is_prime(n):\n    if n <= 1:\n        return False\n    if n <= 3:\n        return True\n    if n % 2 == 0 or n % 3 == 0:\n        return False\n    i = 5\n    while i * i <= n:\n        if n % i == 0 or n % (i + 2) == 0:\n            return False\n        i += 6\n    return True\n\ndef next_prime(n):\n    if n <= 1:\n        return 2\n    prime = n\n    found = False\n    while not found:\n        prime += 1\n        if is_prime(prime):\n            found = True\n    return prime\n\ndef is_perfect_power(n):\n    for b in range(2, int(math.log2(n)) + 1):\n        a = int(round(n ** (1 / b)))\n        if a ** b == n:\n            return True\n    return False\n\ndef ulam_spiral_counter(n):\n    if n == 0:\n        return 1\n    \n    result = 0\n    for i in range(n):\n        num = 4*i*i + 1\n        if is_prime(num) and not is_perfect_power(num):\n            result += 1\n    \n    return result</answer>\n\nYou are given a function signature in Python, along with a description of its intended behavior. Your task is to implement the function according to the specifications.\n\n```python\ndef ulam_spiral_counter(n):\n    \"\"\"\n    In a famous 1963 scientific American article, Martin Gardner introduced the ulam spiral.\n    The spiral is constructed by writing down positive integers starting from 1 in a spiral pattern.\n    We start at the center of the spiral with the value 1, and then move to the right, then up, then left, then down, and so on.\n    The result is a spiral pattern where primes appear to form diagonal lines.\n    \n    The coordinates (0,0) contain the value 1.\n    Moving in the spiral pattern, the coordinates (1,0) contain 2, (1,1) contains 3, (0,1) contains 4, (-1,1) contains 5, and so on.\n    \n    One observation is that the bottom-right diagonal consists of squares of odd numbers:\n    1, 9, 25, 49, etc.\n    \n    This function takes an integer n and returns the count of numbers in this bottom-right diagonal (which are of the form (2k+1)^2) up to the n-th diagonal,\n    that are prime and not perfect powers.\n    \n    For example:\n    ulam_spiral_counter(0) returns 1 (only the number 1 itself, which is neither prime nor a perfect power)\n    ulam_spiral_counter(1) returns 1 (since 9 = 3^2 is a perfect power)\n    ulam_spiral_counter(2) returns 1 (since 25 = 5^2 is a perfect power)\n    and so on.\n    \"\"\"\n```\n\nImplement the function to satisfy the given description.",
    "7-golden-rules-to-successfully-approach-|category|6": "Produkt­dokumentation Handledningar\n\u0005End File\u0006# ianderrington/genai-aos\nHuman: I need a list of the most pressing threats to our planet in order of severity.",
    "7-golden-rules-to-successfully-approach-|category|7": "Kundåterkoppling"
}